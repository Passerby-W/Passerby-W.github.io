<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>正则表达式</title>
      <link href="/2022/01/10/zheng-ze-biao-da-shi/"/>
      <url>/2022/01/10/zheng-ze-biao-da-shi/</url>
      
        <content type="html"><![CDATA[<h1 id="Regular-Expression"><a href="#Regular-Expression" class="headerlink" title="Regular Expression"></a>Regular Expression</h1><p>![](/Users/haoranwan/Documents/blog/source/_posts/正则表达式/截屏2021-12-28 下午2.58.08.png)</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import re# matching stringpattern1 = "cat"pattern2 = "bird"string = "dog runs to cat"print(pattern1 in string)    # Trueprint(pattern2 in string)    # False# regular expressionpattern1 = "cat"pattern2 = "bird"string = "dog runs to cat"print(re.search(pattern1, string))  # &lt;_sre.SRE_Match object; span=(12, 15), match='cat'&gt;print(re.search(pattern2, string))  # None# multiple patterns ("run" or "ran")ptn = r"r[au]n"       # start with "r" means raw stringprint(re.search(ptn, "dog runs to cat"))    # &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;# continueprint(re.search(r"r[A-Z]n", "dog runs to cat"))     # Noneprint(re.search(r"r[a-z]n", "dog runs to cat"))     # &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;print(re.search(r"r[0-9]n", "dog r2ns to cat"))     # &lt;_sre.SRE_Match object; span=(4, 7), match='r2n'&gt;print(re.search(r"r[0-9a-z]n", "dog runs to cat"))  # &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;# \d : decimal digitprint(re.search(r"r\dn", "run r4n"))                # &lt;_sre.SRE_Match object; span=(4, 7), match='r4n'&gt;# \D : any non-decimal digitprint(re.search(r"r\Dn", "run r4n"))                # &lt;_sre.SRE_Match object; span=(0, 3), match='run'&gt;# \s : any white space [\t\n\r\f\v]print(re.search(r"r\sn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(0, 3), match='r\nn'&gt;# \S : opposite to \s, any non-white spaceprint(re.search(r"r\Sn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(4, 7), match='r4n'&gt;# \w : [a-zA-Z0-9_]print(re.search(r"r\wn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(4, 7), match='r4n'&gt;# \W : opposite to \wprint(re.search(r"r\Wn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(0, 3), match='r\nn'&gt;# \b : empty string (only at the start or end of the word)print(re.search(r"\bruns\b", "dog runs to cat"))    # &lt;_sre.SRE_Match object; span=(4, 8), match='runs'&gt;# \B : empty string (but not at the start or end of a word)print(re.search(r"\B runs \B", "dog   runs  to cat"))  # &lt;_sre.SRE_Match object; span=(8, 14), match=' runs '&gt;# \\ : match \print(re.search(r"runs\\", "runs\ to me"))          # &lt;_sre.SRE_Match object; span=(0, 5), match='runs\\'&gt;# . : match anything (except \n)print(re.search(r"r.n", "r[ns to me"))              # &lt;_sre.SRE_Match object; span=(0, 3), match='r[n'&gt;# ^ : match line beginningprint(re.search(r"^dog", "dog runs to cat"))        # &lt;_sre.SRE_Match object; span=(0, 3), match='dog'&gt;# $ : match line endingprint(re.search(r"cat$", "dog runs to cat"))        # &lt;_sre.SRE_Match object; span=(12, 15), match='cat'&gt;# ? : may or may not occurprint(re.search(r"Mon(day)?", "Monday"))            # &lt;_sre.SRE_Match object; span=(0, 6), match='Monday'&gt;print(re.search(r"Mon(day)?", "Mon"))               # &lt;_sre.SRE_Match object; span=(0, 3), match='Mon'&gt;# multi-linestring = """dog runs to cat.I run to dog."""print(re.search(r"^I", string))                     # Noneprint(re.search(r"^I", string, flags=re.M))         # &lt;_sre.SRE_Match object; span=(18, 19), match='I'&gt;# * : occur 0 or more timesprint(re.search(r"ab*", "a"))                       # &lt;_sre.SRE_Match object; span=(0, 1), match='a'&gt;print(re.search(r"ab*", "abbbbb"))                  # &lt;_sre.SRE_Match object; span=(0, 6), match='abbbbb'&gt;# + : occur 1 or more timesprint(re.search(r"ab+", "a"))                       # Noneprint(re.search(r"ab+", "abbbbb"))                  # &lt;_sre.SRE_Match object; span=(0, 6), match='abbbbb'&gt;# {n, m} : occur n to m timesprint(re.search(r"ab{2,10}", "a"))                  # Noneprint(re.search(r"ab{2,10}", "abbbbb"))             # &lt;_sre.SRE_Match object; span=(0, 6), match='abbbbb'&gt;# groupmatch = re.search(r"(\d+), Date: (.+)", "ID: 021523, Date: Feb/12/2017")print(match.group())                                # 021523, Date: Feb/12/2017print(match.group(1))                               # 021523print(match.group(2))                               # Date: Feb/12/2017match = re.search(r"(?P&lt;id&gt;\d+), Date: (?P&lt;date&gt;.+)", "ID: 021523, Date: Feb/12/2017")print(match.group('id'))                            # 021523print(match.group('date'))                          # Date: Feb/12/2017# findallprint(re.findall(r"r[ua]n", "run ran ren"))         # ['run', 'ran']# | : orprint(re.findall(r"(run|ran)", "run ran ren"))      # ['run', 'ran']# re.sub() replaceprint(re.sub(r"r[au]ns", "catches", "dog runs to cat"))     # dog catches to cat# re.split()print(re.split(r"[,;\.]", "a;b,c.d;e"))             # ['a', 'b', 'c', 'd', 'e']# compilecompiled_re = re.compile(r"r[ua]n")print(compiled_re.search("dog ran to cat"))     # &lt;_sre.SRE_Match object; span=(4, 7), match='ran'&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>贪婪模式：会匹配最长的以开始位置开始，以结束位置结束的字符串；</p><p>懒惰模式：匹配尽可能少的字符。</p><p>![](/Users/haoranwan/Documents/blog/source/_posts/正则表达式/截屏2022-01-10 下午3.27.32.png)</p><p>![](/Users/haoranwan/Documents/blog/source/_posts/正则表达式/截屏2022-01-10 下午3.28.07.png)</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Regular Expression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法-二分法</title>
      <link href="/2022/01/06/suan-fa-er-fen-fa/"/>
      <url>/2022/01/06/suan-fa-er-fen-fa/</url>
      
        <content type="html"><![CDATA[<p>二分法主要是通过左右逼近的方式，向目标值靠拢，并且取得对数复杂度的算法。</p><p>下面这段代码就是一般二分法的代码，不过应该如何理解二分法的本质呢？</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def search(self, nums: List[int], target: int) -&gt; int:left, right = 0, len(nums) - 1while left &lt;= right: mid = left + (right - left)//2if nums[mid] &lt; target: # select1 [&lt;, &lt;=]left = mid + 1else:right = mid - 1return left # select2 left, right<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在做题的时候经常会对上面的选择部分感到迷惑，在具体情况下应该怎么选择呢？</p><p>我先设定一个数组——&gt;[1,2,3,5,6,6,6,6,8,8,9]</p><p>首先我们要先明确一个概念，在while left &lt;= right这个条件下，我们要确保数组的完整性，所以left, right = 0, len(nums) - 1，这样可以保证index的取值空间为[0, len(nums) - 1]这个闭区间。其次while的终止条件如下面表格所示：</p><table><thead><tr><th>1</th><th>2</th><th>3</th><th>5</th><th>6</th><th>6</th><th>6</th><th>6</th><th>8</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>L</td><td></td><td></td><td></td><td></td><td>M</td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td></td><td></td><td></td><td>R</td><td>L</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>由表格可见，在左右指针交换位置的时候终止循环。那么L和R指针分别的意义是什么呢？</p><p>场景1：寻找小于target的位置，则目标位置是6，index为3</p><p>则在终止循环前的一个循环时L,R的位置如下：</p><p>如果select1为&lt;时，等价于求&lt;6和first&gt;=6，如下所示：</p><table><thead><tr><th>1</th><th align="left">2</th><th>3</th><th>5</th><th align="center">6</th><th>6</th><th>6</th><th>6</th><th>8</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>L</td><td align="left"></td><td></td><td></td><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td>L</td><td align="left"></td><td></td><td></td><td align="center">R</td><td></td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td align="left"></td><td></td><td>L</td><td align="center">R</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td align="left"></td><td></td><td></td><td align="center">L,R</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td align="left"></td><td></td><td>R</td><td align="center">L</td><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>如果select1为&lt;=时，等价于求 &gt;6 和last &lt;= 6，如下所示：</p><table><thead><tr><th>1</th><th align="left">2</th><th>3</th><th>5</th><th align="center">6</th><th>6</th><th>6</th><th>6</th><th>8</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>L</td><td align="left"></td><td></td><td></td><td align="center"></td><td></td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td align="left"></td><td></td><td></td><td align="center"></td><td></td><td>L</td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td align="left"></td><td></td><td></td><td align="center"></td><td></td><td>L</td><td>R</td><td></td><td></td><td></td></tr><tr><td></td><td align="left"></td><td></td><td></td><td align="center"></td><td></td><td></td><td>L,R</td><td></td><td></td><td></td></tr><tr><td></td><td align="left"></td><td></td><td></td><td align="center"></td><td></td><td></td><td>R</td><td>L</td><td></td><td></td></tr></tbody></table><p>场景1：寻找小于target的位置</p><p>场景2：寻找大于target的位置</p><p>场景3：寻找小于等于target的首次出现的位置</p><p>场景4：寻找小于等于target的末次出现的位置</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">// 模板int BinarySearch(int *arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt; target) { // select [&lt;, &lt;=]            left = mid + 1;        } else {            right = mid - 1;        }    }    return right; // select [left, right]}// 4种场景对应的代码：// 场景1：&lt; 6int BinarySearchLowerTarget(int *arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt; target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return right;}// 场景2：&gt; 6int BinarySearchHigherTarget(int* arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt;= target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return left;}// 场景3：&lt;=6, 取首int BinarySearchFirstLowerEqualTarget(int* arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt; target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return left;}// 场景4：&lt;=6, 取尾int BinarySearchLastLowerEqualTarget(int* arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt;= target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return right;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>总结：</strong></p><ol><li><p>L一定是要+1，R一定是-1，因为区间要不断收缩才能退出，如果没有+1，和-1，会导致死循环风险。</p></li><li><p>while (left &lt;= right)中的退出条件是，left &gt; right,出现了翻转，此时right跑到left左边。</p></li><li><p>正对4种场景只需要修改if中的符号和返回值。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Binary Search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NER学习整理</title>
      <link href="/2021/12/31/ner-xue-xi-zheng-li/"/>
      <url>/2021/12/31/ner-xue-xi-zheng-li/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是NER"><a href="#什么是NER" class="headerlink" title="什么是NER"></a>什么是NER</h2><p>命名体识别（Named Entity Recognition, NER），旨在识别文本中感兴趣的实体，如位置、组织和时间。已识别的实体可以在各种下游应用程序中使用，如根据患者记录去识别和信息提取系统，也可以作为机器学习系统的特性，用于其他自然语言处理任务。比如下面的例子：「Michael Jeffrey Jordan」是一个「Person」实体，「Brooklyn」是一个「Location」，「NewYork」也是一个「Location」。命名实体识别本质上是一个模式识别任务, 即给定一个句子, 识别句子中实体的边界和实体的类型是自然语言处理任务中一项重要且基础性的工作。</p><h2 id="NER相关综述类论文"><a href="#NER相关综述类论文" class="headerlink" title="NER相关综述类论文"></a>NER相关综述类论文</h2><ul><li><a href="https://arxiv.org/pdf/2101.11420.pdf">2021-Recent Trends in Named Entity Recognition (NER)</a></li><li><a href="https://arxiv.org/pdf/1812.09449.pdf">2020- Survey on Deep Learning for Named Entity Recognition</a></li><li><a href="https://arxiv.org/pdf/1910.11470.pdf">2019-A survey on recent advances in named entity recognition from deep learning models</a></li><li><a href="https://aclanthology.org/P18-3006.pdf">2018-Recognizing complex entity mentions: A review and future directions</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S1574013717302782">2018-Recent named entity recognition and classification techniques: A systematic review </a></li><li><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.714.342&amp;rep=rep1&amp;type=pdf">2013-Named entity recognition: fallacies, challenges and opportunities</a></li><li><a href="https://time.mk/trajkovski/thesis/li07.pdf">2007-A survey of named entity recognition and classification</a></li></ul><h2 id="NER数据集、评测方法、工具库"><a href="#NER数据集、评测方法、工具库" class="headerlink" title="NER数据集、评测方法、工具库"></a>NER数据集、评测方法、工具库</h2><h3 id="NER相关的数据集"><a href="#NER相关的数据集" class="headerlink" title="NER相关的数据集"></a>NER相关的数据集</h3><p>NER相关数据集可以参考：<a href="https://link.zhihu.com/?target=https://github.com/SimmerChan/corpus">SimmerChan/corpus</a></p><h3 id="NER评测方法"><a href="#NER评测方法" class="headerlink" title="NER评测方法"></a>NER评测方法</h3><p>命名实体识别评测方式分为两种，一是通用的基于token标签进行直接评测，二是考虑实体边界+实体类型的评测，实际中往往是采用后者的评测方法，按照实体边界+实体类型的评测，分为exact match和relaxed match两类。对于exact match来说，只有命名实体的开始和结束索引以及类型完全正确才算正确，relaxed match只需要类型正确且将ground truth覆盖即可，基于此有Precision，Recall，F-Score等标准。另外，对于多实体类型，经常需要评估模型跨实体类的表现，这时候会用到macro-averaged F-Score和micro-average F-Score。</p><h3 id="NER标注方法"><a href="#NER标注方法" class="headerlink" title="NER标注方法"></a>NER标注方法</h3><p>一般来说，不同的NER任务会有不同的Tags，但都基本基于以下两种：</p><ul><li>BIO(B-begin，I-inside，O-outside)</li><li>BIOES(B-begin，I-inside，O-outside，E-end，S-single)</li></ul><p>其中，B 开始位置、I 中间位置、O 其他类别、E表示结束位置、S 单字表示一个实体。</p><h3 id="NER工具库"><a href="#NER工具库" class="headerlink" title="NER工具库"></a>NER工具库</h3><p>《Survey on Deep Learning for Named Entity Recognition》对学术界和工业界一些NER工具进行汇总，工具中通常都包含预训练模型，可以直接在自己的语料上做实体识别。不过一般研究使用的话（所定义实体类别与工具预定的不符），还需要依据待抽取领域语料再训练模型，或重新训练模型。</p><h2 id="输入的分布式表示"><a href="#输入的分布式表示" class="headerlink" title="输入的分布式表示"></a>输入的分布式表示</h2><p>分布式表示是使用维度向量表示词的信息，每一维都表示一种特征。在NER模型中使用的三种分布式表示：字级、字符级和混合表示。</p><ul><li><em><strong>Word - Level Representation：</strong></em>通常是预训练的词表示。可以使用bag of - words (CBOW) 或 continuous skip-gram models。CBOW本质是通过context word来预测target word，复杂度$O(vocabulary length)$，skip-gram的训练过程则和CBOW相反，但是复杂度为$O(vocabulary length * window length)$。但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。</li><li><em><strong>Character - Level Representation：</strong></em>字符级表示对于利用显式子词级信息（如前缀和后缀）非常有用。字符级表示法的另一个优点是，它不需要词汇表并且还能避免在模型输出上的计算瓶颈。此外，基于字符的模型能够推断不可见词的表示并且能弹性地处理拼写错误，并共享语素级规则的信息，可以解决OOV等信息。提取字符级别嵌入的方法有基于CNN的模型和基于RNN的模型。其中CNN中字符经过卷积池化获得字符级别的嵌入；RNN中为每个字符预测生成标签的分布，词级别的标签由字符级别的标签计算得到。Word - Level Representation和Character - Level Representation通常连接在一起表示。</li><li><em><strong>Hybrid Representation：</strong></em>除了单词级和字符级表示外，一些研究还将其他信息（如地名录、词汇相似性、语言依赖性和视觉特征纳入单词的最终表示，然后再输入上下文编码层。添加额外信息可能会导致NER性能的改善，代价是损害这些系统的通用性。相当于联合使用基于深度学习的方法和基于特征的方法。BiLSTM-CRF模型中融合4种特征：spelling features, context features, word embeddings, gazetteer features。</li></ul><h2 id="上下文编码器体系结构"><a href="#上下文编码器体系结构" class="headerlink" title="上下文编码器体系结构"></a>上下文编码器体系结构</h2><p>表示单词的一个简单选项是热向量表示。在一个热向量空间中，两个不同的单词具有完全不同的表示形式，并且是正交的。分布式表示表示低维实值密集向量中的单词，其中每个维度表示潜在特征。分布式表示从文本中自动学习，捕获单词的语义和句法属性，而这些属性在文本中并不显式出现，所以需要Context Encoder Architectures去学习这些属性。有主流的方法有：</p><ul><li><p><em><strong>Convolutional Neural Networks：</strong></em>使用每个词的嵌入作为输入，卷积层计算得到本地特征，将所有局部特征综合之后得到输入的全局特征。其中提取全局特征的方法可以对句中的位置进行最大或平均运算，然后将句子的全局特征将输入到tag decoder里计算所有可能标签的分布。即每个单词的标签由整个句子决定。ID-CNNs适用于大规模文本和结构化预测。</p></li><li><p><em><strong>Recurrent Neural Networks：</strong></em>RNN通常使用两大变种GRU、LSTM，利用门机制缓解长程依赖的问题。双向RNNs有效利用过去的信息和未来的信息，适合建模上下文存在依赖的表示。Recurrent是时间维度的展开（如下图所示），代表信息在时间维度从前往后的的传递和积累，可以类比markov假设，后面的信息的概率建立在前面信息的基础上，在神经网络结构上表现为后面的神经网络的隐藏层的输入是前面的神经网络的隐藏层的输出。</p></li><li><p><em><strong>Recursive Neural Networks：</strong></em>循环神经网络也是递归神经网路的一种。一般而言，循环神经网络是在时间维度上的递归，而递归神经网络是在结构上的递归。递归神经网络是一种自适应的非线性模型，能够按照拓扑顺序遍历给定结构来学习深层结构信息。</p></li><li><p><em><strong>Neural Language Models：</strong></em>通过前向/后向神经语言模型在分别给定前面/后面的词情况下预测当前词，通过根据双向RNN得到当前位置的表示，这种表示在序列标注任务中很实用。ELMo通过双层的双向语言模型可以词的复杂特定，例如，语法语义等。</p></li><li><p><em><strong>Deep Transformer：</strong></em>transformer使用堆叠的自注意力和逐点连接的全连接层构建编码器和解码器，彻底消除CNN和RNN。Bert模型（双向Transformer的Encoder）通过同时使用上下文可以更好的捕捉词语和句子级别的representation。</p></li></ul><h2 id="标签解码器结构"><a href="#标签解码器结构" class="headerlink" title="标签解码器结构"></a>标签解码器结构</h2><p>输入上下文相关的表示，输出标签序列。常用的解码器结构包括：MLP+softmax、CRFs、RNN和Pointer Network。</p><ul><li><em><strong>多层感知机+softmax：</strong></em>多层感知机和softmax层将序列标注任务转换成为多分类任务，为每个词标注tag。</li><li><em><strong>CRFs：</strong></em>基于深度学习的CRFs主要用在双向LSTM或者CNN层上，例如CoNLL03, OntoNotes5.0。CRFs的缺陷在于无法充分利用分段信息，因为分段信息无法完全使用词嵌入表示。改进方法是直接建模分段的表示，而不是词的表示。</li><li><em><strong>RNN：</strong></em>当命名实体类别很多时，使用RNN作为解码器表现更好，且更容易训练。</li><li><em><strong>Pointer Networks：</strong></em>指针网络应用在RNN上，输出位置下标集合，形成离散的位置范围。使用softmax概率分布作为指针，表示可变长的词典。NER中，PN将输入的序列分段，然后输出标签。</li></ul><h2 id="NER主要方法"><a href="#NER主要方法" class="headerlink" title="NER主要方法"></a>NER主要方法</h2><ol><li><em><strong>基于专家规则的方法：</strong></em>一般情况下，由于使用领域内的特定规则和不完整的词典，这种方法会出现precision高，recall低的情况。这种方法非常适合字典有限且内容量不是过多的的场景，而当已知据大多数词时，基于规则的NER方法效果很好。但是在一些内容种类和数量不断拓展的领域，这种方法需要耗费大量的人力成本和精力成本，并且可移植性差。</li><li><em><strong>无监督学习方法：</strong></em>典型的无监督NER方法是clustering，通过context相似度抽取命名实体。聚类算法基于上下文的相似性将命名实体集聚在特定的聚类簇中。常用到的特征或者辅助信息有词汇资源、语料统计信息（TF-IDF）、浅层语义信息（分块NP-chunking）等。该项技术往往依赖于词汇资源例如WordNet，依赖于词汇模式，依赖于统计等。</li><li><em><strong>监督学习方法：</strong></em>监督学习方法可以将NER转换为多分类问题或者序列标注任务。监督学习往往需要进行一些特征工程来提取特定的特征。监督学习中特征工程的设计非常重要，使用特征向量表示文本的摘要信息，一般有三种类型的特征：（1）布尔型特征 （2）数值型特征 （3）类别特征。模型选择有：隐马尔可夫模型、决策树、最大熵模型、最大熵马尔科夫模型、支持向量机、条件随机场。</li><li><em><strong>深度学习方法：</strong></em>将深度学习技术应用于NER有三个核心优势。首先，NER受益于非线性转换，它生成从输入到输出的非线性映射。与线性模型（如对数线性HMM和线性链CRF）相比，基于DL的模型能够通过非线性激活函数从数据中学习复杂的特征。第二，深度学习节省了设计NER特性的大量精力。传统的基于特征的方法需要大量的工程技术和领域专业知识。另一方面，基于DL的模型可以有效地从原始数据中自动学习有用的表示形式和潜在因素。第三，深层神经网络模型可以通过梯度下降法在端到端范式中进行训练。这一特性使我们能够设计可能复杂的NER系统。基于深度学习的NER模型有很多种，下面将列举几种深度学习解决NER的例子。</li></ol><h3 id="BiLSTM-CRF"><a href="#BiLSTM-CRF" class="headerlink" title="BiLSTM+CRF"></a>BiLSTM+CRF</h3><p>BiLSTM+CRF结构如下图所示：首先，句中的每个单词是一条包含词嵌入和字嵌入的词向量，词嵌入通常是事先训练好的，字嵌入则是随机初始化的。所有的嵌入都会随着训练的迭代过程被调整。其次，BiLSTM-CRF的输入是词嵌入向量，输出是每个单词对应的预测标签。</p><p><img src="https://pic1.zhimg.com/80/v2-80fc94217cfd3ae9d85636859a8b27bc_1440w.jpg"></p><p>BiLSTM层的输入表示该单词对应各个类别的分数。如W0，BiLSTM节点的输出是1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) and 0.05 (O)。这些分数将会是CRF层的输入。所有的经BiLSTM层输出的分数将作为CRF层的输入，类别序列中分数最高的类别就是预测的最终结果。</p><h5 id="为什么使用CRF层"><a href="#为什么使用CRF层" class="headerlink" title="为什么使用CRF层"></a>为什么使用CRF层</h5><p>使用CRF层的的原因是LSTM在进行序列建模时只考虑了输入序列的信息，无法对标签转移关系进行建模，以BIO标注为例，可能会出现如下问题：</p><p><strong>例如输入序列为：“北京烤鸭“</strong></p><p><strong>理想标注结果：“B-地名 ｜ I-地名 ｜B-食物 ｜ I-食物”</strong></p><p><strong>可能出现结果：“B-食物 ｜ I-地名 ｜B-地名 ｜ I-食物”</strong></p><p>这时候就需要考虑对标签转移矩阵和标签转移关系进行建模。对于状态关系转移这种问题，常见的模型有HMM（生成模型）和CRF（判别模型）两种。不过在这里选择CRF这种判别式模型。这是因为NER问题中，标签的特性都是固定的，如”B-食物“后只能加“I-食物”或者“O”，不能是”B-食物“后加“I-地名“。而生成模型有更高的泛化能力和普适性，也就意味着更高的计算复杂度，CRF能帮助发现数据中新的特性。下面一个例子将展示判别模型和生成模型的区别：</p><p><strong>假如有4个samples：</strong></p><table><thead><tr><th></th><th>sample1</th><th>sample2</th><th>sample3</th><th>Sample4</th></tr></thead><tbody><tr><td>x</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td>y</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table><p><strong>生成模型的预测主要基于联合概率：</strong></p><p>$$<br>\sum_{}P(x,y)=1<br>$$<br>​    </p><table><thead><tr><th>生成模型</th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=0</td><td>1/2</td><td>0</td></tr><tr><td>x=1</td><td>1/4</td><td>1/4</td></tr></tbody></table><p><strong>而判别模型的预测主要基于后验概率：</strong><br>$$<br>\sum_{y}P(y|x)=1<br>$$</p><table><thead><tr><th>判别模型</th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=0</td><td>1</td><td>0</td></tr><tr><td>x=1</td><td>1/2</td><td>1/2</td></tr></tbody></table><p>因此结合LSTM和CRF，可以在LSTM进行序列建模的基础上对标签转移矩阵和标签转移关系进行建模。CRF层的作用是加入一些约束来保证最终预测结果是有效的。这些约束可以在训练数据时被CRF层自动学习得到。</p><p>可能的约束条件有：</p><ul><li><p>句子的开头应该是“B-label”或“O”，而不是“I- label？”。</p></li><li><p>“B-label1， I-label2， I-label3…”，在该模式中，类别1,2,3应该是同一种实体类别。比如，</p><p>“B-Person，I-Person” 是正确的，而“B-Person I-Organization”则是错误的。</p></li><li><p>“O，I-label”是错误的，命名实体的开头应该是“B-”而不是“I-”。</p></li></ul><p>有了这些有用的约束，错误的预测序列将会大大减少。</p><h5 id="CRF层损失函数"><a href="#CRF层损失函数" class="headerlink" title="CRF层损失函数"></a>CRF层损失函数</h5><p>CRF损失函数由两部分组成，<strong>发射分数(Emission Score)</strong> 和 **转移分数(Transition Score)**。真实路径的分数应该是所有路径中分数最高的。</p><h6 id="发射分数"><a href="#发射分数" class="headerlink" title="发射分数"></a>发射分数</h6><p>分别对单词和类别建立索引。如$i $是 word 的索引，$y$ 是 label 的索引，则发射分数为$X_i,_y$</p><p>$X_1,_0$意思就是词汇表第二个单词$W_1$转移到第一个标签$L_0$的概率。</p><h6 id="转移分数"><a href="#转移分数" class="headerlink" title="转移分数"></a>转移分数</h6><p>使用 $T_{y_i},_{y_j}$来表示转移分数。$y$代表不同Lable。为了使 transition 评分矩阵更健壮，需要添加另外两个标签，START 和 END。START 是指一个句子的开头，而不是第一个单词。END 表示句子的结尾。</p><p>建一个transition得分矩阵，用来存储所有标签之间的所有得分。该矩阵是 BiLSTM-CRF 模型的一个参数。在训练模型之前，可以随机初始化矩阵中的所有 transition 分数。所有的随机分数将在训练过程中自动更新。换句话说，CRF 层可以自己学习这些约束，不需要手动构建矩阵。随着训练迭代次数的增加，分数会逐渐趋于合理。</p><h6 id="路径分数"><a href="#路径分数" class="headerlink" title="路径分数"></a>路径分数</h6><p>$S_i=Emission Score + Transition Score$</p><p>$P_{real}=e^{s_{real}}$</p><p>$P_{total}=P_1+P_2+P_3+…+P_n=e^{s_1}+e^{s_2}+e^{s_3}+…+e^{s_n}$</p><h6 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h6><p>$LossFunction=\frac{P_{real}}{P_{total}}$</p><h3 id="IDCNN-CRF"><a href="#IDCNN-CRF" class="headerlink" title="IDCNN+CRF"></a>IDCNN+CRF</h3><p>尽管BILSTM在NER任务中有很好的表现，但是却不能充分利用GPU的并行性，因此出现了一种新的NER模型方案IDCNN+CRF。</p><p>尽管传统的CNN有明显的计算优势，但是传统的CNN在经过卷积之后，末梢神经元只能得到输入文本的一小部分信息，为了获取上下文信息，需要加入更多的卷积层，导致网络越来越深，参数越来越多，容易发生过拟合。</p><p>IDCNN的改进基于CNN做出了改进，卷积操作更换为空洞卷积，那么为什么要做空洞卷积呢？因为正常CNN卷积之后会接一个池化层，池化层会损失信息，降低精度，那么不如不加池化层，直接在卷积操作中增大感受野，所以就有了空洞卷积（如下图所示）。</p><img src="https://pic4.zhimg.com/v2-4959201e816888c6648f2e78cccfd253_b.jpg" alt="" style="zoom:67%;"><p>IDCNN为这片filter增加了一个dilation width，作用在输入矩阵的时候，会skip掉所有dilation width中间的输入数据；而filter矩阵本身的大小仍然不变，这样filter获取到了更广阔的输入矩阵上的数据，看上去就像是“膨胀”了一般。dilated width会随着层数的增加而指数增加。这样随着层数的增加，参数数量是线性增加的，而receptive field却是指数增加的，可以很快覆盖到全部的输入数据。图像的空洞卷积如下图所示：</p><img src="截屏2021-12-10 上午10.53.09.png" alt="" style="zoom: 33%;"><p>对应到文本中的卷积操作如下图所示：</p><img src="截屏2021-12-10 上午10.55.50.png" alt="" style="zoom:33%;"><p>IDCNN获取特征之后，与BiLSTM一样，将结果输入到CRF层，利用维特比解码，得到最终的标签。</p><h3 id="Bert-BiLSTM-CRF"><a href="#Bert-BiLSTM-CRF" class="headerlink" title="Bert+BiLSTM+CRF"></a>Bert+BiLSTM+CRF</h3><p>BERT的全称为Bidirectional Encoder Representation from Transformers，由谷歌2018年提出来，是一个预训练的语言表征模型。它采用新的masked language model（MLM），以致能生成深度的双向语言表征。BERT通过微调的方法可以灵活的应用到下游业务，所以可以考虑使用Bert作为embedding层，将特征输入到Bilstm+CRF中，以谋求更好的效果。</p><p>该模型有以下主要优点：</p><p>1）以往的预训练模型的结构会受到单向语言模型<em>（从左到右或者从右到左）</em>的限制，因而也限制了模型的表征能力，使其只能获取单方向的上下文信息。而BERT利用MLM进行预训练并且采用深层的双向Transformer组件/（单向的Transformer一般被称为Transformer decoder，而双向的Transformer则被称为Transformer encoder）来构建整个模型，因此最终生成能融合左右上下文信息的深层双向语言表征。Transformer结构如下：</p><img src="截屏2021-12-10 上午11.36.43.png" alt="" style="zoom: 30%;"><p>而Transformer又可以进行堆叠，形成一个更深的神经网络：</p><img src="截屏2021-12-10 上午11.51.32.png" alt="" style="zoom: 20%;"><p>MLM是BERT能够不受单向语言模型所限制的原因。简单来说就是以15%的概率用mask token （[MASK]）随机地对每一个训练序列中的token进行替换，然后预测出[MASK]位置原有的单词。然而，由于[MASK]并不会出现在下游任务的微调（fine-tuning）阶段，因此预训练阶段和微调阶段之间产生了不匹配（预训练的目标会令产生的语言表征对[MASK]敏感，但是却对其他token不敏感）。因此BERT采用了以下策略来解决这个问题：</p><p>首先在每一个训练序列中以15%的概率随机地选中某个token位置用于预测，假如是第i个token被选中，则会被替换成以下三个token之一</p><ul><li>80%的时候是[MASK]。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>[MASK]</strong></li><li>10%的时候是随机的其他token。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>apple</strong></li><li>10%的时候是原来的token<em>（保持不变，个人认为是作为2）所对应的负类）</em>。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>hairy</strong></li></ul><p>再用该位置对应的$T_i$去预测出原来的token（<em>输入到全连接，然后用softmax输出每个token的概率，最后用交叉熵计算loss）</em>。该策略令到BERT不再只对[MASK]敏感，而是对所有的token都敏感，以致能抽取出任何token的表征信息。</p><p>2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p><h3 id="FLAT"><a href="#FLAT" class="headerlink" title="FLAT"></a>FLAT</h3><img src="https://pic4.zhimg.com/80/v2-a317ee70e90df352fe518386fd0b1c3b_1440w.jpg" alt="" style="zoom:50%;"><p>FLAT的基本思想来源于Lattice-LSTM（如上图），Lattice-LSTM采取的RNN结构无法捕捉长距离依赖，同时引入词汇信息是有损的，同时动态的Lattice结构也不能充分进行GPU并行。为解决计算效率低下、引入词汇信息有损的这两个问题，FLAT基于Transformer结构进行了两大改进：</p><p><strong>改进1：Flat-Lattice Transformer，无损引入词汇信息</strong></p><p>FLAT设计了一种巧妙position encoding来融合Lattice 结构，具体地情况如下上图所示，对于每一个字符和词汇都构建两个head position encoding 和tail position encoding，这种方式可以重构原有的Lattice结构。</p><img src="截屏2021-12-10 下午1.48.04.png" alt="" style="zoom:50%;"><p><strong>改进2：相对位置编码，让Transformer适用NER任务</strong></p><p>Lattice结构由不同长度的跨度组成。为了编码跨域之间的交互，FLAT提出了跨域的相对位置编码。对于两个空间的 $x_i $和 $x_j$，它们之间有三种关系:交集、包含和分离，由它们的正面和反面决定。这三种关系不是直接对其编码，而是使用密集向量来建模它们的关系。它是通过对头部和尾部信息的连续变换来计算的。因此，相对位置编码不仅可以表示两个符号之间的关系，还可以表示更详细的信息，如字符和单词之间的距离。让$head[i]$和$tail[i]$表示跨度$x_i$的头和尾位置。可以用四种相对距离来表示$x_i $和 $x_j$和 之间的关系,具体公式如下所示：<br>$$<br>d_{ij}^{hh}=head[i]-head[j]\<br>d_{ij}^{ht}=head[i]-tail[j]\<br>d_{ij}^{th}=tail[i]-head[j]\<br>d_{ij}^{tt}=tail[i]-tail[j]\<br>$$</p><h2 id="中文NER任务特点"><a href="#中文NER任务特点" class="headerlink" title="中文NER任务特点"></a>中文NER任务特点</h2><p>近年来，引入词汇信息逐渐成为提升中文NER指标的重要手段。不同于英文NER，中文NER通常以字符为单位进行序列标注建模。这主要是由于中文分词存在误差，导致基于字符通常要好于基于词汇（经过分词）的序列标注建模方法。但是中文NER仍然需要词汇信息，因为引入词汇信息往往可以强化实体边界，特别是对于span较长的实体边界更加有效。此外引入词汇信息也是一种增强方式。对于NLP分类任务增益明显的数据增强方法，往往不能直接应用于NER任务，并且指标增益也极为有限。相反，引入词汇信息的增强方式对于小样本下的中文NER增益明显。</p><p>下面列举两种词汇增强的方法：</p><ol><li><em><strong>词向量&amp;词汇列表：</strong></em>利用一个具备良好分词结果的词向量；或者不再利用词向量，仅利用词汇或者实体边界信息，通常可通过图网络提取相关信息。这种增强方式主要分为Dynamic Architecture和Adaptive Embedding两种。</li><li><em><strong>分词器：</strong></em>单一的分词器会造成边界错误，可以引入多源分词器并pooling不同分词结果。</li></ol><h2 id="NER的应用"><a href="#NER的应用" class="headerlink" title="NER的应用"></a>NER的应用</h2><ul><li>知识图谱]</li><li>文本理解</li><li>对话系统</li><li>舆情分析</li><li>槽位抽取</li></ul><h2 id="NER未来的研究方向"><a href="#NER未来的研究方向" class="headerlink" title="NER未来的研究方向"></a>NER未来的研究方向</h2><ol><li>多类别实体</li><li>嵌套实体</li><li>实体识别与实体链接联合任务</li><li>利用辅助资源进行基于深度学习的非正式文本NER</li><li>NER模型压缩</li><li>深度迁移学习 for NER</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>Strubell, Emma, et al. “<a href="https://aclanthology.org/D17-1283.pdf">Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.</a>” <em>arXiv preprint arXiv:1702.02098</em> (2017).</li><li>Vaswani, Ashish, et al. “<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need.</a>“ <em>Advances in neural information processing systems</em>. 2017.</li><li>Devlin, Jacob, et al. “<a href="https://arxiv.org/pdf/1810.04805.pdf">Bert: Pre-training of deep bidirectional transformers for language understanding.</a>“ <em>arXiv preprint arXiv:1810.04805</em> (2018).</li><li>Li, Jing, et al. “<a href="https://arxiv.org/pdf/1812.09449.pdf">A survey on deep learning for named entity recognition.</a>“ <em>IEEE Transactions on Knowledge and Data Engineering</em> (2020).</li><li>Li, Xiaonan, et al. “<a href="https://arxiv.org/pdf/2004.11795.pdf">FLAT: Chinese NER using flat-lattice transformer.</a>“ <em>arXiv preprint arXiv:2004.11795</em> (2020).</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ASR错误自动识别与纠正</title>
      <link href="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/"/>
      <url>/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h2><p>随着科技的发展，越来越多种类的智能语音产品出现。虽然智能语音产品种类，但是他们依赖着有着像汽车发动机一样重要的内核——语音对话系统（Spoken dialogue systems）。一个好的语音对话系统可以帮助用户高效的解决一些问题，或是可以和用户进行社交对话交流。下面这张图展示了语音对话系统的一般结构：</p><img src="截屏2021-12-15 上午10.29.52.png" alt="" style="zoom: 50%;"><p>在一般的语音对话系统中都会有一个唤醒功能来启动系统，如APPLE产品中的“Hi，Siri”。系统在唤醒后会开启Voice Activity Detection ( VAD ) 状态来检测当前有没有说话声音，如果有声音则会将语音记录下来传递到ASR模块。通过ASR模块将语音转文本后，这些信息讯号会依次经过三个不同的模块（红色）处理才会反馈回人类。这三个不同的模块大致的任务可以理解为——理解信息，规划反馈，生成反馈。在语音对话系统中，第一步首先要保证输入信息的准确性，这项任务相对于上面三个模块处于上游。因为在语音交互中，由于ASR等技术的缺陷，很容易出现记录噪声或者识别错误等情况，这会影响下游任务的准确性。 基于以上原因，准确的输入信息是保证下游任务质量的基础。</p><p>在语音对话系统的信息输入中除了声音输入，还有使用文本输入的情况。中文文本纠错任务，常见错误类型包括：</p><ul><li>谐音字词，如 配副眼睛-配副眼镜</li><li>混淆音字词，如 流浪织女-牛郎织女</li><li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li><li>字词补全，如 爱有天意-假如爱有天意</li><li>形似字错误，如 高梁-高粱</li><li>中文拼音全拼，如 xingfu-幸福</li><li>中文拼音缩写，如 sz-深圳</li><li>语法错误，如 想象难以-难以想象</li></ul><p>在识别这两种信息的错误时，其方法也是不一样的的。虽然一般的语音对话系统在处理声音信息时，也是将声音讯号通过ASR技术转为文本内容，但是这种经过转换的文本内容和普通的文本内容在纠错时面临的问题是不同的，一种是根据发音的相似性纠错，另一种是根据字形的相似性纠错，所以这两种信息出错的数据分布也是不同的。此外，由于声音的约束性，一般来说普通文本信息的错误范围会比声音转换的文本信息更广。不过，在本篇文章中主要探讨针对于ASR识别后的文本数据纠错模型。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a><strong>解决方法</strong></h2><h3 id="基于规则的解决思路"><a href="#基于规则的解决思路" class="headerlink" title="基于规则的解决思路"></a><strong>基于规则的解决思路</strong></h3><ol><li>中文纠错分为两步走，第一步是错误检测，第二步是错误纠正；</li><li>错误检测部分先通过中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度（语言模型困惑度（ppl）检测某字的似然概率值低于句子文本平均值，则判定该字是疑似错别字的概率大）和词粒度（切词后不在词典中的词是疑似错词的概率大）两方面检测错误， 整合这两种粒度的疑似错误结果，形成疑似错误位置候选集；</li><li>错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。</li></ol><h3 id="基于深度学习的解决思路"><a href="#基于深度学习的解决思路" class="headerlink" title="基于深度学习的解决思路"></a><strong>基于深度学习的解决思路</strong></h3><ol><li>端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；</li><li>CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；</li><li>seq2seq模型是使用encoder-decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一。</li></ol><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a><strong>评估方法</strong></h2><p>在纠错领域，准确率的重要性往往比召回率重要的多，所以一般使用F0.5-Score作为评估标准。在科大讯飞的纠错比赛中使用了一种综合的评估方法：$$0.8<em>检测得分 + 0.2</em>纠错得分$$。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a><strong>模型</strong></h2><h3 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a><strong>统计语言模型</strong></h3><h4 id="错误检测"><a href="#错误检测" class="headerlink" title="错误检测"></a><strong>错误检测</strong></h4><ul><li>混淆词典匹配：混淆词典（人工添加）支持纠错和错误改正，如高梁-&gt;高粱</li><li>常用词典匹配：切词后不在常用词典中的词直接放入混淆集</li><li>Ngram语言模型：某个字前后搭配的2gram和3gram的似然概率值低于句子文本的平均ppl值放入混淆集</li></ul><h4 id="候选召回"><a href="#候选召回" class="headerlink" title="候选召回"></a><strong>候选召回</strong></h4><p>可根据任务的不同对混淆集进行音似或形似替换</p><h4 id="候选排序"><a href="#候选排序" class="headerlink" title="候选排序"></a><strong>候选排序</strong></h4><p>基于统计语言模型计算对所有替换过的句子的似然概率，取概率最大的那一个</p><h3 id="ConvSeq2Seq"><a href="#ConvSeq2Seq" class="headerlink" title="ConvSeq2Seq"></a><strong>ConvSeq2Seq</strong></h3><p>Paper: 微软亚洲研究院发表的”<a href="https://arxiv.org/pdf/1807.01270.pdf">Reaching human-level performance in automatic grammatical error correction: An empirical study</a>“中使用了ConvSqe2Sqe，从而首次在英文语法纠错上超过人类水平。</p><p>这篇文章的最大贡献就是提出了Fluency boost learning和Boost inference。因为在过往seq2seq的模型纠错时往往会出现两种问题，一是模型容易受训练数据的影响，对训练数据中没有见过的语法错误，改正能力很差。其二是模型很难一次修复多个错误。Fluency boost learning通过在训练过程中对数据进行增强（如下图），可以让模型看到更多的错误，增强模型的泛化能力。</p><img src="截屏2021-12-20 下午3.31.22.png" alt="" style="zoom:45%;"><p>Boost inference即预测过程中进行增强，，一是多轮预测，二是循环预测。循环预测即从左到右预测和从右到左预测，比如右向左可以有效解决冠词出错问题，左向右可以解决主谓一致出错问题。</p><h3 id="Bert模型"><a href="#Bert模型" class="headerlink" title="Bert模型"></a><strong>Bert模型</strong></h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>BERT模型的工作原理是，在大型语料库（Masked LM任务）上训练BERT模型，然后通过在最后添加一些额外的层来微调我们自己的任务的模型，该模型可以是分类，问题回答或NER等。Bert是通过使用Transformer-Encoder结构，成为了一个较好的的预训练语言表示模型。Bert有两种训练任务，一种是Masked Language Model ( MLM )，另一种是Next Sentence Prediction ( NSP )。通过把Bert模型来当作任务的预训练模型，可以对提取词向量的语义特征，从而提高下有任务的表现。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a><strong>结构</strong></h4><p>Bert模型中与AS纠错任务相关的是MLM部分（如下图所示）， MLM训练阶段有 15% 的token被随机替换为 【MASK】 ( 占位符 )，模型需要学会根据 【MASK】 上下文预测这些被替换的token。</p><img src="截屏2021-12-15 下午4.53.30.png" alt="" style="zoom:67%;"><p>因为在实际训练中任务中没有MASK的情况，如果只利用MASK机制训练无法让模型得到好的迁移，所以模型需要对MASK的方式进行优化。</p><ul><li>10% 的【MASK】会被随机替换成另一个词</li><li>10% 的【MASK】会被还原为正确的词</li><li>80% 的【MASK】保留这个占位符状态</li></ul><h4 id="限制"><a href="#限制" class="headerlink" title="限制"></a><strong>限制</strong></h4><ul><li>此外，由于BERT模型过于庞大，在一些实时要求很高的项目上很难满足时间响应范围的要求，从而导致无法上线。因此需要对BERT进行压缩处理。</li><li>原始的MLM的训练效率是比较低的，因为每次只能mask掉一小部分的token来训练，所以没有足够的能力来检测每个位置是否存在误差（只有15%的错误被找出），所以这种方法的精度不够好。BERT的MASK位置是随机选择的，所以并不擅长侦测句子中出现错误的位置；并且BERT纠错未考虑约束条件，导致准确率低，比如：”今 [明] 天天气怎么样”，MASK的位置是”今”， 那么纠错任务需要给出的结果是”今”。但是由于训练预料中大多数人的query都是”明天天气怎么样”，这样在没有约束的条件下，大概率给出的纠正结果是”明”，虽然句子结构是合理的，但结果显然是不正确的。</li></ul><h3 id="Soft-Masked-BERT纠错模型"><a href="#Soft-Masked-BERT纠错模型" class="headerlink" title="Soft-Masked BERT纠错模型"></a><strong>Soft-Masked BERT纠错模型</strong></h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>Paper：<a href="https://arxiv.org/pdf/2005.07421.pdf">Spelling Error Correction with Soft-Masked BERT</a></p><p>Soft-Masked BERT是字节AI-Lab与复旦大学合作提出了一种中文文本纠错模型。<a href="https://arxiv.org/pdf/2005.07421.pdf">“Soft-Masked BERT”</a>发表在了ACL 2020上。论文首次提出了Soft-Masked BERT模型，主要创新点在于：</p><ul><li>将文本纠错划分为检测网络（Detection）和纠正网络（Correction）两部分，纠正网络的输入来自于检测网络输出。</li><li>以检测网络的输出作为权重，将 masking-embedding以“soft方式”添加到各个字符特征上，即“Soft-Masked”。</li></ul><h4 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a><strong>结构</strong></h4><p>具体来看，模型Input是字粒度的word-embedding，可以使用BERT-Embedding层的输出或者word2vec。检测网络由Bi-GRU组成，充分学习输入的上下文信息，输出是每个位置 i 可能为错别字的概率 p(i)，值越大表示该位置出错的可能性越大。结构图如下：</p><img src="截屏2021-12-16 下午4.19.03.png" alt="" style="zoom: 33%;"><h5 id="检测网络与Soft-Masking"><a href="#检测网络与Soft-Masking" class="headerlink" title="检测网络与Soft Masking"></a><strong>检测网络与Soft Masking</strong></h5><p>Soft Masking 部分，将每个位置的特征以 $ pi $ 的概率乘上masking 字符的特征 $emark$，以 $1-pi$ 的概率乘上原始的输入特征，最后两部分相加作为每一个字符的特征，输入到纠正网络中。</p><h5 id="纠正网络"><a href="#纠正网络" class="headerlink" title="纠正网络"></a><strong>纠正网络</strong></h5><p>纠正网络部分，是一个基于BERT的序列多分类标记模型。检测网络输出的特征作为BERT 12层Transformer模块的输入，最后一层的输出 + Input部分的Embedding特征 $ei$ (残差连接)作为每个字符最终的特征表示。最后，将每个字特征过一层 Softmax 分类器，从候选词表中输出概率最大的字符认为是每个位置的正确字符。</p><h3 id="ELECTRA模型"><a href="#ELECTRA模型" class="headerlink" title="ELECTRA模型"></a><strong>ELECTRA模型</strong></h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>ELECTRA的全称是Efficiently Learning an Encoder that Classifies Token Replacements Accurately。ELECTRA通过类似GAN的结构和新的预训练任务，在更少的参数量和数据下超过了BERT，而且仅用1/4的算力就达到了当时SOTA模型RoBERTa的效果。</p><img src="截屏2021-12-15 下午5.06.53.png" alt="" style="zoom: 60%;"><center>纵轴是GLUE分数，横轴是FLOPs (floating point operations)</center><h4 id="结构-2"><a href="#结构-2" class="headerlink" title="结构"></a><strong>结构</strong></h4><p>ELECTRA最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过。虽然BERT模型中也可以处理RTD任务，但是在随机替换一些词后，BERT在替换预测的效果并不好，因为随机替换过于简单了。ELECTRA使用一个MLM的G-BERT来对输入句子进行更改，然后丢给D-BERT去判断哪个字被改过，如下图所示：</p><img src="截屏2021-12-15 下午5.49.14.png" alt="" style="zoom:50%;"><ul><li>生成器（Generator）的作用是输入一个正确的句子，负责生成一个错误的版本，如”the chef cooked the meal”经过生成器内部随机抽样15%的token进行MASK后，再对这些MASK的位置进行预测，输出结果为”the chef ate the meal”，生成器中语言模型保证了生成的错误句子仍然是比较合理的，只是区别于原始句子。</li><li>判别器 ( ELECTRA ) 是用来判别生成器输出的句子中哪些位置的token被改动了，因此对每个token的位置进行original/replaced标注，如”cooked”变成了”ate”，标注为”repalced”，其余位置相同token标注为”original”，类似于序列标注任务，判别器的输出为0或1。</li></ul><p>虽然模型的结构类似于GAN，但由于GAN应用于文本的困难，所以改模型以最大似然而不是逆向的方式训练生成器。在预训练之后，需要去掉生成器，只针对对下游任务中的鉴别器（ELECTRA模型）进行微调。</p><p>ELECTRA模型的判别器虽然可以检测错误，但模型设计不是为了纠错，而是为了在有限计算资源的条件下能提取更好特征表示，进而得到更好的效果，文章中表示在GLUE数据集上表现明显优于BERT。ELECTRA的一个变体ELECTRA-MLM模型，不再输出0和1，而是预测每个MASK位置正确token的概率。如果词表大小是10000个，那么每个位置的输出就是对应的一个10000维的向量分布，概率最大的是正确token的结果，这样就从原生ELECTRA检测错误变成具有纠错功能的模型。</p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a><strong>工具</strong></h2><h3 id="pycorrector"><a href="#pycorrector" class="headerlink" title="pycorrector"></a><strong><a href="https://github.com/shibing624/pycorrector">pycorrector</a></strong></h3><p>pycorrector依据语言模型检测错别字位置，通过拼音音似特征、笔画五笔编辑距离特征及语言模型困惑度特征纠正错别字。</p><h4 id="Models-in-pycorrector"><a href="#Models-in-pycorrector" class="headerlink" title="Models in pycorrector"></a><strong>Models in pycorrector</strong></h4><ul><li>kenlm：kenlm统计语言模型工具</li><li>rnn_attention模型：参考Stanford University的nlc模型，该模型是参加2014英文文本纠错比赛并取得第一名的方法</li><li>rnn_crf模型：参考阿里巴巴2016参赛中文语法纠错比赛CGED2018并取得第一名的方法</li><li>seq2seq_attention模型：在seq2seq模型加上attention机制，对于长文本效果更好，模型更容易收敛，但容易过拟合</li><li>transformer模型：全attention的结构代替了lstm用于解决sequence to sequence问题，语义特征提取效果更好</li><li>bert模型：中文fine-tuned模型，使用MASK特征纠正错字</li><li>conv_seq2seq模型：基于Facebook出品的fairseq，北京语言大学团队改进ConvS2S模型用于中文纠错，在NLPCC-2018的中文语法纠错比赛中，是唯一使用单模型并取得第三名的成绩</li></ul><h4 id="Detector-in-pycorrector"><a href="#Detector-in-pycorrector" class="headerlink" title="Detector in pycorrector"></a><strong>Detector in pycorrector</strong></h4><ul><li>字粒度：语言模型困惑度（ppl）检测某字的似然概率值低于句子文本平均值，则判定该字是疑似错别字的概率大</li><li>词粒度：切词后不在词典中的词是疑似错词的概率大</li></ul><h4 id="Corrector-in-pycorrector"><a href="#Corrector-in-pycorrector" class="headerlink" title="Corrector in pycorrector"></a><strong>Corrector in pycorrector</strong></h4><ul><li>通过错误检测定位所有疑似错误后，取所有疑似错字的音似、形似候选词</li><li>使用候选词替换，基于语言模型得到类似翻译模型的候选排序结果，得到最优纠正词</li></ul><h4 id="Defect-in-pycorrector"><a href="#Defect-in-pycorrector" class="headerlink" title="Defect in pycorrector"></a><strong>Defect in pycorrector</strong></h4><ul><li>现在的处理手段，在词粒度的错误召回还不错，但错误纠正的准确率还有待提高</li><li>现在的文本错误不再局限于字词粒度上的拼写错误，需要提高中文语法错误检测（CGED, Chinese Grammar Error Diagnosis）及纠正能力</li></ul><h3 id="YoungCorrector"><a href="#YoungCorrector" class="headerlink" title="YoungCorrector"></a><strong>YoungCorrector</strong></h3><p>本项目是参考Pycorrector实现的一套基于规则的纠错系统。 总体来说，基于规则的文本纠错，性能取决于纠错词典和分词质量。目前与 Pycorrector相比，在准确率差不多的情况下，本模型所用的时间会少很多（归功于前向最大匹配替代了直接索引混淆词典）。</p><h4 id="基于规则的中文纠错流程"><a href="#基于规则的中文纠错流程" class="headerlink" title="基于规则的中文纠错流程"></a><strong>基于规则的中文纠错流程</strong></h4><ol><li><p>文本处理</p><ul><li>是否为空</li><li>是否全是英文</li><li>统一编码</li><li>统一文本格式</li></ul></li><li><p>错误检索</p><ul><li><p>从混淆词典/字典中检索</p><p>直接检索</p><p>最大匹配算法</p></li><li><p>分词后，查看是否存在于通用词典</p><p>分词质量</p><p>分词后产生的单字（字本身是ok的，但是在整个句子中是错误的）</p><p>分词后产生的错词（词本身是ok的，但是在整个句子中是错误的）</p><p>分词后的词是否包含其他字母及符号</p></li><li><p>词粒度的 N-gram</p><p>计算局部得分</p><p>MAD(Median Absolute Deviation)</p></li><li><p>字粒度的 N-gram</p><p>计算局部得分</p><p>MAD(Median Absolute Deviation)</p></li><li><p>将候选错误词中的连续单字合并</p></li></ul></li><li><p>候选召回</p><ul><li><p>编辑距离</p><p>错误词的长度大于 1</p><p>只使用编辑距离为 1</p><p>只使用变换和修改，无添加和删除</p><p>对编辑距离得到的词使用拼音加以限制</p></li><li><p>音近词</p></li><li><p>形近词</p></li></ul></li><li><p>纠错排序</p><ul><li>语言模型计算得分（困惑度）</li></ul></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Chen, Wei, et al. “ASR error detection in a conversational spoken language translation system.” <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>. IEEE, 2013.</p><p>Ge, Tao, Furu Wei, and Ming Zhou. “Reaching human-level performance in automatic grammatical error correction: An empirical study.” <em>arXiv preprint arXiv:1807.01270</em> (2018).</p><p>Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” <em>arXiv preprint arXiv:1810.04805</em> (2018).</p><p>Zhang, Shaohua, et al. “Spelling error correction with soft-masked BERT.” <em>arXiv preprint arXiv:2005.07421</em> (2020).</p><p>Clark, Kevin, et al. “Electra: Pre-training text encoders as discriminators rather than generators.” <em>arXiv preprint arXiv:2003.10555</em> (2020).</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASR纠错 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>M1搭建hexo博客</title>
      <link href="/2021/12/31/m1-da-jian-hexo-bo-ke/"/>
      <url>/2021/12/31/m1-da-jian-hexo-bo-ke/</url>
      
        <content type="html"><![CDATA[<p>重度引用：<a href="http://www.aspoir.cn/posts/20210715195018/">Aspoir的《在M1上搭建hexo博客》</a></p><p>简述在M1系统上搭建hexo博客，使用master主题</p><h2 id="本地搭建hexo"><a href="#本地搭建hexo" class="headerlink" title="本地搭建hexo"></a>本地搭建hexo</h2><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p>首先检查是否已经安装</p><pre class="line-numbers language-none"><code class="language-none">npm&gt; zsh: command not found: npm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>提示<code>command not found</code>则未安装，去<a href="https://nodejs.org/zh-cn/download/">node.js官网</a>下载安装包，按照提示安装。</p><p>再次在终端中输入npm显示已经安装</p><pre class="line-numbers language-none"><code class="language-none">npm&gt; Usage: npm &lt;command&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h3><p>在终端中输入</p><pre class="line-numbers language-none"><code class="language-none">sudo npm install hexo-cli -g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>再次输入hexo，若显示如下则安装成功</p><pre class="line-numbers language-none"><code class="language-none">hexoUsage: hexo &lt;command&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="初始化hexo"><a href="#初始化hexo" class="headerlink" title="初始化hexo"></a>初始化hexo</h3><pre class="line-numbers language-none"><code class="language-none">cd Documents/hexo init blog<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>提示安装git直接安装即可</p><h3 id="安装模块"><a href="#安装模块" class="headerlink" title="安装模块"></a>安装模块</h3><pre class="line-numbers language-none"><code class="language-none">cd blognpm install<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><pre class="line-numbers language-none"><code class="language-none">hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://miro.medium.com/max/1400/0*2JvuTw2MYcjCYjo1"></p><h2 id="发布到github上"><a href="#发布到github上" class="headerlink" title="发布到github上"></a>发布到github上</h2><h3 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h3><p>登录github，新建项目</p><p>Respository name必须以自己用户名为开头：<code>用户名.github.io</code></p><p>选择public，点击创建即可</p><h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><pre class="line-numbers language-none"><code class="language-none">git config --global user.name "your name" &lt;-引号内为自定义内容git config --global user.email "mail" &lt;-引号内为自定义内容<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="生成ssh密钥"><a href="#生成ssh密钥" class="headerlink" title="生成ssh密钥"></a>生成ssh密钥</h3><pre class="line-numbers language-none"><code class="language-none">ssh-keygen -t rsa -C "mail" &lt;-之前输入的邮箱<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="进入ssh文件夹"><a href="#进入ssh文件夹" class="headerlink" title="进入ssh文件夹"></a>进入ssh文件夹</h3><p>返回根目录（当前目录为blog）</p><pre class="line-numbers language-none"><code class="language-none">cd ..cd ..cd .ssh/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="生成ssh-keys"><a href="#生成ssh-keys" class="headerlink" title="生成ssh keys"></a>生成ssh keys</h3><pre class="line-numbers language-none"><code class="language-none">vim id_rsa.pub<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>复制粘贴到github上：settings -&gt; SSH and GPG keys -&gt; SSH keys</p><p><a href="https://tva1.sinaimg.cn/large/008i3skNly1gshwdk2h9cj312o0o6q5r.jpg"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gshwdk2h9cj312o0o6q5r.jpg"></a></p><h3 id="测试连接github"><a href="#测试连接github" class="headerlink" title="测试连接github"></a>测试连接github</h3><pre class="line-numbers language-none"><code class="language-none">ssh -T git@github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h3><p>当前位于根目录</p><pre class="line-numbers language-none"><code class="language-none">cd Documents/blog/npm install hexo-deployer-git --save<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="更改配置"><a href="#更改配置" class="headerlink" title="更改配置"></a>更改配置</h3><p>打开blog/_config.yml，在最下方更改配，然后保存，最好使用ssh 来git clone来避免输入账号密码</p><pre class="line-numbers language-none"><code class="language-none">deploy:  type: 'git'  repo: git@github.com:github名字/github名字.github.io.git  branch: master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><pre class="line-numbers language-none"><code class="language-none">hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>本人博客使用<a href="https://github.com/blinkfox/hexo-theme-matery">matery</a>主题</p><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><pre class="line-numbers language-none"><code class="language-none">git clone git@github.com:blinkfox/hexo-theme-matery.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置文档详见<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md</a></p><p>每次更新文章需要做:</p><pre class="line-numbers language-none"><code class="language-none">hexo clean // 清除hexo g //generatehexo d //deploy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="更进一步"><a href="#更进一步" class="headerlink" title="更进一步"></a>更进一步</h2><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/#toc-heading-7">Hexo博客主题之hexo-theme-matery的介绍</a></p><p><a href="https://m3df.xyz/2020/06/13/e9fff968/#toc-heading-84">Hexo进阶：基于matery主题的网站配置教程</a></p><p><a href="https://chen-shang.github.io/2019/08/15/ji-zhu-zong-jie/hexo/hexo-theme-matery-zhu-ti-you-hua/#toc-heading-5">hexo-theme-matery主题优化</a></p><p><a href="https://blog.csdn.net/howareyou2104/article/details/106312703">如何用Hexo优雅的书写文章</a></p><p><a href="https://yanyinhong.github.io/2017/05/02/How-to-insert-image-in-hexo-post/">Hexo博客搭建之在文章中插入图片</a></p>]]></content>
      
      
      <categories>
          
          <category> Installment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Node.js </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
