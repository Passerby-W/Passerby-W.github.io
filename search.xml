<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>DCNN论文解读</title>
      <link href="/2022/09/22/dcnn-lun-wen-jie-du/"/>
      <url>/2022/09/22/dcnn-lun-wen-jie-du/</url>
      
        <content type="html"><![CDATA[<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>DCNN（Dynamic  ConvolutionalNeural Network）是一种sentence mode，它的目的是分析和表示句子的语义内容，方便NLP中分类或生成任务。</p><p>作者通过四个实验对DCNN进行了测试：小尺度二进制和多类情感预测、六向问题分类和远程监控推特情感预测。该网络在前三个任务中取得了优异的性能，在最后一个任务中，相对于最强基线，错误减少了25%以上。</p><h2 id="Background"><a href="#Background" class="headerlink" title="Background"></a>Background</h2><p>下面这些概念是DCNN的核心，我们接下来将对其进行描述。</p><h3 id="Time-Delay-Neural-Networks"><a href="#Time-Delay-Neural-Networks" class="headerlink" title="Time-Delay Neural Networks"></a>Time-Delay Neural Networks</h3><p>CNN的发明是受语音信号处理中时延神经网络（TDNN）影响。TDNN在识别”B”, “D”, “G”三个浊音中得到98.5%的准确率，高于HMM的93.7%。是CNN的先驱。下图就是TDNN的网络结构。</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 10.29.54-3813799.png" alt style="zoom:40%;"></p><p>其中input是一段语音的频谱，也就是声谱图，它是由下雨这些过程得到的，相应频率的幅度值越大，对应的区域就越黑。</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 10.41.28.png" alt style="zoom:33%;"></p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 10.42.30-3814558.png" alt style="zoom:33%;"></p><p>假如把input layer和hidden layer拿出来，如下图所示， 就是一个包含多帧的神经网络，考虑延时为2，则连续的3帧都会被考虑。其中隐含层起到特征抽取的作用，输入层每一个矩形内共有13个小黑点，代表该帧的13维MFCC特征（连续语音 → 预加重 → 分帧 → 加窗 → FFT → Mel滤波器组 → 对数运算 → DCT）。假设有10个隐含层，那么连接的权重数目为<script type="math/tex">3*13*10=390</script></p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-21 22.17.09-3769837.png" alt style="zoom:50%;"></p><p>随着时间向前，我们不断地对语音帧使用上图所示的滤波器，我们可以得到下图的结构。</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-21 22.19.32-3769978.png" alt style="zoom:33%;"></p><p>以上就是延时神经网络的精髓了，其中绿色的线权值相同，红色的线权值相同，蓝色的线权值相同。相当于把滤波器延时。输入与隐层共390个权值变量待确定。</p><p>假如每个隐层矩形内包含10个节点，那么每条棕色的线包含10个权值，假设输出层与隐层的延时为4，则接收5个隐层矩形内的数据，那么隐层与输出层合计权值为<script type="math/tex">10*5*3=150</script>。所以TDNN权值非常少，便于训练。</p><p>但是上面的模型在处理语音问题时，输入的窗口都是固定的，因此无法解决不定长序列的输入。为了解决句子变长的问题，Max-TDNN在获取到的矩阵中在sequence的维度取的最大值，产生新的输入向量。例如，输入矩阵的大小为<script type="math/tex">d*s</script>，通过max操作后形状变为了<script type="math/tex">d*1</script>。如下图所示：</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 10.57.05-3815429.png" alt style="zoom:50%;"></p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 10.57.28-3815452.png" alt style="zoom:50%;"></p><h3 id="Convolution"><a href="#Convolution" class="headerlink" title="Convolution"></a>Convolution</h3><p>首先定义一个一维卷积核长度为<script type="math/tex">m</script>，<script type="math/tex">m</script>的向量为<script type="math/tex">m\in \mathbb{R}^m</script>，输入的sequences表示为<script type="math/tex">s</script>，<script type="math/tex">s\in \mathbb{R}^s</script>。</p><blockquote><p>卷积的公式表示为：<script type="math/tex">c_j=m^Ts_{j-m+1:j}</script></p></blockquote><p>卷积的种类也分为图下两种，narrow types和wide types。其中narrow types需要<script type="math/tex">s>=m</script>，wide types则对<script type="math/tex">s</script>和<script type="math/tex">m</script>长度无特别要求。narrow types产生的向量<script type="math/tex">c\in \mathbb{R}^{s-m+1}</script>，wide types产生的向量<script type="math/tex">c\in \mathbb{R}^{s+m-1}</script>。</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-21 22.01.00-3768877.png" alt style="zoom:50%;"></p><p>Convolution操作的意义在于捕获n-grams的语义。</p><h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 10.59.21.png" alt style="zoom: 50%;"></p><p>DCNN创新的点在于使用了Dynamic k-Max Pooling这种pooling技术，Dynamic k-Max Pooling技术的核心就是先确定最上层的feature map上每个feature上的max k的个数，然后通过一个公式计算中间层的max k的个数。论文中使用的公式如下所示，L为总层数，l为当前层数。</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 11.06.01-3815969.png" alt style="zoom:43%;"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import torchdef dynamic_k_max_pooling(x, layers_nums, layer_index, s_dim, k_top):    temp_num = int((layers_nums - layer_index) / layers_nums * x.size()[s_dim])    k = temp_num if temp_num &gt; k_top else k_top    return x.topk(k, dim=s_dim)[0]<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>Folding层进行折叠操作，该层不会增加参数数量，简单的将两个维度进行求和，使输入在embed_size维度上减半，论文中还提到这样做的好处是可以再全连接层之前就考虑输入的行与行之间的关联。</p><h2 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h2><p>总体论文读下来，感觉和text-cnn差不多原理，甚至不如text-cnn的操作简单。不同于DCNN的<script type="math/tex">d*s</script>输入，text-cnn的输入形状为<script type="math/tex">s*d</script>。此外，text-cnn不用k top的池化层，直接进行max pooling就可以，最后连接所有卷积后的一维feature map池化后的元素送入全连接层。个人感觉来说，text-cnn和DCNN基本属于一个模型。</p><p><img src="/2022/09/22/dcnn-lun-wen-jie-du/截屏2022-09-22 11.49.39-3818593.png" alt style="zoom:30%;"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.cs.toronto.edu/~fritz/absps/waibelTDNN.pdf">Phoneme recognition using time-delay neural networks</a></p><p><a href="https://blog.csdn.net/zouxy09/article/details/9156785">语音信号处理之（四）梅尔频率倒谱系数（MFCC）</a></p><p><a href="https://arxiv.org/pdf/1404.2188.pdf">A Convolutional Neural Network for Modelling Sentences</a></p><p><a href="https://arxiv.org/pdf/1408.5882.pdf">Convolutional Neural Networks for Sentence Classification</a></p>]]></content>
      
      
      <categories>
          
          <category> Paper </category>
          
      </categories>
      
      
        <tags>
            
            <tag> CNN </tag>
            
            <tag> DCNN </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>高斯过程</title>
      <link href="/2022/09/01/gao-si-guo-cheng/"/>
      <url>/2022/09/01/gao-si-guo-cheng/</url>
      
        <content type="html"><![CDATA[<p>高斯过程(Gaussian process)，简单的说，就是一系列关于连续域（时间或空间）的随机变量的联合，而且针对每一个时间或是空间点上的随机变量都是服从高斯分布的。</p><p>下图就是一个大概的高斯过程的步骤：</p><ol><li>首先先设定均值，然后随机生成一些先验的函数。</li><li>根据观测数据得到后验函数。</li><li>若干这些函数的曲线共同构成了高斯过程给出的置信区间。</li></ol><p><img src="/2022/09/01/gao-si-guo-cheng/截屏2022-09-01 14.54.32.png" alt style="zoom:50%;"></p><p>上图一共有5条不同颜色的线，每一条曲线代表了一个GP的样本。首先来看其中的蓝色的线，对应x轴上找一个点，比如x=2大概对应的的y轴约为0.6左右。它所代表的是对应x=2时的一个随机变量$X(x=2)$”正好”出现的点罢了，本质上对应x=2的是一个随机变量而不是一个确定的点。另外对应于任一时刻的随机变量还需要服从一个确定的高斯分布（并不是说所有时刻对应的随机变量需要服从同一个高斯，只是服从的高斯分布的具体参数只跟时刻有关）。除此之外，若是GP的样本，还需要满足的是，任意个点（对应不同时刻的随机变量）联合需要服从多元高斯分布。<strong>因此GP的分布就是对于时间域上所有随机变量的联合分布。</strong></p><p>既然是满足多元高斯分布，那么必定需要一个确定的mean和确定的covariance 吧。反过来说，若是给定一个mean和covariance，那么自然不同时刻的随机变量的出现的整体位置和相关情况也就被限定住。<strong>因此，一个GP可以被mean和covariance function共同唯一决定</strong>。</p><p>至于covariance function么，这个才是之后在machine learning的应用中被广为探讨的部分，因而就被称为了核函数kernel，<strong>原因就是它捕捉了不同输入点之间的关系，并且反映在了之后样本的位置上。这样的话，就可以做到，利用点与点之间关系，以从输入的训练数据预测未知点的值</strong>。</p><p>高斯过程是一种无参数方法，只通过训练得到，不用关心也不用显式的指定模型长什么样子。</p><p><strong>自然底数</strong></p><blockquote><p>$e=(1+1/n)^n\approx2.71828…$</p></blockquote><p><strong>协方差矩阵</strong></p><blockquote><p>$\sum=Cov(X_i,X_j)=E[(X_i-\mu_i)(X_j-\mu_j)^T]$</p><p>可以一定程度反映两组分布的线性相关关系</p><p>若两组分布独立，那他们的协方差矩阵在非对角线位置都应该是0</p><p><img src="/2022/09/01/gao-si-guo-cheng/截屏2022-09-01 11.27.12.png" alt style="zoom: 25%;"></p></blockquote><p><strong>一元高斯分布</strong></p><blockquote><p>$f(x)=\frac{1}{\sigma\sqrt{2\pi}}e^{-\frac{(x-\mu)^2}{2\sigma^2}}$</p><p>$\mu：期望$</p><p>$\sigma:方差$</p><p>$\frac{1}{\sigma\sqrt{2\pi}}$：个常数项，为了让高斯分布的积分等于1不了/。，</p></blockquote><p><strong>多元高斯分布</strong></p><blockquote><p><img src="/2022/09/01/gao-si-guo-cheng/截屏2022-09-01 16.35.34.png" alt style="zoom:80%;"></p></blockquote><p><strong>无限元高斯分布</strong></p><blockquote><p>在多元高斯分布的基础上考虑进一步扩展，假设有无限多维呢？用一个例子来展示这个扩展的过程。假设我们在周一到周四每天的 7:00 测试了 4 次心率，如下图中 4 个点，可能的高斯分布如图所示（高瘦的那条）。这是一个一元高斯分布，只有每天 7: 00 的心率这个维度。</p><p><img src="/2022/09/01/gao-si-guo-cheng/截屏2022-09-01 16.53.56.png" alt style="zoom:40%;"></p><p>现在考虑不仅在每天的 7: 00 测心率（横轴），在 8:00 时也进行测量（纵轴），这个时候变成两个维度（二元高斯分布）。</p><p><img src="/2022/09/01/gao-si-guo-cheng/截屏2022-09-01 17.09.01.png" alt style="zoom:45%;"></p><p>更进一步，如果我们在每天的无数个时间点都进行测量，则变成了下图的情况。注意下图中把测量时间作为横轴，则每个颜色的一条线代表一个（无限个时间点的测量）无限维的采样。当对每次对无限维进行采样得到无限多个点时，其实可以理解为我们采样得到了一个函数。</p><p><img src="/2022/09/01/gao-si-guo-cheng/截屏2022-09-01 16.55.30.png" alt style="zoom:45%;"></p><p>当从函数的视角去看待采样，理解了每次采样无限维相当于采样一个函数之后，原本的概率密度函数不再是点的分布 ，而变成了<strong>函数的分布</strong>。这个无限元高斯分布即称为高斯过程。高斯过程正式地定义为：对于所有 $x=[x_1,x_2,⋯,x_n] ,f(x)=[f(x_1),f(x_2),⋯,f(x_n)] $都服从多元高斯分布，则称 f 是一个高斯过程，表示如下：</p><p>$f(x)\sim N(\mu(x),k(x,x))$</p><p>这里 $μ(x):\mathbb{R}^n→\mathbb{R}^n$表示均值函数（Mean function），返回各个维度的均值；$k(x,x):\mathbb{R}^n×\mathbb{R}^n→\mathbb{R}^n$ 为协方差函数 Covariance Function（也叫核函数 Kernel Function）返回两个向量各个维度之间的协方差矩阵。一个高斯过程为一个均值函数和协方差函数唯一地定义，并且<strong>一个高斯过程的有限维度的子集都服从一个多元高斯分布</strong>（为了方便理解，可以想象二元高斯分布两个维度各自都服从一个高斯分布）。</p></blockquote><p><strong>参考</strong></p><p><a href="https://zhuanlan.zhihu.com/p/36982945">多元正态分布与高斯判别分析算法</a></p><p><a href="https://zhuanlan.zhihu.com/p/27555501">什么是Gaussian process? —— 说说高斯过程与高斯分布的关系</a></p><p><a href="http://bridg.land/posts/gaussian-processes-1">Introduction to Gaussian Processes - Part I</a></p><p><a href="https://zhuanlan.zhihu.com/p/75589452">高斯过程 Gaussian Processes 原理、可视化及代码实现</a></p>]]></content>
      
      
      <categories>
          
          <category> Math </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Gaussian Process </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Word Embedding Download</title>
      <link href="/2022/08/16/word-embedding-download/"/>
      <url>/2022/08/16/word-embedding-download/</url>
      
        <content type="html"><![CDATA[<h2 id="Word2Vector"><a href="#Word2Vector" class="headerlink" title="Word2Vector"></a>Word2Vector</h2><p><a href="https://github.com/Embedding/Chinese-Word-Vectors">Chinese Word Vectors 中文词向量</a></p><p><a href="https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models">英文词向量</a></p><h2 id="Fasttext"><a href="#Fasttext" class="headerlink" title="Fasttext"></a>Fasttext</h2><p>Find more fasttext pretrained model at: <a href="https://fasttext.cc">fastText</a>.</p><div class="table-container"><table><thead><tr><th style="text-align:center">语言</th><th style="text-align:center">文件名</th></tr></thead><tbody><tr><td style="text-align:center">zh</td><td style="text-align:center"><a href="https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.zh.300.vec.gz">cc.zh.300.vec.zip</a></td></tr><tr><td style="text-align:center">en</td><td style="text-align:center"><a href="https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip">wiki-news-300d-1M.vec.zip</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"><a href="https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M-subword.vec.zip">wiki-news-300d-1M-subword.vec.zip</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"><a href="https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M.vec.zip">crawl-300d-2M.vec.zip</a></td></tr><tr><td style="text-align:center"></td><td style="text-align:center"><a href="https://dl.fbaipublicfiles.com/fasttext/vectors-english/crawl-300d-2M-subword.zip">crawl-300d-2M-subword.zip</a></td></tr></tbody></table></div><p>The first line of the file contains the number of words in the vocabulary and the size of the vectors. Each line contains a word followed by its vectors, like in the default fastText text format. Each value is space separated. Words are ordered by descending frequency. These text models can easily be loaded in Python using the following code:</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import iodef load_vectors(fname):    fin = io.open(fname, 'r', encoding='utf-8', newline='\n', errors='ignore')    n, d = map(int, fin.readline().split())    data = {}    for line in fin:        tokens = line.rstrip().split(' ')        data[tokens[0]] = map(float, tokens[1:])    return data<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h2 id="Bert"><a href="#Bert" class="headerlink" title="Bert"></a>Bert</h2><div class="table-container"><table><thead><tr><th></th><th>H=128</th><th>H=256</th><th>H=512</th><th>H=768</th></tr></thead><tbody><tr><td><strong>L=2</strong></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-2_H-128"><strong>2/128 (Tiny)</strong></a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-2_H-256">2/256</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-2_H-512">2/512</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-2_H-768">2/768</a></td></tr><tr><td><strong>L=4</strong></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-4_H-128">4/128</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-4_H-256"><strong>4/256 (Mini)</strong></a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-4_H-512"><strong>4/512 (Small)</strong></a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-4_H-768">4/768</a></td></tr><tr><td><strong>L=6</strong></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-6_H-128">6/128</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-6_H-256">6/256</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-6_H-512">6/512</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-6_H-768">6/768</a></td></tr><tr><td><strong>L=8</strong></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-8_H-128">8/128</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-8_H-256">8/256</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-8_H-512"><strong>8/512 (Medium)</strong></a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-8_H-768">8/768</a></td></tr><tr><td><strong>L=10</strong></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-10_H-128">10/128</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-10_H-256">10/256</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-10_H-512">10/512</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-10_H-768">10/768</a></td></tr><tr><td><strong>L=12</strong></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-12_H-128">12/128</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-12_H-256">12/256</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-12_H-512">12/512</a></td><td><a href="https://huggingface.co/uer/chinese_roberta_L-12_H-768"><strong>12/768 (Base)</strong></a></td></tr></tbody></table></div><h2 id="其他资源"><a href="#其他资源" class="headerlink" title="其他资源"></a>其他资源</h2><p><a href="https://sunyancn.github.io/post/28052.html">NLP 常用模型和数据集高速下载</a></p><p><a href="https://docs.qq.com/sheet/DVnpkTnF6VW9UeXdh?tab=BB08J2&amp;c=D22A0I0">fastNLP可加载embedding与数据集</a></p>]]></content>
      
      
      <categories>
          
          <category> Sources </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Word Embedding </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>梯度下降算法</title>
      <link href="/2022/08/10/ti-du-xia-jiang-suan-fa/"/>
      <url>/2022/08/10/ti-du-xia-jiang-suan-fa/</url>
      
        <content type="html"><![CDATA[<p>梯度下降法作为机器学习中较常使用的优化算法，其有着3种不同的形式：<strong>批量梯度下降（Batch Gradient Descent）、随机梯度下降（Stochastic Gradient Descent）、小批量梯度下降（Mini-Batch Gradient Descent）</strong>。其中小批量梯度下降法也常用在深度学习中进行模型的训练。本文将对这3种不同的梯度下降法进行理解。</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/3种梯度下降方法收敛曲线.png" alt="3种梯度下降方法收敛曲线" style="zoom:30%;"></p><p>下面我们以线性回归算法来对三种梯度下降法进行比较。</p><p>一般线性回归函数的假设函数为：</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/线性回归函数.png" alt="线性回归函数" style="zoom:40%;"></p><p>对应的损失函数为：</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/线性函数损失函数.png" alt="线性函数损失函数" style="zoom:40%;"></p><h2 id="批量梯度下降"><a href="#批量梯度下降" class="headerlink" title="批量梯度下降"></a>批量梯度下降</h2><h3 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h3><p><strong>批量梯度下降</strong>(Batch Gradient Descent，BGD)，是梯度下降法最原始的形式，它的具体思路是在更新每一参数时都使用所有的样本来进行更新。</p><p>训练的目的是要误差函数尽可能的小，即求解weights使误差函数尽可能小。首先，我们随机初始化weigths，然后不断反复的更新weights使得误差函数减小，直到满足要求时停止。这里更新算法我们选择梯度下降算法，利用初始化的weights并且反复更新weights：</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/梯度更新公式.png" alt="梯度更新公式" style="zoom:30%;"></p><p>这里α代表学习率，表示每次向着J最陡峭的方向迈步的大小。为了更新weights，我们需要求出函数J的偏导数。首先当我们只有一个数据点（x,y）的时候，J的偏导数是：</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/梯度计算公式.png" alt="梯度计算公式" style="zoom:30%;"></p><p>则对<strong>所有数据点</strong>，上述损失函数的偏导求和为：</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/偏导求和.png" alt="偏导求和" style="zoom:30%;"></p><p>那么每次参数更新的伪代码如下：</p><p><img src="/2022/08/10/ti-du-xia-jiang-suan-fa/BGD伪代码.png" alt="BGD伪代码" style="zoom:30%;"></p><p>由上图更新公式可以看到，每一次的参数更新都用到了所有的训练数（比如有m个，就用到了m个），如果训练数据非常多的话，是非常耗时的。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><ol><li>BGD在每次更新模型的时候，都要使用全量样本来计算更新的梯度值。如果有m个样本，迭代n轮，那么需要是m*n的计算复杂度。BGD的优势在于计算的是全局最优解，效果较SGD会好一些，劣势在于计算开销大。</li><li>可以在每次迭代时对所有样本向量进行并行计算。但是遍历全部样本仍需要大量时间，尤其是当数据集很大时（几百万甚至上亿），就有点力不从心了。</li></ol><h2 id="随机梯度下降"><a href="#随机梯度下降" class="headerlink" title="随机梯度下降"></a>随机梯度下降</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p><strong>随机梯度下降</strong>(Stochastic Gradient Descent，SGD)，随机梯度下降是通过每个样本来迭代更新一次，如果样本量很大的情况（例如几十万），那么可能只用其中几万条或者几千条的样本，就已经将theta迭代到最优解了，对比上面的批量梯度下降，迭代一次需要用到十几万训练样本，一次迭代不可能最优，如果迭代10次的话就需要遍历训练样本10次。但是，SGD伴随的一个问题是噪音较BGD要多，使得SGD并不是每次迭代都向着整体最优化方向。</p><h3 id="总结-1"><a href="#总结-1" class="headerlink" title="总结"></a>总结</h3><ol><li>SGD在每次更新模型的时候，只要当前遍历到的样本来计算更新的梯度值就行了。如果迭代n轮，则只需要n的计算复杂度，因为每轮只计算一个样本。</li><li>SGD优势在于计算开销减小很多，劣势在于计算的是局部最优解，可能最终达不到全局最优解。在数据量大的时候，SGD是较好的折衷选择。</li><li>在学习过程中加入了噪声，提高了泛化误差。</li><li>SGD可以跳出局部最优解，从而寻找到真正的全局最优解。</li></ol><h2 id="小批量梯度下降"><a href="#小批量梯度下降" class="headerlink" title="小批量梯度下降"></a>小批量梯度下降</h2><h3 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h3><p>mini-batch梯度下降法是介于SGD和BGD中间的一种优化方式，基本思想是每次拿出batch_size大小的数据做参数更新，既不是全部数据量也不是单个数据量，这也是并行计算的一种应用。</p><h4 id="batch-size对收敛速度的影响"><a href="#batch-size对收敛速度的影响" class="headerlink" title="batch size对收敛速度的影响"></a>batch size对收敛速度的影响</h4><p>Batch size大，收敛速度会比较慢，因为参数每次更新所需要的样本量增加了，但是会沿着比较准确的方向进行。</p><h4 id="在合理范围内，增大Batch-Size有何好处？"><a href="#在合理范围内，增大Batch-Size有何好处？" class="headerlink" title="在合理范围内，增大Batch_Size有何好处？"></a>在合理范围内，增大Batch_Size有何好处？</h4><ol><li><p>内存利用率提高了，大矩阵乘法的并行化效率提高。</p></li><li><p>跑完一次 epoch（全数据集）所需的迭代次数减少，对于相同数据量的处理速度进一步加快。</p></li><li><p>在一定范围内，一般来说 Batch_Size 越大，其确定的下降方向越准，引起训练震荡越小</p></li></ol><h4 id="盲目增大-batch-size-有何坏处？"><a href="#盲目增大-batch-size-有何坏处？" class="headerlink" title="盲目增大 batch_size 有何坏处？"></a>盲目增大 batch_size 有何坏处？</h4><ol><li><p>内存利用率提高了，但是内存容量可能撑不住了。</p></li><li><p>跑完一次 epoch（全数据集）所需的迭代次数减少，要想达到相同的精度，其所花费的时间大大增加了，从而对参数的修正也就显得更加缓慢。</p></li><li><p>Batch_size 增大到一定程度，其确定的下降方向已经基本不再变化。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Bert</title>
      <link href="/2022/07/29/bert/"/>
      <url>/2022/07/29/bert/</url>
      
        <content type="html"><![CDATA[<p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为Decoder是不能获要预测的信息的。</p><p>模型的主要创新点都在pre-train方法上，即用了Masked LM（MLM）和Next Sentence Prediction（NSP）两种方法分别捕捉词语和句子级别的representation。</p><h2 id="模型结构"><a href="#模型结构" class="headerlink" title="模型结构"></a>模型结构</h2><p>BERT的网络架构使用的是《Attention is all you need》中提出的多层Transformer结构，其最大的特点是抛弃了传统的RNN和CNN，通过Attention机制将任意位置的两个单词的距离转换成1，有效的解决了NLP中棘手的长期依赖问题。<br>Transformer的结构在NLP领域中已经得到了广泛应用，并且作者已经发布在TensorFlow的tensor2tensor库中。</p><p>Transformer的网络架构下图所示，Transformer是一个encoder-decoder的结构，由若干个编码器和解码器堆叠形成。</p><p>左侧部分为编码器，由Multi-Head Attention和一个全连接组成，用于将输入语料转化成特征向量。</p><p>右侧部分是解码器，其输入为编码器的输出以及已经预测的结果，由Masked Multi-Head Attention, Multi-Head Attention以及一个全连接组成，用于输出最后结果的条件概率。</p><p><img src="/2022/07/29/bert/Transformer模型结构.png" alt="Transformer模型结构" style="zoom:45%;"></p><p>Self- Attention与传统的Attention机制非常的不同：传统的Attention是基于source端和target端的隐变量（hidden state）计算Attention的，得到的结果是源端（source端）的每个词与目标端（target端）每个词之间的依赖关系。</p><p>但Self -Attention不同，它首先分别在source端和target端进行自身的attention，仅与source input或者target input自身相关的Self -Attention，以捕捉source端或target端自身的词与词之间的依赖关系；然后再把source端的得到的self -Attention加入到target端得到的Attention中，称作为Cross-Attention，以捕捉source端和target端词与词之间的依赖关系。</p><p>因此，Self -Attention比传统的Attention mechanism效果要好，主要原因之一是，传统的Attention机制<strong>忽略了源端或目标端句子中词与词之间的依赖关系</strong>，相对比，self Attention可以不仅可以得到源端与目标端词与词之间的依赖关系，同时还可以有效获取源端或目标端自身词与词之间的依赖关系。</p><p>当隐藏了Transformer的详细结构后，我们就可以用一个只有输入和输出的黑盒子来表示它了，而Transformer又可以进行堆叠，形成一个更深的神经网络：</p><p><img src="/2022/07/29/bert/Bert模型结构.png" alt="Bert模型结构" style="zoom:50%;"></p><p>huggingface库：</p><blockquote><p>128维度：uer/chinese_roberta_L-2_H-128 </p><p>768维度：hfl/chinese-macbert-base</p></blockquote><h2 id="Bert-源码解析"><a href="#Bert-源码解析" class="headerlink" title="Bert 源码解析"></a>Bert 源码解析</h2><p>本文基于Transformers版本4.4.2（2021年3月19日发布）项目中，pytorch版的BERT相关代码。</p><h3 id="Tokenization（BertTokenizer）"><a href="#Tokenization（BertTokenizer）" class="headerlink" title="Tokenization（BertTokenizer）"></a>Tokenization（BertTokenizer）</h3><p>和BERT有关的Tokenizer主要写在<code>/models/bert/tokenization_bert.py</code>和<code>/models/bert/tokenization_bert_fast.py</code> 中。</p><p>这两份代码分别对应基本的<code>BertTokenizer</code>，以及不进行token到index映射的<code>BertTokenizerFast</code>，这里主要讲解第一个。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">class BertTokenizer(PreTrainedTokenizer):    """    Construct a BERT tokenizer. Based on WordPiece.    This tokenizer inherits from :class:`~transformers.PreTrainedTokenizer` which contains most of the main methods.    Users should refer to this superclass for more information regarding those methods.    ...    """<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><code>BertTokenizer</code> 是基于<code>BasicTokenizer</code>和<code>WordPieceTokenizer</code> 的分词器：</p><p><strong><code>BasicTokenizer</code></strong>负责处理的第一步——按标点、空格等分割句子，并处理是否统一小写，以及清理非法字符。</p><ul><li>对于中文字符，通过预处理（加空格）来按字分割；</li><li>同时可以通过<code>never_split</code>指定对某些词不进行分割；这一步是可选的（默认执行）。</li></ul><p><strong><code>WordPieceTokenizer</code></strong>在词的基础上，进一步将词分解为<strong>子词</strong>（subword） 。</p><ul><li>subword介于char和word之间，既在一定程度保留了词的含义，又能够照顾到英文中单复数、时态导致的词表爆炸和未登录词的OOV（Out-Of-Vocabulary）问题，将词根与时态词缀等分割出来，从而减小词表，也降低了训练难度；</li><li>例如，tokenizer这个词就可以拆解为“token”和“##izer”两部分，注意后面一个词的“##”表示接在前一个词后面。</li></ul><p><code>BertTokenizer</code> 有以下常用方法：</p><ul><li><strong><code>from_pretrained</code></strong>：从包含词表文件（vocab.txt）的目录中初始化一个分词器；</li><li><strong><code>tokenize</code></strong>：将文本（词或者句子）分解为子词列表；</li><li><strong><code>convert_tokens_to_ids</code></strong>：将子词列表转化为子词对应下标的列表；</li><li><strong><code>convert_ids_to_tokens</code></strong> ：与上一个相反；</li><li><strong><code>convert_tokens_to_string</code></strong>：将subword列表按“##”拼接回词或者句子；</li><li><strong><code>encode</code></strong>：对于单个句子输入，分解词并加入特殊词形成“[CLS], x, [SEP]”的结构并转换为词表对应下标的列表；对于两个句子输入（多个句子只取前两个），分解词并加入特殊词形成“[CLS], x1, [SEP], x2, [SEP]”的结构并转换为下标列表；</li><li><strong><code>decode</code></strong>：可以将encode方法的输出变为完整句子。</li></ul><p>以及，类自身的方法：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">&gt;&gt;&gt; from transformers import BertTokenizer&gt;&gt;&gt; bt = BertTokenizer.from_pretrained('./bert-base-uncased/')&gt;&gt;&gt; bt('I like natural language progressing!'){'input_ids': [101, 1045, 2066, 3019, 2653, 27673, 999, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1]}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="Model（BertModel）"><a href="#Model（BertModel）" class="headerlink" title="Model（BertModel）"></a>Model（BertModel）</h3><p>和BERT模型有关的代码主要写在<code>/models/bert/modeling_bert.py</code>中，这一份代码有一千多行，包含BERT模型的基本结构和基于它的微调模型等。</p><p>下面从BERT模型本体入手分析：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">class BertModel(BertPreTrainedModel):    """    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of    cross-attention is added between the self-attention layers, following the architecture described in `Attention is    all you need &lt;https://arxiv.org/abs/1706.03762&gt;`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.    To behave as an decoder the model needs to be initialized with the :obj:`is_decoder` argument of the configuration    set to :obj:`True`. To be used in a Seq2Seq model, the model needs to initialized with both :obj:`is_decoder`    argument and :obj:`add_cross_attention` set to :obj:`True`; an :obj:`encoder_hidden_states` is then expected as an    input to the forward pass.    """ <span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>BertModel主要为transformer encoder结构，包含三个部分：</p><ol><li><strong><code>embeddings</code></strong>，即<code>BertEmbeddings</code>类的实体，对应词嵌入； </li><li><strong><code>encoder</code></strong>，即<code>BertEncoder</code>类的实体；</li><li><strong><code>pooler</code></strong>， 即<code>BertPooler</code>类的实体，这一部分是可选的。（BertPooler模块本质上就是一个全连接层网络），使BertPooler模块的全连接层只接收BertEncoder模块中第一个token（[CLS]）对应的输出，因为BertPooler模块用于解决序列级任务，而序列级任务只会用到[CLS]位置对应的输出。</li></ol><blockquote><p><strong>补充：注意BertModel也可以配置为Decoder，不过下文中不包含对这一部分的讨论。</strong></p></blockquote><p>下面将介绍BertModel的前向传播过程中各个参数的含义以及返回值：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def forward(    self,    input_ids=None,    attention_mask=None,    token_type_ids=None,    position_ids=None,    head_mask=None,    inputs_embeds=None,    encoder_hidden_states=None,    encoder_attention_mask=None,    past_key_values=None,    use_cache=None,    output_attentions=None,    output_hidden_states=None,    return_dict=None,): ...<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><ul><li><strong><code>input_ids</code></strong>：经过tokenizer分词后的subword对应的下标列表；</li><li><strong><code>attention_mask</code></strong>：在self-attention过程中，这一块mask用于标记subword所处句子和padding的区别，将padding部分填充为0；</li><li><strong><code>token_type_ids</code></strong>： 标记subword当前所处句子（第一句/第二句/padding）；</li><li><strong><code>position_ids</code></strong>： 标记当前词所在句子的位置下标；</li><li><strong><code>head_mask</code></strong>： 用于将某些层的某些注意力计算无效化；</li><li><strong><code>inputs_embeds</code></strong>： 如果提供了，那就不需要<code>input_ids</code>，跨过embedding lookup过程直接作为Embedding进入Encoder计算；</li><li><strong><code>encoder_hidden_states</code></strong>： 这一部分在BertModel配置为decoder时起作用，将执行cross-attention而不是self-attention；</li><li><strong><code>encoder_attention_mask</code></strong>： 同上，在cross-attention中用于标记encoder端输入的padding；</li><li><strong><code>past_key_values</code></strong>：这个参数貌似是把预先计算好的K-V乘积传入，以降低cross-attention的开销（因为原本这部分是重复计算）；</li><li><strong><code>use_cache</code></strong>： 将保存上一个参数并传回，加速decoding；</li><li><strong><code>output_attentions</code></strong>：是否返回中间每层的attention输出；</li><li><strong><code>output_hidden_states</code></strong>：是否返回中间每层的输出；</li><li><strong><code>return_dict</code></strong>：是否按键值对的形式（ModelOutput类，也可以当作tuple用）返回输出，默认为真。</li></ul><blockquote><p><strong>补充：注意，这里的head_mask对注意力计算的无效化，和下文提到的注意力头剪枝不同，而仅仅把某些注意力的计算结果给乘以这一系数。</strong></p></blockquote><p>返回部分如下：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># BertModel的前向传播返回部分        if not return_dict:            return (sequence_output, pooled_output) + encoder_outputs[1:]        return BaseModelOutputWithPoolingAndCrossAttentions(            last_hidden_state=sequence_output,            pooler_output=pooled_output,            past_key_values=encoder_outputs.past_key_values,            hidden_states=encoder_outputs.hidden_states,            attentions=encoder_outputs.attentions,            cross_attentions=encoder_outputs.cross_attentions,        )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>可以看出，返回值不但包含了encoder和pooler的输出，也包含了其他指定输出的部分（hidden_states和attention等，这一部分在<code>encoder_outputs[1:]</code>）方便取用：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"># BertEncoder的前向传播返回部分，即上面的encoder_outputs        if not return_dict:            return tuple(                v for v in [                    hidden_states,                    next_decoder_cache,                    all_hidden_states,                    all_self_attentions,                    all_cross_attentions,                ]                if v is not None            )        return BaseModelOutputWithPastAndCrossAttentions(            last_hidden_state=hidden_states,            past_key_values=next_decoder_cache,            hidden_states=all_hidden_states,            attentions=all_self_attentions,            cross_attentions=all_cross_attentions,        )<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>此外，BertModel还有以下的方法，方便BERT玩家进行各种操作：</p><ol><li><strong><code>get_input_embeddings</code></strong>：提取embedding中的word_embeddings即词向量部分；</li><li><strong><code>set_input_embeddings</code></strong>：为embedding中的word_embeddings赋值；</li><li><strong><code>_prune_heads</code></strong>：提供了将注意力头剪枝的函数，输入为<code>{layer_num: list of heads to prune in this layer}</code>的字典，可以将指定层的某些注意力头剪枝。</li></ol><blockquote><p><strong>补充：剪枝是一个复杂的操作，需要将保留的注意力头部分的Wq、Kq、Vq和拼接后全连接部分的权重拷贝到一个新的较小的权重矩阵（注意先禁止grad再拷贝），并实时记录被剪掉的头以防下标出错。具体参考<code>BertAttention</code>部分的<code>prune_heads</code>方法。</strong></p></blockquote><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></p><p><a href="https://zhuanlan.zhihu.com/p/360988428">HuggingFace Transformers最新版本源码解读</a></p><p><a href="https://zhuanlan.zhihu.com/p/370526126">从源码解析 Bert 的 BertPooler 模块</a></p><p><a href="http://fancyerii.github.io/2021/05/11/huggingface-transformers-1/">Huggingface Transformer教程(一)</a></p><p><a href="https://mp.weixin.qq.com/s?__biz=MzAxMTU5Njg4NQ==&amp;mid=2247488814&amp;idx=2&amp;sn=739a56a23759d98b02e3977dff68884d&amp;scene=21#wechat_redirect">【关于 Bert 源码解析I 之 主体篇 】 那些的你不知道的事</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Bert </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习笔记3——Normalization</title>
      <link href="/2022/07/27/nlp-xue-xi-bi-ji-3/"/>
      <url>/2022/07/27/nlp-xue-xi-bi-ji-3/</url>
      
        <content type="html"><![CDATA[<h1 id="Normalization"><a href="#Normalization" class="headerlink" title="Normalization"></a>Normalization</h1><p>我们最希望输入网络模型中的数据具有什么样的特点？</p><p>回答是：“<strong>独立同分布</strong>”。</p><p>独立同分布的数据可以简化常规机器学习模型的训练、提升机器学习模型的预测能力，已经是一个共识。因此，在把数据喂给机器学习模型之前，“<strong>白化（whitening）</strong>”是一个重要的数据预处理步骤</p><p>白化一般包含两个目的：</p><p>（1）去除特征之间的相关性 —&gt; 独立（每次抽样之间是没有关系的，不会相互影响）；</p><p>（2）使得所有特征具有相同的均值和方差 —&gt; 同分布（每次抽样，样本都服从同样的一个分布）。</p><p>白化最典型的方法就是PCA。在传统的机器学习方法中，对数据进行预处理之后，由于模型深度太浅，故可以满足数据是独立同分布的特征。但是对于深度学习模型，就会出现一个问题：经过卷积层、池化层的层层叠加，层层改变，在高层网络中，数据就会失去原始数据的特征，其数据的分布特征也会相应的改变。此问题被google总结为Internal Covariate Shift。</p><p>那么ICS会对深度神经网络有什么影响呢？</p><blockquote><p>其一，上层参数需要不断适应新的输入数据分布，降低学习速度</p><p>其二，下层输入的变化可能趋向于变大或者变小，导致上层落入饱和区，使得学习过早停止</p><p>其三，每层的更新都会影响到其它层，因此每层的参数更新策略需要尽可能的谨慎。</p></blockquote><h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h2><p><img src="/2022/07/27/nlp-xue-xi-bi-ji-3/Batch Normalization.png" alt="Batch Normalization" style="zoom: 33%;"></p><p>针对单个神经元进行，利用网络训练时一个 mini-batch 的数据来计算该神经元 x_i的均值和方差,因而称为 Batch Normalization</p><h3 id="适用的场景"><a href="#适用的场景" class="headerlink" title="适用的场景"></a>适用的场景</h3><p>每个 mini-batch 比较大，数据分布比较接近。在进行训练之前，要做好充分的 shuffle. 否则效果会差很多。</p><h3 id="存在问题"><a href="#存在问题" class="headerlink" title="存在问题"></a>存在问题</h3><ol><li>BN特别依赖Batch Size；当Batch size很小的时候，BN的效果就非常不理想了。在很多情况下，Batch size大不了，因为你GPU的显存不够。所以，通常会有其他比较麻烦的手段去解决这个问题，比如MegDet的CGBN等；</li><li>BN对处理序列化数据的网络比如RNN是不太适用的。</li><li>BN只在训练的时候用，inference的时候不会用到，因为inference的输入不是批量输入。</li></ol><h2 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a>Layer Normalization</h2><p><img src="/2022/07/27/nlp-xue-xi-bi-ji-3/Layer Normalization.png" alt="Layer Normalization" style="zoom:33%;"></p><p>方式：综合考虑一层所有维度的输入，计算该层的平均输入值和输入方差，然后用同一个规范化操作来转换各个维度的输入。</p><h3 id="适用的场景-1"><a href="#适用的场景-1" class="headerlink" title="适用的场景"></a>适用的场景</h3><p>LN 针对单个训练样本进行，不依赖于其他数据，因此可以避免 BN 中受 mini-batch 数据分布影响的问题，可以用于 小mini-batch场景、动态网络场景和 RNN，特别是自然语言处理领域。此外，LN 不需要保存 mini-batch 的均值和方差，节省了额外的存储空间。</p><h2 id="BN-vs-LN"><a href="#BN-vs-LN" class="headerlink" title="BN vs LN"></a>BN vs LN</h2><p>BN 的转换是针对单个神经元可训练的——不同神经元的输入经过再平移和再缩放后分布在不同的区间，而 LN 对于一整层的神经元训练得到同一个转换——所有的输入都在同一个区间范围内。如果不同输入特征不属于相似的类别（比如颜色和大小），那么 LN 的处理可能会降低模型的表达能力。</p>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习笔记2——归一化</title>
      <link href="/2022/07/27/nlp-xue-xi-bi-ji-2/"/>
      <url>/2022/07/27/nlp-xue-xi-bi-ji-2/</url>
      
        <content type="html"><![CDATA[<h1 id="归一化"><a href="#归一化" class="headerlink" title="归一化"></a>归一化</h1><p>因为 每一列数据的量纲不同，导致 数据分布区间存在差异。（人的身高可以是 180cm，也可以是 1.8m，这两个虽然表示意义相同，但是由于单位的不同，导致 机器学习在计算过程中也容易出现差异，所以就需要对数据进行归一化）。通过归一化，每个维度都是去量纲化的，避免了不同量纲的选取对距离计算产生的巨大影响。</p><h2 id="归一化方法"><a href="#归一化方法" class="headerlink" title="归一化方法"></a>归一化方法</h2><ul><li>线性比例变换法：</li></ul><p><img src="/2022/07/27/nlp-xue-xi-bi-ji-2/线性比例变换法.png" alt="线性比例变换法" style="zoom:66%;"></p><ul><li>极差变换法：</li></ul><p><img src="/2022/07/27/nlp-xue-xi-bi-ji-2/极差变换法.png" alt="极差变换法" style="zoom:66%;"></p><p>将原始数据线性化的方法转换到[0 1]的范围。由于极值化方法在对变量无量纲化过程中仅仅与该变量的最大值和最小值这两个极端值有关，而与其他取值无关，这使得该方法在改变各变量权重时过分依赖两个极端取值。</p><ul><li>0均值标准化：</li></ul><p><img src="/2022/07/27/nlp-xue-xi-bi-ji-2/0均值标准化.png" alt="0均值标准化" style="zoom:66%;"></p><p>虽然该方法在无量纲化过程中利用了所有的数据信息，但是该方法在无量纲化后不仅使得转换后的各变量均值相同，且标准差也相同，即无量纲化的同时还消除了各变量在变异程度上的差异，从而转换后的各变量在聚类分析中的重要性程度是同等看待的。而实际分析中，经常根据各变量在不同单位间取值的差异程度大小来决定其在分析中的重要性程度，差异程度大的其分析权重也相对较大。</p><h2 id="需要归一化的模型"><a href="#需要归一化的模型" class="headerlink" title="需要归一化的模型"></a>需要归一化的模型</h2><ul><li>基于距离计算的模型：KNN；</li><li>通过梯度下降法求解的模型：线性回归、逻辑回归、支持向量机、神经网络</li></ul><h2 id="不需要归一化的模型"><a href="#不需要归一化的模型" class="headerlink" title="不需要归一化的模型"></a>不需要归一化的模型</h2><ul><li>树形模型：决策树、随机森林(Random Forest)</li></ul>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NLP学习笔记1——过拟合和欠拟合</title>
      <link href="/2022/07/26/nlp-xue-xi-bi-ji-1/"/>
      <url>/2022/07/26/nlp-xue-xi-bi-ji-1/</url>
      
        <content type="html"><![CDATA[<h1 id="过拟合和欠拟合"><a href="#过拟合和欠拟合" class="headerlink" title="过拟合和欠拟合"></a>过拟合和欠拟合</h1><p>推荐博客：<a href="http://scott.fortmann-roe.com/docs/BiasVariance.html">Understanding the Bias-Variance Tradeoff</a></p><p>当我们讨论预测模型时，预测误差可以分解为两个我们关心的主要子成分：由“偏差”引起的误差和由“方差”引起的误差。模型最小化偏差和方差的能力之间存在权衡。了解这两种类型的误差可以帮助我们诊断模型结果，避免过度或欠拟合的错误。</p><p><img src="/2022/07/26/nlp-xue-xi-bi-ji-1/bias and variance.png" alt="bias and variance" style="zoom: 33%;"></p><p>每一个蓝点，都代表了一个训练模型的预测数据，即根据不同的训练集训练出一个训练模型，再用这个训练模型作出一次预测结果。如果将这个过程重复N次，相当于进行了N次射击。我们假设真实的函数关系是Y=f(x)，而训练模型预测的结果是p(x)，则</p><ul><li>偏差错误：偏差是衡量预测值和真实值的关系。即N次预测的平均值（也叫期望值），和实际真实值的差距。所以偏差bias=E(p(x)) - f(x)。<br><strong>即bias是指一个模型在不同训练集上的平均表现和真实值的差异，用来衡量一个模型的拟合能力</strong>。</li><li>方差错误：方差用于衡量预测值之间的关系，和真实值无关。即对于给定的某一个输入，N次预测结果之间的方差。variance= E((p(x) - E(p(x)))^2)。这个公式就是数学里的方差公式，反应的是统计量的离散程度。只不过，我们需要搞清楚我们计算的方差的意义，它反应的是不同训练模型针对同一个预测的离散程度。<br><strong>即variance指一个模型在不同训练集上的差异，用来衡量一个模型是否容易过拟合</strong>。</li></ul><p>高偏差，低方差：<br>每次射击都很准确的击中同一个位置，故极端的情况方差为0。只不过，这个位置距离靶心相差了十万八千里。对于射击而言，每次都打到同一个点，很可能是因为它打的不是靶心。对于模型而言，往往是因为模型过于简单，才会造成“准”的假象。提高模型的复杂度，往往可以减少高偏差。</p><p>高方差，低偏差：<br>是不是偏差越低越好？是不是低偏差时，方差也会低呢？通过对偏差的定义，不难发现，偏差是一个期望值（平均值），如果一次射击偏左5环，另一次射击偏右5环，最终偏差是0。但是没一枪打中靶心，所以方差是巨大的，这种情况也是需要改进的。</p><h2 id="过拟合-高方差"><a href="#过拟合-高方差" class="headerlink" title="过拟合/高方差"></a>过拟合/高方差</h2><h3 id="原因"><a href="#原因" class="headerlink" title="原因"></a>原因</h3><ol><li>训练集数量不足，样本类型单一。</li><li>训练集中存在噪声。</li><li>模型复杂度过高。</li></ol><h3 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li>标注不同类型的样本，是 样本尽可能的均衡。</li><li>降低训练模型复杂度</li><li>正则化</li><li>采用 bagging(如随机森林等）集成学习方法 来 防止过拟合</li><li>交叉检验</li><li>DropOut策略</li></ol><h2 id="欠拟合-高偏差"><a href="#欠拟合-高偏差" class="headerlink" title="欠拟合/高偏差"></a>欠拟合/高偏差</h2><h3 id="原因-1"><a href="#原因-1" class="headerlink" title="原因"></a>原因</h3><p>模型没有充分学习到 数据中的特征信息，使得模型无法很好地拟合数据</p><h3 id="解决方法-1"><a href="#解决方法-1" class="headerlink" title="解决方法"></a>解决方法</h3><ol><li>特征工程，添加更多的特征项</li><li>集成学习方法，boosting（如GBDT）能有效解决 high bias</li><li>提高模型复杂度</li><li>减小正则化系数</li></ol><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><p>PCA不被推荐用来避免过拟合：PCA是一种无监督学习，其存在的假设是：方差越大信息量越多。但是信息（方差）小的特征并不代表表对于分类没有意义，可能正是某些方差小的特征直接决定了分类结果，而PCA在降维过程中完全不考虑目标变量    的做法会导致一些关键但方差小的分类信息被过滤掉。</p>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logging for Python</title>
      <link href="/2022/02/08/logging-for-python/"/>
      <url>/2022/02/08/logging-for-python/</url>
      
        <content type="html"><![CDATA[<blockquote><p>import logging</p></blockquote><h2 id="logging模块简介"><a href="#logging模块简介" class="headerlink" title="logging模块简介"></a>logging模块简介</h2><p>Python的logging模块提供了通用的日志系统，可以方便第三方模块或者是应用使用。模块提供logger，handler，filter，formatter。</p><ul><li>logger：提供日志接口，供应用代码使用。logger最长用的操作有两类：配置和发送日志消息。可以通过logging.getLogger(name)获取logger对象，如果不指定name则返回root对象，多次使用相同的name调用getLogger方法返回同一个logger对象。</li><li>handler：将日志记录（log record）发送到合适的目的地（destination），比如文件，socket等。一个logger对象可以通过addHandler方法添加0到多个handler，每个handler又可以定义不同日志级别，以实现日志分级过滤显示。</li><li>filter：提供一种优雅的方式决定一个日志记录是否发送到handler。</li><li>formatter：指定日志记录输出的具体格式。formatter的构造方法需要两个参数：消息的格式字符串和日期字符串，这两个参数都是可选的。</li></ul><h2 id="Logging-Level"><a href="#Logging-Level" class="headerlink" title="Logging Level"></a>Logging Level</h2><p>DEBUG: Detailed information, typically of interest only when diagnosing problems.</p><p>INFO: Confirmation that things are working as expected.</p><p>WARNING: An indication that unexpected happened, or indicative of some problem in the near future (e.g. ‘disk space low’). The software is still working as expected.</p><p>ERROR: Due to a more serious problem, the software has not been able to perform some functions.</p><p>CRITICAL: A serious error, indicating that the program maybe unable to continue running.</p><h2 id="Set-Level"><a href="#Set-Level" class="headerlink" title="Set Level"></a>Set Level</h2><p>Logging.basicConfig(filename=’’, level=logging.DEBUG</p><p>​                                    format=’%(asctime)s:%(levelname)s:%(name)s:%(message)s’)</p><h1 id="default-by-warning"><a href="#default-by-warning" class="headerlink" title="default by warning"></a>default by warning</h1><h2 id="Output-to-Console"><a href="#Output-to-Console" class="headerlink" title="Output to Console"></a>Output to Console</h2><p>logging.debug(‘{}’.format())</p><h2 id="Set-Logger"><a href="#Set-Logger" class="headerlink" title="Set Logger"></a>Set Logger</h2><p>以上部分是设置的root logger，下面是设置logger，以方便import时日志文件不会混乱。</p><p>import logging</p><p>logger = logging.getlogger(__name__)</p><p>logger.setLevel(logging.INFO)</p><p>formatter = logging.Formatter(‘%(asctime)s:%(levelname)s:%(name)s:%(message)s’)</p><p>file_handler = logging.FileHandler(‘’)    #input: file name</p><p>file_handler.setFormatter(formatter)</p><p>logger.addHandler(file_handler)</p><p>输出的时候logging换成logger</p><h2 id="Formatter"><a href="#Formatter" class="headerlink" title="Formatter"></a>Formatter</h2><ul><li>%(name)s Logger的名字</li><li>%(levelname)s 文本形式的日志级别</li><li>%(message)s 用户输出的消息</li><li>%(asctime)s 字符串形式的当前时间。默认格式是 “2003-07-08 16:49:45,896”。逗号后面的是毫秒</li><li>%(levelno)s 数字形式的日志级别</li><li>%(pathname)s 调用日志输出函数的模块的完整路径名，可能没有</li><li>%(filename)s 调用日志输出函数的模块的文件名</li><li>%(module)s  调用日志输出函数的模块名</li><li>%(funcName)s 调用日志输出函数的函数名</li><li>%(lineno)d 调用日志输出函数的语句所在的代码行</li><li>%(created)f 当前时间，用UNIX标准的表示时间的浮 点数表示</li><li>%(relativeCreated)d 输出日志信息时的，自Logger创建以 来的毫秒数</li><li>%(thread)d 线程ID。可能没有</li><li>%(threadName)s 线程名。可能没有</li><li>%(process)d 进程ID。可能没有</li></ul><h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><pre class="line-numbers language-python" data-language="python"><code class="language-python">import logginglogger = logging.getLogger("simple_example")logger.setLevel(logging.DEBUG)# 建立一个filehandler来把日志记录在文件里，级别为debug以上fh = logging.FileHandler("spam.log")fh.setLevel(logging.DEBUG)# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上ch = logging.StreamHandler()ch.setLevel(logging.ERROR)# 设置日志格式formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")ch.setFormatter(formatter)fh.setFormatter(formatter)#将相应的handler添加在logger对象中logger.addHandler(ch)logger.addHandler(fh)# 开始打日志logger.debug("debug message")logger.info("info message")logger.warn("warn message")logger.error("error message")logger.critical("critical message")<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>当一个项目比较大的时候，不同的文件中都要用到Log,可以考虑将其封装为一个类来使用</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">#! /usr/bin/env python#coding=gbkimport logging,os class Logger: def __init__(self, path,clevel = logging.DEBUG,Flevel = logging.DEBUG):  self.logger = logging.getLogger(path)  self.logger.setLevel(logging.DEBUG)  fmt = logging.Formatter('[%(asctime)s] [%(levelname)s] %(message)s', '%Y-%m-%d %H:%M:%S')  #设置CMD日志  sh = logging.StreamHandler()  sh.setFormatter(fmt)  sh.setLevel(clevel)  #设置文件日志  fh = logging.FileHandler(path)  fh.setFormatter(fmt)  fh.setLevel(Flevel)  self.logger.addHandler(sh)  self.logger.addHandler(fh)  def debug(self,message):  self.logger.debug(message)  def info(self,message):  self.logger.info(message)  def war(self,message):  self.logger.warn(message)  def error(self,message):  self.logger.error(message)  def cri(self,message):  self.logger.critical(message) if __name__ =='__main__': logyyx = Logger('yyx.log',logging.ERROR,logging.DEBUG) logyyx.debug('一个debug信息') logyyx.info('一个info信息') logyyx.war('一个warning信息') logyyx.error('一个error信息') logyyx.cri('一个致命critical信息')<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>这样每次使用的时候只要实例化一个对象就可以了</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">logobj = Logger(‘filename',clevel,Flevel)<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Logging </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Transformer</title>
      <link href="/2022/02/08/transformer/"/>
      <url>/2022/02/08/transformer/</url>
      
        <content type="html"><![CDATA[<h2 id="Why-Transformer"><a href="#Why-Transformer" class="headerlink" title="Why Transformer?"></a>Why Transformer?</h2><p>在谷歌2007年提出《Attention Is All You Need》后，基于Attention思想的Transformer模型开始越来越多地出现在大众眼中。Transformer模型基于encoder-decoder架构，抛弃了传统的RNN、CNN模型，仅由Attention机制实现，并且由于encoder端是并行计算的，训练时间大大缩短。<br>Transformer模型广泛应用于NLP领域，机器翻译、文本摘要、问答系统等等，目前火热的Bert模型就是基于Transformer模型构建的。</p><p><img src="/2022/02/08/transformer/截屏2022-02-08 上午9.54.17.png" alt style="zoom: 25%;"></p><h2 id="What-is-Transformer"><a href="#What-is-Transformer" class="headerlink" title="What is Transformer?"></a>What is Transformer?</h2><p>上一节说到Transformer模型基于encoder-decoder架构。下面拿一个机器翻译的模型来进行演示。</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午10.45.07.png" alt style="zoom: 25%;"></p><p>那么Encoders和Decoders里面有哪些东西呢？在《Attention Is All You Need》里分别对Encoders和Decoders使用了6个chunks。结构如下：</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午10.50.06.png" alt style="zoom:25%;"></p><p>其中Encoders和Decoders里的每个chunk的结构如下。</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午10.54.42.png" alt style="zoom:25%;"></p><p>读到这里你或许会发现transformer的核心就是这个attention机制。那么attention又是个什么东西呢？</p><h3 id="Attention"><a href="#Attention" class="headerlink" title="Attention"></a>Attention</h3><p>假设以下句子是我们要翻译的输入句子：</p><pre class="line-numbers language-none"><code class="language-none">The animal didn't cross the street because it was too tired<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这个句子中的“it”指的是什么？它指的是街道还是动物？对人类来说，这是一个简单的问题，但对算法来说，就不那么简单了。当模型处理it这个词时，Attention允许它将it与animal联系起来。<br>当模型处理每个单词（输入序列中的每个位置）时，Attention允许它查看输入序列中的其他位置，以寻找有助于更好地编码该单词的线索。</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午11.10.12.png" alt style="zoom: 33%;"></p><p>如上图所示，在对it进行encode时，it这个单词主要focus on “The animal”。</p><p>在attention中有三个非常重要的矩阵——Q，K，V。由下图可见，x为embedding过的词汇向量，其attention的计算过程如下：</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午11.15.08.png" alt style="zoom:50%;"></p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午11.18.18.png" alt style="zoom: 25%;"></p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午11.21.46.png" alt style="zoom: 25%;"></p><p>或许读到这里你很疑惑，这一堆东西tmd在做什么，他的模型有个锤子的物理意义？别着急，首先我们要知道矩阵相乘实际是对空间内向量的线性变换，内积其中的一个作用就是用来计算投影，所以可以拿来计算两个word embedding的相似度。如下面所示：</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 上午11.59.54.png" alt style="zoom: 33%;"></p><p><img src="/2022/02/08/transformer/截屏2022-02-07 下午12.01.18.png" alt style="zoom:33%;"></p><p>其中“早上好”向量经过softmax进行归一化处理。这个过程可以用公式表示为:</p><p>$Softmax(XX^T)X$</p><p>通过$X$与$W^Q,W^K,W^V$相乘可以提高模型的拟合能力，因为$W^Q,W^K,W^V$是可以训练的，起到一个缓冲的效果。</p><p>在新的向量中，每一个维度的数值都是由三个词向量在这一维度的数值加权求和得来的，这个新的行向量就是”早”词向量经过注意力机制加权求和之后的表示。</p><p>至于$d_k$意思是W矩阵的dimension，假设$Q,K$里的元素的均值为0，方差为1，那么$A^T=Q^TK$中元素的均值为0，方差为d。当d变得很大时，$A$中的元素的方差也会变得很大，如果$A$中的元素方差很大，那么$Softmax(A)$的分布会趋于陡峭(分布的方差大，分布集中在绝对值大的区域)。总结一下就是$Softmax(A)$的分布会和d有关。因此$A$中每一个元素除以$\sqrt {d_k}$后，方差又变为1。这使得$Softmax(A)$的分布“陡峭”程度与d解耦，从而使得训练过程中梯度值保持稳定。</p><h3 id="Multi-Head-Attention"><a href="#Multi-Head-Attention" class="headerlink" title="Multi-Head Attention"></a>Multi-Head Attention</h3><p>可以理解为在多个关注点上的attention。其过程如下：</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 下午2.31.23.png" alt style="zoom:50%;"></p><h3 id="Layer-Normalization"><a href="#Layer-Normalization" class="headerlink" title="Layer Normalization"></a><strong>Layer Normalization</strong></h3><p><strong>随着网络层数的增加，数据分布不断发生变化，偏差越来越大，导致我们不得不使用更小的学习率来稳定梯度</strong>。Layer Normalization 的作用就是保证数据特征分布的稳定性，将数据标准化到ReLU激活函数的作用区域，可以使得激活函数更好的发挥作用。</p><p><img src="/2022/02/08/transformer/截屏2022-02-07 下午5.27.54.png" alt style="zoom:25%;"></p><h3 id="Positional-Encoding"><a href="#Positional-Encoding" class="headerlink" title="Positional Encoding"></a>Positional Encoding</h3><p>因为Attention机制里没有位置信息，所以要手动添加。</p><h3 id="Feed-Forward-Network"><a href="#Feed-Forward-Network" class="headerlink" title="Feed Forward Network"></a>Feed Forward Network</h3><p>每一层经过attention之后，还会有一个FFN，FFN包含了2层linear transformation层，中间的激活函数是ReLu。FFN通过变换了attention output的空间, 从而增加了模型的表现能力。把FFN去掉模型也是可以用的，但是效果差了很多。</p><h2 id="Encoder-and-Decoder"><a href="#Encoder-and-Decoder" class="headerlink" title="Encoder and Decoder"></a>Encoder and Decoder</h2><p><img src="/2022/02/08/transformer/截屏2022-02-07 下午5.11.39.png" alt style="zoom:50%;"></p><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p><p><a href="https://zhuanlan.zhihu.com/p/410776234">超详细图解Self-Attention</a></p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Getting Started with SPARQL</title>
      <link href="/2022/01/14/sparql/"/>
      <url>/2022/01/14/sparql/</url>
      
        <content type="html"><![CDATA[<h2 id="SPARQL简介"><a href="#SPARQL简介" class="headerlink" title="SPARQL简介"></a>SPARQL简介</h2><p>SPARQL即SPARQL Protocol and RDF Query Language的缩写，专门用于访问和操作RDF数据，是语义网的核心技术之一。</p><p>从SPARQL的全称我们可以知道，其由两个部分组成：协议和查询语言。</p><ul><li>查询语言很好理解，就像SQL用于查询关系数据库中的数据，XQuery用于查询XML数据，SPARQL用于查询RDF数据。</li><li>协议是指我们可以通过HTTP协议在客户端和SPARQL服务器（SPARQL endpoint）之间传输查询和结果，这也是和其他查询语言最大的区别。</li></ul><p>一个SPARQL查询本质上是一个带有变量的RDF图，以下面这条的罗纳尔多RDF数据为例：</p><pre class="line-numbers language-none"><code class="language-none">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/chineseName&gt; "罗纳尔多·路易斯·纳萨里奥·德·利马"^^string.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>把属性值用变量代替（SPARQL中，用问号加变量名的方式来表示一个变量。），即：</p><pre class="line-numbers language-none"><code class="language-none">&lt;http://www.kg.com/person/1&gt; &lt;http://www.kg.com/ontology/chineseName&gt; ?x.<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>SPARQL查询是基于图匹配的思想。我们把上述的查询与RDF图进行匹配，找到符合该匹配模式的所有子图，最后得到变量的值。就上面这个例子而言，在RDF图中找到匹配的子图后，将”罗纳尔多·路易斯·纳萨里奥·德·利马”和“?x”绑定，我们就得到最后的结果。简而言之，SPARQL查询分为三个步骤：</p><ol><li>构建查询图模式，表现形式就是带有变量的RDF。</li><li>匹配，匹配到符合指定图模式的子图。</li><li>绑定，将结果绑定到查询图模式对应的变量上。</li></ol><h2 id="SPARQL语法"><a href="#SPARQL语法" class="headerlink" title="SPARQL语法"></a>SPARQL语法</h2><h3 id="Triple-Patterns"><a href="#Triple-Patterns" class="headerlink" title="Triple Patterns"></a><strong>Triple Patterns</strong></h3><p><img src="/2022/01/14/sparql/Triple Patterns.png" alt style="zoom:50%;"></p><h3 id="Main-Query-Form"><a href="#Main-Query-Form" class="headerlink" title="Main Query Form"></a><strong>Main Query Form</strong></h3><p><img src="/2022/01/14/sparql/Main Query Form.png" alt style="zoom:50%;"></p><h3 id="Single-Triple-Pattern"><a href="#Single-Triple-Pattern" class="headerlink" title="Single Triple Pattern"></a>Single Triple Pattern</h3><p><img src="/2022/01/14/sparql/Single Triple Pattern.png" alt style="zoom:50%;"></p><h3 id="Joins"><a href="#Joins" class="headerlink" title="Joins"></a><strong>Joins</strong></h3><p><img src="/2022/01/14/sparql/Joins.png" alt style="zoom:50%;"></p><h3 id="Optional-Join"><a href="#Optional-Join" class="headerlink" title="Optional Join"></a><strong>Optional Join</strong></h3><p><img src="/2022/01/14/sparql/Optional Join.png" alt style="zoom:50%;"></p><h3 id="Subqueries"><a href="#Subqueries" class="headerlink" title="Subqueries"></a><strong>Subqueries</strong></h3><p><img src="/2022/01/14/sparql/Subqueries.png" alt style="zoom:50%;"></p><h3 id="Alternatives"><a href="#Alternatives" class="headerlink" title="Alternatives"></a>Alternatives</h3><p><img src="/2022/01/14/sparql/Alternatives.png" alt style="zoom:50%;"></p><h3 id="Negation"><a href="#Negation" class="headerlink" title="Negation"></a><strong>Negation</strong></h3><p><img src="/2022/01/14/sparql/Negation.png" alt style="zoom:50%;"></p><h3 id="Sort-Results"><a href="#Sort-Results" class="headerlink" title="Sort Results"></a><strong>Sort Results</strong></h3><p><img src="/2022/01/14/sparql/Sort Results.png" alt style="zoom:50%;"></p><h3 id="Limit-Results"><a href="#Limit-Results" class="headerlink" title="Limit Results"></a>Limit Results</h3><p><img src="/2022/01/14/sparql/Limit Results.png" alt style="zoom:50%;"></p><h3 id="Offset-Results"><a href="#Offset-Results" class="headerlink" title="Offset Results"></a><strong>Offset Results</strong></h3><p><img src="/2022/01/14/sparql/Offset Results.png" alt style="zoom:50%;"></p><h3 id="Filtering-Results"><a href="#Filtering-Results" class="headerlink" title="Filtering Results"></a>Filtering Results</h3><p><img src="/2022/01/14/sparql/Filtering Results.png" alt style="zoom:50%;"></p><h3 id="Binding-Variables"><a href="#Binding-Variables" class="headerlink" title="Binding Variables"></a><strong>Binding Variables</strong></h3><p><img src="/2022/01/14/sparql/Binding Variables.png" alt style="zoom:50%;"></p><h3 id="Removing-Duplicates"><a href="#Removing-Duplicates" class="headerlink" title="Removing Duplicates"></a>Removing Duplicates</h3><p><img src="/2022/01/14/sparql/Removing Duplicates.png" alt style="zoom:50%;"></p><h3 id="Aggregation"><a href="#Aggregation" class="headerlink" title="Aggregation"></a><strong>Aggregation</strong></h3><p><img src="/2022/01/14/sparql/Aggregation.png" alt style="zoom:50%;"></p><h3 id="Grouping-Results"><a href="#Grouping-Results" class="headerlink" title="Grouping Results"></a>Grouping Results</h3><p><img src="/2022/01/14/sparql/Grouping Results.png" alt style="zoom:50%;"></p><h3 id="Property-Paths"><a href="#Property-Paths" class="headerlink" title="Property Paths"></a><strong>Property Paths</strong></h3><p><img src="/2022/01/14/sparql/Property Paths.png" alt style="zoom:50%;"></p><h3 id="Recursive-Paths"><a href="#Recursive-Paths" class="headerlink" title="Recursive Paths"></a><strong>Recursive Paths</strong></h3><p><img src="/2022/01/14/sparql/Recursive Paths.png" alt style="zoom:50%;"></p><h2 id="Query-Types"><a href="#Query-Types" class="headerlink" title="Query Types"></a><strong>Query Types</strong></h2><h3 id="ASK-Query"><a href="#ASK-Query" class="headerlink" title="ASK Query"></a><strong>ASK Query</strong></h3><p>返回的是布尔值</p><p><img src="/2022/01/14/sparql/ASK Query.png" alt style="zoom:50%;"></p><h3 id="DESCRIBE-Query"><a href="#DESCRIBE-Query" class="headerlink" title="DESCRIBE Query"></a><strong>DESCRIBE Query</strong></h3><p>DESCRIBE`查询返回RDF图。SPARQL规范中没有规定节点返回的三重，事实上取决于系统。实现返回节点的所有输出边缘</p><p><img src="/2022/01/14/sparql/DESCRIBE Query.png" alt style="zoom:50%;"></p><h3 id="CONSTRUCT-Query"><a href="#CONSTRUCT-Query" class="headerlink" title="CONSTRUCT Query"></a><strong>CONSTRUCT Query</strong></h3><p><img src="/2022/01/14/sparql/CONSTRUCT Query.png" alt style="zoom:50%;"></p><h2 id="Updates"><a href="#Updates" class="headerlink" title="Updates"></a><strong>Updates</strong></h2><h3 id="INSERT-And-DELETE-Triples"><a href="#INSERT-And-DELETE-Triples" class="headerlink" title="INSERT And DELETE Triples"></a>INSERT And DELETE Triples</h3><p><img src="/2022/01/14/sparql/INSERT And DELETE Triples.png" alt style="zoom:50%;"></p><h3 id="INSERT-Query"><a href="#INSERT-Query" class="headerlink" title="INSERT Query"></a><strong>INSERT Query</strong></h3><p><img src="/2022/01/14/sparql/INSERT Query.png" alt style="zoom:50%;"></p><h3 id="INSERT-And-DELETE-Query"><a href="#INSERT-And-DELETE-Query" class="headerlink" title="INSERT And DELETE Query"></a>INSERT And DELETE Query</h3><p><img src="/2022/01/14/sparql/INSERT And DELETE Query.png" alt style="zoom:50%;"></p><h3 id="Graph-Management"><a href="#Graph-Management" class="headerlink" title="Graph Management"></a><strong>Graph Management</strong></h3><p><img src="/2022/01/14/sparql/Graph Management.png" alt style="zoom:50%;"></p><h2 id="Named-Graphs"><a href="#Named-Graphs" class="headerlink" title="Named Graphs"></a><strong>Named Graphs</strong></h2><h3 id="RDF-Datasets"><a href="#RDF-Datasets" class="headerlink" title="RDF Datasets"></a>RDF Datasets</h3><p><img src="/2022/01/14/sparql/RDF Datasets.png" alt style="zoom:50%;"></p><h3 id="RDF-Data-in-TriG-Turtle-with-Named-Graphs"><a href="#RDF-Data-in-TriG-Turtle-with-Named-Graphs" class="headerlink" title="RDF Data in TriG: Turtle with Named Graphs"></a><strong>RDF Data in TriG: Turtle with Named Graphs</strong></h3><p><img src="/2022/01/14/sparql/Turtle with Named Graphs.png" alt style="zoom:50%;"></p><h3 id="Specifying-SPARQL-Dataset"><a href="#Specifying-SPARQL-Dataset" class="headerlink" title="Specifying SPARQL Dataset"></a><strong>Specifying SPARQL Dataset</strong></h3><p><img src="/2022/01/14/sparql/Specifying SPARQL Dataset1.png" alt style="zoom:50%;"></p><p><img src="/2022/01/14/sparql/Specifying SPARQL Dataset2.png" alt style="zoom:50%;"></p><h3 id="Querying-a-Specific-Dataset"><a href="#Querying-a-Specific-Dataset" class="headerlink" title="Querying a Specific Dataset"></a>Querying a Specific Dataset</h3><p><img src="/2022/01/14/sparql/Querying a Specific Dataset.png" alt style="zoom:50%;"></p><h3 id="Named-Graph-Query"><a href="#Named-Graph-Query" class="headerlink" title="Named Graph Query"></a><strong>Named Graph Query</strong></h3><p><img src="/2022/01/14/sparql/Named Graph Query.png" alt style="zoom:50%;"></p><h3 id="Override-Default-Graph"><a href="#Override-Default-Graph" class="headerlink" title="Override Default Graph"></a><strong>Override Default Graph</strong></h3><p><img src="/2022/01/14/sparql/Override Default Graph.png" alt style="zoom:50%;"></p><h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><p><a href="https://www.ruanyifeng.com/blog/2020/02/sparql.html">RDF 和 SPARQL 初探：以维基数据为例</a></p><p><a href="https://zhuanlan.zhihu.com/p/32703794">RDF查询语言SPARQL</a></p><p><a href="https://info.stardog.com/hubfs/Stardog%20Academy%20-%20Stage%203%20Fundamentals_Slide%20Decks/Stardog%20Academy%20Video%203_%20Getting%20Started%20with%20RDF%20and%20SPARQL.pdf">Stardog</a></p>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SPARQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Getting Started with RDF</title>
      <link href="/2022/01/13/rdf/"/>
      <url>/2022/01/13/rdf/</url>
      
        <content type="html"><![CDATA[<p>资源描述框架(Resource Description Framework，RDF)是一个使用<a href="https://baike.baidu.com/item/XML">XML</a>语法来表示的资料模型(Datamodel),用来描述Web资源的特性,及资源与资源之间的关系 。</p><p>资源描述框架为表示数据及其含义提供了一个标准化的通用模型</p><ul><li>轻松支持混合、多样和不断变化的数据模型</li><li>易于表示数据或模式中的任何更改</li><li>可互操作和可组合</li></ul><h2 id="RDF模型的定义"><a href="#RDF模型的定义" class="headerlink" title="RDF模型的定义"></a>RDF模型的定义</h2><p>RDF提出了一个简单的二元关系模型来表示事物之间的语义关系，即使用三元组集合的方式来描述事物和关系。三元组是知识图谱中<strong>知识表示的基本单位，简称SPO</strong>，三元组被用来表示<strong>实体与实体之间的关系,或者实体的某个属性的属性值是什么</strong>。</p><p>从内容上看三元组的结构为 “<strong>资源-属性-属性值</strong>” ，资源<strong>实体由URI表示</strong>，属性值可以是另一个资源实体的URI，也可以是某种数据类型的值,也称为literals（字面量）。</p><p>主语和宾语也可以由第三种结点类型空节点（blank nodes）表示。blank node简单来说就是没有IRI和literal的资源，或者说匿名资源。</p><p>由于RDF规定资源的命名必须使用URI，所以也直接解决了命名空间的问题。这里我们具体说一下<strong>IRI，URI，URL和URN这几个术语的区别</strong>：</p><ul><li>URI：<strong>统一资源标识符</strong>，字符集被限制为US-ASCII（英文字符），通过指定唯一名称来标识资源；</li><li>IRI：<strong>国际化资源标识符</strong>（Internationalized Resource Identifier），定义与URI相同，URI，只是将字符集扩展到通用字符集（包含了非英文字符），所以它是URI的超集，同样唯一标识了一个资源；</li><li>URN: <strong>统一资源名称</strong>（Uniform Resource Name），由命名空间标识符（NID）和命名空间特定字符串（NSS）组成；</li><li>URL：<strong>统一资源定位符</strong>，即我们通常提到的网址，通常指的是不包含URN的URI子集</li></ul><p>以及它们的<strong>集合包涵关系</strong>：</p><ul><li>IRI ⊃ URI</li><li>URI ⊃ URL</li><li>URI ⊃ URN</li><li>URL ∩ URN = ∅</li></ul><h2 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h2><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午5.17.36.png" alt style="zoom:50%;"></p><p>首先假设：类别“artist”包括solo artist和band，band成员是solo artist。</p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午5.35.15.png" alt style="zoom:50%;"></p><p>RDF描述这些关系的方式基于我们如何用言语表达。如果我们说“<strong>The Beatles</strong> has as a member <strong>Paul McCartney</strong>”，那么用RDF来表示就如图下所示：</p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午5.36.36.png" alt style="zoom:50%;"></p><h2 id="Objects"><a href="#Objects" class="headerlink" title="Objects"></a>Objects</h2><p>类（<strong>class</strong>）是指某种类型的事物（如artist和band），一个类由一组个体（<strong>individuals</strong>），如披头士或约翰·列侬组成，也可称为实例或对象。类或个体可以是RDF三元组结构中的<strong>subject</strong>或<strong>object</strong>。    </p><h2 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h2><p>RDF三元组的中间部分是谓词（<strong>predicate</strong>），有两种用法。当它在我们的模型中描述两个对象（<strong>classes</strong>或<strong>individuals</strong>）之间的关系时，它被称为对象属性（<strong>object property</strong>）。</p><p>如果谓词提供有关对象的数据（数字、日期、字符串等），则称为描述属性的数据属性（<strong>attribute</strong>）。</p><h2 id="Graph"><a href="#Graph" class="headerlink" title="Graph"></a>Graph</h2><p>上面这些元素合在一起构成了一个图（<strong>graph</strong>）。在图中，表示对象或数据的点称为节点（<strong>node</strong>），而连接它们的谓词（对象属性或数据属性）称为边（<strong>edge</strong>）。</p><h2 id="RDF-Concepts"><a href="#RDF-Concepts" class="headerlink" title="RDF Concepts"></a><strong>RDF Concepts</strong></h2><ul><li><strong>IRI:</strong> Nodes and edges with a unique identifier</li><li><strong>Literal:</strong> Nodes representing values like numbers and dates</li><li><strong>Blank node:</strong> Nodes without an explicit identifier</li></ul><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午5.54.24.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午5.54.54.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.02.03.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.02.33.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.03.16.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.03.46.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.04.30.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.04.54.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.07.27.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.05.12.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.07.58.png" alt style="zoom:50%;"></p><p><img src="/2022/01/13/rdf/截屏2022-01-13 下午6.08.44.png" alt style="zoom:50%;"></p><h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://www.stardog.com/trainings/getting-started-with-rdf-sparql/">STARDOG</a></p><p><a href="https://www.cnblogs.com/xiaoqi/p/kg-study-part-1.html">知识图谱学习笔记</a></p>]]></content>
      
      
      <categories>
          
          <category> Tutorial </category>
          
      </categories>
      
      
        <tags>
            
            <tag> RDF </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>知识图谱学习笔记</title>
      <link href="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/"/>
      <url>/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/</url>
      
        <content type="html"><![CDATA[<p>2012 年 5 月 17 日，Google 正式提出了<strong>知识图谱（Knowledge Graph）</strong>的概念，其初衷是为了优化搜索引擎返回的结果，增强用户搜索质量及体验。传统的互联网技术是基于关键字匹配，然后通过一系列的打分策略来返回搜索结果的。这种方法的一个很大的缺陷就是无法让计算机理解背后的语义，因此构建知识图谱的目的就是为了解决这一问题，比如在Google搜索引擎里输入“Who is the wife of Bill Gates?”，我们直接可以得到答案-“Melinda Gates”。这是因为系统层面上已经创建好了一个包含“Bill Gates”和“Melinda Gates”的实体以及他俩之间关系的知识库。所以，当我们执行搜索的时候，就可以通过关键词提取（”Bill Gates”, “Melinda Gates”, “wife”）以及知识库上的匹配可以直接获得最终的答案。通过知识图谱，让机器可以具备认知能力，理解这个世界。</p><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 上午11.36.31.png" alt style="zoom:100%;"></p><h2 id="知识图谱简介"><a href="#知识图谱简介" class="headerlink" title="知识图谱简介"></a>知识图谱简介</h2><p>知识图谱是一种基于图的语义网络，用来展示实体间的关系。由下图可见一个知识图谱是由节点和边组成的。﻿在知识图谱里，我们通常用“实体（Entity）”来表达图里的节点、用“关系（Relation）”来表达图里的“边”。实体指的是现实世界中的事物比如人、地名、概念、药物、公司等，关系则用来表达不同实体之间的某种联系，比如人-“居住在”-北京、张三和李四是“朋友”、逻辑回归是深度学习的“先导知识”等等。</p><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 上午11.00.57.png" alt="构建知识图谱流程" style="zoom:50%;"></p><p>﻿在现实世界中，实体和关系也会拥有各自的属性，比如人可以有“姓名”和“年龄”。</p><h2 id="知识图谱中的本体"><a href="#知识图谱中的本体" class="headerlink" title="知识图谱中的本体"></a>知识图谱中的本体</h2><p>本体的概念来源自哲学，指的是对客观存在系统的解释和说明。</p><p>下面这张图就是一个本体的展示。</p><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 上午11.57.20.png" alt="Yago知识库中的本体" style="zoom:50%;"></p><h2 id="知识图谱的架构"><a href="#知识图谱的架构" class="headerlink" title="知识图谱的架构"></a>知识图谱的架构</h2><p>知识图谱在架构上分，可以分为<strong>逻辑架构和技术架构</strong>。</p><h3 id="逻辑架构"><a href="#逻辑架构" class="headerlink" title="逻辑架构"></a>逻辑架构</h3><p>知识图谱在逻辑上可分为<strong>模式层</strong>与<strong>数据层</strong>两个层次。</p><ul><li>模式层构建在数据层之上，是知识图谱的核心，通常采用本体库来管理知识图谱的模式层。本体是结构化知识库的概念模板，通过本体库而形成的知识库不仅层次结构较强，并且冗余程度较小。</li></ul><p><strong>模式层：实体-关系-实体，实体-属性-性值</strong></p><ul><li>数据层主要是由一系列的事实组成，而知识将以事实为单位进行存储。如果用(实体1，关系，实体2)、(实体、属性，属性值)这样的三元组来表达事实，可选择图数据库作为存储介质，例如开源的Neo4j、Twitter的FlockDB、sones的GraphDB等。</li></ul><p><strong>数据层：比尔盖茨-妻子-梅琳达·盖茨，比尔盖茨-总裁-微软</strong></p><h3 id="技术架构"><a href="#技术架构" class="headerlink" title="技术架构"></a>技术架构</h3><p>知识图谱的整体架构如下图所示，其中虚线框内的部分为知识图谱的构建过程，同时也是知识图谱更新的过程。</p><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 下午2.22.45.png" alt="知识图谱的技术架构" style="zoom:50%;">    </p><p>我们来一步一步的分析下这张图。</p><ol><li>虚线框的最左边是三种输入数据结构，结构化数据、半结构化数据、非结构化数据。这些数据可以来自任何地方，只要它对要构建的这个知识图谱有帮助。</li><li>虚线框里面的是整个的知识图谱的构建过程。其中主要包含了3个阶段，信息抽取、知识融合、知识加工。</li><li>最右边是生成的知识图谱，而且这个技术架构是循环往复，迭代更新的过程。知识图谱不是一次性生成，是慢慢积累的过程。</li></ol><ul><li><strong>信息抽取</strong>：从各种类型的数据源中提取出实体、属性以及实体间的相互关系，在此基础上形成本体化的知识表达；</li><li><strong>知识融合</strong>：在获得新知识之后，需要对其进行整合，以消除矛盾和歧义，比如某些实体可能有多种表达，某个特定称谓也许对应于多个不同的实体等；</li><li><strong>知识加工</strong>：对于经过融合的新知识，需要经过质量评估之后（部分需要人工参与甄别），才能将合格的部分加入到知识库中，以确保知识库的质量。</li></ul><h2 id="知识图谱数据模型"><a href="#知识图谱数据模型" class="headerlink" title="知识图谱数据模型"></a>知识图谱数据模型</h2><p>知识图谱的原始数据类型一般来说有三类（也是互联网上的三类原始数据）：</p><ul><li><strong>结构化数据（Structed Data）</strong>：如关系数据库</li><li><strong>半结构化数据（Semi-Structed Data）</strong>：如XML、JSON、百科</li><li><strong>非结构化数据（UnStructed Data）</strong>：如图片、音频、视频、文本</li></ul><p>如何存储上面这三类数据类型呢？一般有两种选择，一个是通过RDF（资源描述框架）这样的规范存储格式来进行存储，还有一种方法，就是使用图数据库来进行存储，常用的有Neo4j等。</p><h3 id="RDF"><a href="#RDF" class="headerlink" title="RDF"></a>RDF</h3><p>RDF(Resource Description Framework)，即资源描述框架，其本质是一个数据模型（Data Model）。它提供了一个统一的标准，用于描述实体/资源。简单来说，就是表示事物的一种方法和手段。RDF形式上表示为SPO三元组，有时候也称为一条语句（statement），知识图谱中我们也称其为一条知识，如下图。</p><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 下午2.10.49.png" alt="SPO三元组" style="zoom:50%;"></p><p>RDF在设计上的主要特点是易于发布和分享数据，但RDF的表达能力有限，无法区分类和对象，也无法定义和描述类的关系/属性。RDF是对具体事物的描述，缺乏抽象能力，无法对同一个类别的事物进行定义和描述。例如罗纳尔多和里约热内卢这两个实体，RDF能够表达罗纳尔多和里约热内卢这两个实体具有哪些属性，以及它们之间的关系。但如果我们想定义罗纳尔多是人，里约热内卢是地点，并且人具有哪些属性，地点具有哪些属性，人和地点之间存在哪些关系，这个时候RDF就表示无能为力了。不论是在智能的概念上，还是在现实的应用当中，这种泛化抽象能力都是相当重要的；同时，这也是知识图谱本身十分强调的。RDFS和OWL这两种技术或者说模式语言/本体语言（schema/ontology language）解决了RDF表达能力有限的困境。</p><h3 id="RDFS"><a href="#RDFS" class="headerlink" title="RDFS"></a>RDFS</h3><p>RDFS，即“Resource Description Framework Schema”。是在RDF数据层基础上引入模式层，定义类，属性，关系，属性的定义域与值域来描述与约束资源。</p><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 下午2.26.46.png" alt="RDFS" style="zoom:50%;"></p><h3 id="OWL"><a href="#OWL" class="headerlink" title="OWL"></a>OWL</h3><p>上面提到，RDFS本质上是RDF词汇的一个扩展。后来人们发现RDFS的表达能力还是相当有限，因此提出了OWL。我们也可以把OWL当做是RDFS的一个扩展，其添加了额外的预定义词汇。</p><p>OWL，即“Web Ontology Language”，语义网技术栈的核心之一。OWL有两个主要的功能：</p><ol><li><p>提供快速、灵活的数据建模能力。</p></li><li><p>高效的自动推理。</p></li></ol><p><img src="/2022/01/13/zhi-shi-tu-pu-xue-xi-bi-ji/截屏2022-01-13 下午2.30.31.png" alt="OWL" style="zoom:50%;"></p><p>描述属性特征的词汇</p><ol><li><p>owl:TransitiveProperty. 表示该属性具有传递性质。例如，我们定义“位于”是具有传递性的属性，若A位于B，B位于C，那么A肯定位于C。</p></li><li><p>owl:SymmetricProperty. 表示该属性具有对称性。例如，我们定义“认识”是具有对称性的属性，若A认识B，那么B肯定认识A。</p></li><li><p>owl:FunctionalProperty. 表示该属性取值的唯一性。 例如，我们定义“母亲”是具有唯一性的属性，若A的母亲是B，在其他地方我们得知A的母亲是C，那么B和C指的是同一个人。</p></li><li><p>owl:inverseOf. 定义某个属性的相反关系。例如，定义“父母”的相反关系是“子女”，若A是B的父母，那么B肯定是A的子女。</p></li></ol><p>本体映射词汇（Ontology Mapping）</p><ol><li><p>owl:equivalentClass. 表示某个类和另一个类是相同的。</p></li><li><p>owl:equivalentProperty. 表示某个属性和另一个属性是相同的。</p></li><li><p>owl:sameAs. 表示两个实体是同一个实体。</p></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p><a href="https://www.cnblogs.com/huangyc/p/10043749.html">通俗易懂解释知识图谱</a></p><p><a href="https://www.bilibili.com/video/BV1Kg4y1i7GN?spm_id_from=333.999.0.0">知识图谱原理与应用概述-北京大学邹磊教授</a></p><p><a href="https://zhuanlan.zhihu.com/p/32122644">知识图谱基础之RDF，RDFS与OWL</a></p>]]></content>
      
      
      <categories>
          
          <category> Knowledge Graph </category>
          
      </categories>
      
      
        <tags>
            
            <tag> neo4j </tag>
            
            <tag> py2neo </tag>
            
            <tag> yago </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Mac上安装Neo4j教程</title>
      <link href="/2022/01/11/mac-shang-an-zhuang-neo4j-jiao-cheng/"/>
      <url>/2022/01/11/mac-shang-an-zhuang-neo4j-jiao-cheng/</url>
      
        <content type="html"><![CDATA[<p>一般情况下，我们使用数据库查找事物间的联系的时候，只需要<strong>短程关系</strong>的查询（两层以内的关联）。当需要进行更长程的，更广范围的关系查询时，就需要图数据库的功能。</p><p>而随着社交、电商、金融、零售、物联网等行业的快速发展，现实世界的事物之间织起了一张巨大复杂的关系网，传统数据库面对这样复杂关系往往束手无策。因此，图数据库应运而生。</p><p>图数据库(Graph database)指的是以图数据结构的形式来存储和查询数据的数据库。</p><p>Neo4j 是目前用的最多的图数据库，世界数据库排行榜上排名21位。下面讲介绍一下Neo4j在Mac上的安装流程。</p><ol><li><p>首先，去官网下载<a href="https://neo4j.com/download/other-releases/">社区版neo4j</a></p></li><li><p>然后解压下载tar文件<img src="/2022/01/11/mac-shang-an-zhuang-neo4j-jiao-cheng/截屏2022-01-11 下午4.40.50.png" alt style="zoom: 50%;"></p></li><li><p>然后打开终端，cd到解压的文件夹的bin目录下，输入./neo4j start</p></li><li><p>进入<a href="http://localhost:7474/">http://localhost:7474/</a></p></li><li><p>如果想要退出，cd到解压的文件夹的bin目录下，输入./neo4j stop即可</p></li><li><p>在.bash_profile文件中添加如下两行代码，然后在终端输入source ~/.bash_profile 并回车</p><pre class="line-numbers language-none"><code class="language-none">export NEO4J_HOME= /Users/haoranwan/neo4j-community-4.4.2export PATH=$PATH:$NEO4J_HOME/bin<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre></li></ol><p>在安装中，java版本的不匹配也可能导致安装失败。下面将详细介绍如何安装对应的java版本。首先去oracle下载jdk11。然后打开.bash_profile，配置java路径并设置默认java版本。最后在终端输入source ~/.bash_profile 并回车</p><pre class="line-numbers language-none"><code class="language-none">export JAVA_11_HOME="/Library/Java/JavaVirtualMachines/jdk-11.0.13.jdk/Contents/Home"alias java11='export JAVA_HOME=$JAVA_11_HOME'export JAVA_17_HOME="/Library/Java/JavaVirtualMachines/jdk-17.0.1.jdk/Contents/Home"alias java17='export JAVA_HOME=$JAVA_17_HOME'# 默认使用java11export JAVA_HOME=$JAVA_11_HOME#java END%<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>然后可以输入<code>java -version</code>来查询当前版本</p><p>可以输入java17 or java+其他数字来切换java版本</p>]]></content>
      
      
      <categories>
          
          <category> Installment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> neo4j </tag>
            
            <tag> java </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>正则表达式</title>
      <link href="/2022/01/10/zheng-ze-biao-da-shi/"/>
      <url>/2022/01/10/zheng-ze-biao-da-shi/</url>
      
        <content type="html"><![CDATA[<p>正则表达式(regular expression)描述了一种字符串匹配的模式（pattern），可以用来检查一个串是否含有某种子串、将匹配的子串替换或者从某个串中取出符合某个条件的子串等。</p><p>在线测试正则表达式网站：<a href="https://regex101.com">regex101</a></p><p><img src="/2022/01/10/zheng-ze-biao-da-shi/截屏2021-12-28 下午2.58.08.png" alt style="zoom: 33%;"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python">import re# matching stringpattern1 = "cat"pattern2 = "bird"string = "dog runs to cat"print(pattern1 in string)    # Trueprint(pattern2 in string)    # False# regular expressionpattern1 = "cat"pattern2 = "bird"string = "dog runs to cat"print(re.search(pattern1, string))  # &lt;_sre.SRE_Match object; span=(12, 15), match='cat'&gt;print(re.search(pattern2, string))  # None# multiple patterns ("run" or "ran")ptn = r"r[au]n"       # start with "r" means raw stringprint(re.search(ptn, "dog runs to cat"))    # &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;# continueprint(re.search(r"r[A-Z]n", "dog runs to cat"))     # Noneprint(re.search(r"r[a-z]n", "dog runs to cat"))     # &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;print(re.search(r"r[0-9]n", "dog r2ns to cat"))     # &lt;_sre.SRE_Match object; span=(4, 7), match='r2n'&gt;print(re.search(r"r[0-9a-z]n", "dog runs to cat"))  # &lt;_sre.SRE_Match object; span=(4, 7), match='run'&gt;# \d : decimal digitprint(re.search(r"r\dn", "run r4n"))                # &lt;_sre.SRE_Match object; span=(4, 7), match='r4n'&gt;# \D : any non-decimal digitprint(re.search(r"r\Dn", "run r4n"))                # &lt;_sre.SRE_Match object; span=(0, 3), match='run'&gt;# \s : any white space [\t\n\r\f\v]print(re.search(r"r\sn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(0, 3), match='r\nn'&gt;# \S : opposite to \s, any non-white spaceprint(re.search(r"r\Sn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(4, 7), match='r4n'&gt;# \w : [a-zA-Z0-9_]print(re.search(r"r\wn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(4, 7), match='r4n'&gt;# \W : opposite to \wprint(re.search(r"r\Wn", "r\nn r4n"))               # &lt;_sre.SRE_Match object; span=(0, 3), match='r\nn'&gt;# \b : empty string (only at the start or end of the word)print(re.search(r"\bruns\b", "dog runs to cat"))    # &lt;_sre.SRE_Match object; span=(4, 8), match='runs'&gt;# \B : empty string (but not at the start or end of a word)print(re.search(r"\B runs \B", "dog   runs  to cat"))  # &lt;_sre.SRE_Match object; span=(8, 14), match=' runs '&gt;# \\ : match \print(re.search(r"runs\\", "runs\ to me"))          # &lt;_sre.SRE_Match object; span=(0, 5), match='runs\\'&gt;# . : match anything (except \n)print(re.search(r"r.n", "r[ns to me"))              # &lt;_sre.SRE_Match object; span=(0, 3), match='r[n'&gt;# ^ : match line beginningprint(re.search(r"^dog", "dog runs to cat"))        # &lt;_sre.SRE_Match object; span=(0, 3), match='dog'&gt;# $ : match line endingprint(re.search(r"cat$", "dog runs to cat"))        # &lt;_sre.SRE_Match object; span=(12, 15), match='cat'&gt;# ? : may or may not occurprint(re.search(r"Mon(day)?", "Monday"))            # &lt;_sre.SRE_Match object; span=(0, 6), match='Monday'&gt;print(re.search(r"Mon(day)?", "Mon"))               # &lt;_sre.SRE_Match object; span=(0, 3), match='Mon'&gt;# multi-linestring = """dog runs to cat.I run to dog."""print(re.search(r"^I", string))                     # Noneprint(re.search(r"^I", string, flags=re.M))         # &lt;_sre.SRE_Match object; span=(18, 19), match='I'&gt;# * : occur 0 or more timesprint(re.search(r"ab*", "a"))                       # &lt;_sre.SRE_Match object; span=(0, 1), match='a'&gt;print(re.search(r"ab*", "abbbbb"))                  # &lt;_sre.SRE_Match object; span=(0, 6), match='abbbbb'&gt;# + : occur 1 or more timesprint(re.search(r"ab+", "a"))                       # Noneprint(re.search(r"ab+", "abbbbb"))                  # &lt;_sre.SRE_Match object; span=(0, 6), match='abbbbb'&gt;# {n, m} : occur n to m timesprint(re.search(r"ab{2,10}", "a"))                  # Noneprint(re.search(r"ab{2,10}", "abbbbb"))             # &lt;_sre.SRE_Match object; span=(0, 6), match='abbbbb'&gt;# groupmatch = re.search(r"(\d+), Date: (.+)", "ID: 021523, Date: Feb/12/2017")print(match.group())                                # 021523, Date: Feb/12/2017print(match.group(1))                               # 021523print(match.group(2))                               # Date: Feb/12/2017match = re.search(r"(?P&lt;id&gt;\d+), Date: (?P&lt;date&gt;.+)", "ID: 021523, Date: Feb/12/2017")print(match.group('id'))                            # 021523print(match.group('date'))                          # Date: Feb/12/2017# findallprint(re.findall(r"r[ua]n", "run ran ren"))         # ['run', 'ran']# | : orprint(re.findall(r"(run|ran)", "run ran ren"))      # ['run', 'ran']# re.sub() replaceprint(re.sub(r"r[au]ns", "catches", "dog runs to cat"))     # dog catches to cat# re.split()print(re.split(r"[,;\.]", "a;b,c.d;e"))             # ['a', 'b', 'c', 'd', 'e']# compilecompiled_re = re.compile(r"r[ua]n")print(compiled_re.search("dog ran to cat"))     # &lt;_sre.SRE_Match object; span=(4, 7), match='ran'&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>贪婪模式：会匹配最长的以开始位置开始，以结束位置结束的字符串；</p><p>懒惰模式：匹配尽可能少的字符。</p><p><img src="/2022/01/10/zheng-ze-biao-da-shi/截屏2022-01-10 下午3.27.32.png" alt style="zoom: 33%;"></p><p><img src="/2022/01/10/zheng-ze-biao-da-shi/截屏2022-01-10 下午3.28.07.png" alt style="zoom: 33%;"></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Regular Expression </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法-二分法</title>
      <link href="/2022/01/06/suan-fa-er-fen-fa/"/>
      <url>/2022/01/06/suan-fa-er-fen-fa/</url>
      
        <content type="html"><![CDATA[<p>二分法主要是通过左右逼近的方式，向目标值靠拢，并且取得对数复杂度的算法。</p><p>下面这段代码就是一般二分法的代码，不过应该如何理解二分法的本质呢？</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">def search(self, nums: List[int], target: int) -&gt; int:left, right = 0, len(nums) - 1while left &lt;= right: mid = left + (right - left)//2if nums[mid] &lt; target: # select1 [&lt;, &lt;=]left = mid + 1else:right = mid - 1return left # select2 left, right<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在做题的时候经常会对上面的选择部分感到迷惑，在具体情况下应该怎么选择呢？</p><p>我先设定一个数组——&gt;[1,2,3,5,6,6,6,6,8,8,9]</p><p>首先我们要先明确一个概念，在while left &lt;= right这个条件下，我们要确保数组的完整性，所以left, right = 0, len(nums) - 1，这样可以保证index的取值空间为[0, len(nums) - 1]这个闭区间。其次while的终止条件如下面表格所示：</p><div class="table-container"><table><thead><tr><th>1</th><th>2</th><th>3</th><th>5</th><th>6</th><th>6</th><th>6</th><th>6</th><th>8</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>L</td><td></td><td></td><td></td><td></td><td>M</td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td></td><td></td><td></td><td>R</td><td>L</td><td></td><td></td><td></td><td></td></tr></tbody></table></div><p>由表格可见，在左右指针交换位置的时候终止循环。那么L和R指针分别的意义是什么呢？</p><p>场景1：寻找小于target的位置，则目标位置是6，index为3</p><p>则在终止循环前的一个循环时L,R的位置如下：</p><p>如果select1为&lt;时，等价于求&lt;6和first&gt;=6，如下所示：</p><div class="table-container"><table><thead><tr><th>1</th><th style="text-align:left">2</th><th>3</th><th>5</th><th style="text-align:center">6</th><th>6</th><th>6</th><th>6</th><th>8</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>L</td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center"></td><td></td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td>L</td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center">R</td><td></td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td style="text-align:left"></td><td></td><td>L</td><td style="text-align:center">R</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center">L,R</td><td></td><td></td><td></td><td></td><td></td><td></td></tr><tr><td></td><td style="text-align:left"></td><td></td><td>R</td><td style="text-align:center">L</td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table></div><p>如果select1为&lt;=时，等价于求 &gt;6 和last &lt;= 6，如下所示：</p><div class="table-container"><table><thead><tr><th>1</th><th style="text-align:left">2</th><th>3</th><th>5</th><th style="text-align:center">6</th><th>6</th><th>6</th><th>6</th><th>8</th><th>8</th><th>9</th></tr></thead><tbody><tr><td>L</td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center"></td><td></td><td></td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center"></td><td></td><td>L</td><td></td><td></td><td></td><td>R</td></tr><tr><td></td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center"></td><td></td><td>L</td><td>R</td><td></td><td></td><td></td></tr><tr><td></td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center"></td><td></td><td></td><td>L,R</td><td></td><td></td><td></td></tr><tr><td></td><td style="text-align:left"></td><td></td><td></td><td style="text-align:center"></td><td></td><td></td><td>R</td><td>L</td><td></td></tr></tbody></table></div><p>场景1：寻找小于target的位置</p><p>场景2：寻找大于target的位置</p><p>场景3：寻找小于等于target的首次出现的位置</p><p>场景4：寻找小于等于target的末次出现的位置</p><pre class="line-numbers language-text" data-language="text"><code class="language-text">// 模板int BinarySearch(int *arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt; target) { // select [&lt;, &lt;=]            left = mid + 1;        } else {            right = mid - 1;        }    }    return right; // select [left, right]}// 4种场景对应的代码：// 场景1：&lt; 6int BinarySearchLowerTarget(int *arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt; target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return right;}// 场景2：&gt; 6int BinarySearchHigherTarget(int* arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt;= target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return left;}// 场景3：&lt;=6, 取首int BinarySearchFirstLowerEqualTarget(int* arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt; target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return left;}// 场景4：&lt;=6, 取尾int BinarySearchLastLowerEqualTarget(int* arr, int arrSize, int target) {    int left = 0;    int right = arrSize - 1;    int mid;    while (left &lt;= right) {        mid = left + (right - left) / 2;        if (arr[mid] &lt;= target) {            left = mid + 1;        } else {            right = mid - 1;        }    }    return right;}<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p><strong>总结：</strong></p><ol><li><p>L一定是要+1，R一定是-1，因为区间要不断收缩才能退出，如果没有+1，和-1，会导致死循环风险。</p></li><li><p>while (left &lt;= right)中的退出条件是，left &gt; right,出现了翻转，此时right跑到left左边。</p></li><li><p>正对4种场景只需要修改if中的符号和返回值。</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Binary Search </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>NER学习整理</title>
      <link href="/2021/12/31/ner-xue-xi-zheng-li/"/>
      <url>/2021/12/31/ner-xue-xi-zheng-li/</url>
      
        <content type="html"><![CDATA[<h2 id="什么是NER"><a href="#什么是NER" class="headerlink" title="什么是NER"></a>什么是NER</h2><p>命名体识别（Named Entity Recognition, NER），旨在识别文本中感兴趣的实体，如位置、组织和时间。已识别的实体可以在各种下游应用程序中使用，如根据患者记录去识别和信息提取系统，也可以作为机器学习系统的特性，用于其他自然语言处理任务。比如下面的例子：「Michael Jeffrey Jordan」是一个「Person」实体，「Brooklyn」是一个「Location」，「NewYork」也是一个「Location」。命名实体识别本质上是一个模式识别任务, 即给定一个句子, 识别句子中实体的边界和实体的类型是自然语言处理任务中一项重要且基础性的工作。</p><h2 id="NER相关综述类论文"><a href="#NER相关综述类论文" class="headerlink" title="NER相关综述类论文"></a>NER相关综述类论文</h2><ul><li><a href="https://arxiv.org/pdf/2101.11420.pdf">2021-Recent Trends in Named Entity Recognition (NER)</a></li><li><a href="https://arxiv.org/pdf/1812.09449.pdf">2020- Survey on Deep Learning for Named Entity Recognition</a></li><li><a href="https://arxiv.org/pdf/1910.11470.pdf">2019-A survey on recent advances in named entity recognition from deep learning models</a></li><li><a href="https://aclanthology.org/P18-3006.pdf">2018-Recognizing complex entity mentions: A review and future directions</a></li><li><a href="https://www.sciencedirect.com/science/article/pii/S1574013717302782">2018-Recent named entity recognition and classification techniques: A systematic review </a></li><li><a href="https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.714.342&amp;rep=rep1&amp;type=pdf">2013-Named entity recognition: fallacies, challenges and opportunities</a></li><li><a href="https://time.mk/trajkovski/thesis/li07.pdf">2007-A survey of named entity recognition and classification</a></li></ul><h2 id="NER数据集、评测方法、工具库"><a href="#NER数据集、评测方法、工具库" class="headerlink" title="NER数据集、评测方法、工具库"></a>NER数据集、评测方法、工具库</h2><h3 id="NER相关的数据集"><a href="#NER相关的数据集" class="headerlink" title="NER相关的数据集"></a>NER相关的数据集</h3><p>NER相关数据集可以参考：<a href="https://link.zhihu.com/?target=https%3A//github.com/SimmerChan/corpus">SimmerChan/corpus</a></p><h3 id="NER评测方法"><a href="#NER评测方法" class="headerlink" title="NER评测方法"></a>NER评测方法</h3><p>命名实体识别评测方式分为两种，一是通用的基于token标签进行直接评测，二是考虑实体边界+实体类型的评测，实际中往往是采用后者的评测方法，按照实体边界+实体类型的评测，分为exact match和relaxed match两类。对于exact match来说，只有命名实体的开始和结束索引以及类型完全正确才算正确，relaxed match只需要类型正确且将ground truth覆盖即可，基于此有Precision，Recall，F-Score等标准。另外，对于多实体类型，经常需要评估模型跨实体类的表现，这时候会用到macro-averaged F-Score和micro-average F-Score。</p><h3 id="NER标注方法"><a href="#NER标注方法" class="headerlink" title="NER标注方法"></a>NER标注方法</h3><p>一般来说，不同的NER任务会有不同的Tags，但都基本基于以下两种：</p><ul><li>BIO(B-begin，I-inside，O-outside)</li><li>BIOES(B-begin，I-inside，O-outside，E-end，S-single)</li></ul><p>其中，B 开始位置、I 中间位置、O 其他类别、E表示结束位置、S 单字表示一个实体。</p><h3 id="NER工具库"><a href="#NER工具库" class="headerlink" title="NER工具库"></a>NER工具库</h3><p>《Survey on Deep Learning for Named Entity Recognition》对学术界和工业界一些NER工具进行汇总，工具中通常都包含预训练模型，可以直接在自己的语料上做实体识别。不过一般研究使用的话（所定义实体类别与工具预定的不符），还需要依据待抽取领域语料再训练模型，或重新训练模型。</p><h2 id="输入的分布式表示"><a href="#输入的分布式表示" class="headerlink" title="输入的分布式表示"></a>输入的分布式表示</h2><p>分布式表示是使用维度向量表示词的信息，每一维都表示一种特征。在NER模型中使用的三种分布式表示：字级、字符级和混合表示。</p><ul><li><strong><em>Word - Level Representation：</em></strong>通常是预训练的词表示。可以使用bag of - words (CBOW) 或 continuous skip-gram models。CBOW本质是通过context word来预测target word，复杂度$O(vocabulary length)$，skip-gram的训练过程则和CBOW相反，但是复杂度为$O(vocabulary length * window length)$。但是在skip-gram当中，每个词都要收到周围的词的影响，每个词在作为中心词的时候，都要进行K次的预测、调整。因此， 当数据量较少，或者词为生僻词出现次数较少时， 这种多次的调整会使得词向量相对的更加准确。尽管cbow从另外一个角度来说，某个词也是会受到多次周围词的影响（多次将其包含在内的窗口移动），进行词向量的跳帧，但是他的调整是跟周围的词一起调整的，grad的值会平均分到该词上， 相当于该生僻词没有收到专门的训练，它只是沾了周围词的光而已。</li><li><strong><em>Character - Level Representation：</em></strong>字符级表示对于利用显式子词级信息（如前缀和后缀）非常有用。字符级表示法的另一个优点是，它不需要词汇表并且还能避免在模型输出上的计算瓶颈。此外，基于字符的模型能够推断不可见词的表示并且能弹性地处理拼写错误，并共享语素级规则的信息，可以解决OOV等信息。提取字符级别嵌入的方法有基于CNN的模型和基于RNN的模型。其中CNN中字符经过卷积池化获得字符级别的嵌入；RNN中为每个字符预测生成标签的分布，词级别的标签由字符级别的标签计算得到。Word - Level Representation和Character - Level Representation通常连接在一起表示。</li><li><strong><em>Hybrid Representation：</em></strong>除了单词级和字符级表示外，一些研究还将其他信息（如地名录、词汇相似性、语言依赖性和视觉特征纳入单词的最终表示，然后再输入上下文编码层。添加额外信息可能会导致NER性能的改善，代价是损害这些系统的通用性。相当于联合使用基于深度学习的方法和基于特征的方法。BiLSTM-CRF模型中融合4种特征：spelling features, context features, word embeddings, gazetteer features。</li></ul><h2 id="上下文编码器体系结构"><a href="#上下文编码器体系结构" class="headerlink" title="上下文编码器体系结构"></a>上下文编码器体系结构</h2><p>表示单词的一个简单选项是热向量表示。在一个热向量空间中，两个不同的单词具有完全不同的表示形式，并且是正交的。分布式表示表示低维实值密集向量中的单词，其中每个维度表示潜在特征。分布式表示从文本中自动学习，捕获单词的语义和句法属性，而这些属性在文本中并不显式出现，所以需要Context Encoder Architectures去学习这些属性。有主流的方法有：</p><ul><li><p><strong><em>Convolutional Neural Networks：</em></strong>使用每个词的嵌入作为输入，卷积层计算得到本地特征，将所有局部特征综合之后得到输入的全局特征。其中提取全局特征的方法可以对句中的位置进行最大或平均运算，然后将句子的全局特征将输入到tag decoder里计算所有可能标签的分布。即每个单词的标签由整个句子决定。ID-CNNs适用于大规模文本和结构化预测。</p></li><li><p><strong><em>Recurrent Neural Networks：</em></strong>RNN通常使用两大变种GRU、LSTM，利用门机制缓解长程依赖的问题。双向RNNs有效利用过去的信息和未来的信息，适合建模上下文存在依赖的表示。Recurrent是时间维度的展开（如下图所示），代表信息在时间维度从前往后的的传递和积累，可以类比markov假设，后面的信息的概率建立在前面信息的基础上，在神经网络结构上表现为后面的神经网络的隐藏层的输入是前面的神经网络的隐藏层的输出。</p></li><li><strong><em>Recursive Neural Networks：</em></strong>循环神经网络也是递归神经网路的一种。一般而言，循环神经网络是在时间维度上的递归，而递归神经网络是在结构上的递归。递归神经网络是一种自适应的非线性模型，能够按照拓扑顺序遍历给定结构来学习深层结构信息。</li><li><strong><em>Neural Language Models：</em></strong>通过前向/后向神经语言模型在分别给定前面/后面的词情况下预测当前词，通过根据双向RNN得到当前位置的表示，这种表示在序列标注任务中很实用。ELMo通过双层的双向语言模型可以词的复杂特定，例如，语法语义等。</li><li><strong><em>Deep Transformer：</em></strong>transformer使用堆叠的自注意力和逐点连接的全连接层构建编码器和解码器，彻底消除CNN和RNN。Bert模型（双向Transformer的Encoder）通过同时使用上下文可以更好的捕捉词语和句子级别的representation。</li></ul><h2 id="标签解码器结构"><a href="#标签解码器结构" class="headerlink" title="标签解码器结构"></a>标签解码器结构</h2><p>输入上下文相关的表示，输出标签序列。常用的解码器结构包括：MLP+softmax、CRFs、RNN和Pointer Network。</p><ul><li><strong><em>多层感知机+softmax：</em></strong>多层感知机和softmax层将序列标注任务转换成为多分类任务，为每个词标注tag。</li><li><strong><em>CRFs：</em></strong>基于深度学习的CRFs主要用在双向LSTM或者CNN层上，例如CoNLL03, OntoNotes5.0。CRFs的缺陷在于无法充分利用分段信息，因为分段信息无法完全使用词嵌入表示。改进方法是直接建模分段的表示，而不是词的表示。</li><li><strong><em>RNN：</em></strong>当命名实体类别很多时，使用RNN作为解码器表现更好，且更容易训练。</li><li><strong><em>Pointer Networks：</em></strong>指针网络应用在RNN上，输出位置下标集合，形成离散的位置范围。使用softmax概率分布作为指针，表示可变长的词典。NER中，PN将输入的序列分段，然后输出标签。</li></ul><h2 id="NER主要方法"><a href="#NER主要方法" class="headerlink" title="NER主要方法"></a>NER主要方法</h2><ol><li><strong><em>基于专家规则的方法：</em></strong>一般情况下，由于使用领域内的特定规则和不完整的词典，这种方法会出现precision高，recall低的情况。这种方法非常适合字典有限且内容量不是过多的的场景，而当已知据大多数词时，基于规则的NER方法效果很好。但是在一些内容种类和数量不断拓展的领域，这种方法需要耗费大量的人力成本和精力成本，并且可移植性差。</li><li><strong><em>无监督学习方法：</em></strong>典型的无监督NER方法是clustering，通过context相似度抽取命名实体。聚类算法基于上下文的相似性将命名实体集聚在特定的聚类簇中。常用到的特征或者辅助信息有词汇资源、语料统计信息（TF-IDF）、浅层语义信息（分块NP-chunking）等。该项技术往往依赖于词汇资源例如WordNet，依赖于词汇模式，依赖于统计等。</li><li><strong><em>监督学习方法：</em></strong>监督学习方法可以将NER转换为多分类问题或者序列标注任务。监督学习往往需要进行一些特征工程来提取特定的特征。监督学习中特征工程的设计非常重要，使用特征向量表示文本的摘要信息，一般有三种类型的特征：（1）布尔型特征 （2）数值型特征 （3）类别特征。模型选择有：隐马尔可夫模型、决策树、最大熵模型、最大熵马尔科夫模型、支持向量机、条件随机场。</li><li><strong><em>深度学习方法：</em></strong>将深度学习技术应用于NER有三个核心优势。首先，NER受益于非线性转换，它生成从输入到输出的非线性映射。与线性模型（如对数线性HMM和线性链CRF）相比，基于DL的模型能够通过非线性激活函数从数据中学习复杂的特征。第二，深度学习节省了设计NER特性的大量精力。传统的基于特征的方法需要大量的工程技术和领域专业知识。另一方面，基于DL的模型可以有效地从原始数据中自动学习有用的表示形式和潜在因素。第三，深层神经网络模型可以通过梯度下降法在端到端范式中进行训练。这一特性使我们能够设计可能复杂的NER系统。基于深度学习的NER模型有很多种，下面将列举几种深度学习解决NER的例子。</li></ol><h3 id="BiLSTM-CRF"><a href="#BiLSTM-CRF" class="headerlink" title="BiLSTM+CRF"></a>BiLSTM+CRF</h3><p>BiLSTM+CRF结构如下图所示：首先，句中的每个单词是一条包含词嵌入和字嵌入的词向量，词嵌入通常是事先训练好的，字嵌入则是随机初始化的。所有的嵌入都会随着训练的迭代过程被调整。其次，BiLSTM-CRF的输入是词嵌入向量，输出是每个单词对应的预测标签。</p><p><img src="https://pic1.zhimg.com/80/v2-80fc94217cfd3ae9d85636859a8b27bc_1440w.jpg" alt></p><p>BiLSTM层的输入表示该单词对应各个类别的分数。如W0，BiLSTM节点的输出是1.5 (B-Person), 0.9 (I-Person), 0.1 (B-Organization), 0.08 (I-Organization) and 0.05 (O)。这些分数将会是CRF层的输入。所有的经BiLSTM层输出的分数将作为CRF层的输入，类别序列中分数最高的类别就是预测的最终结果。</p><h5 id="为什么使用CRF层"><a href="#为什么使用CRF层" class="headerlink" title="为什么使用CRF层"></a>为什么使用CRF层</h5><p>使用CRF层的的原因是LSTM在进行序列建模时只考虑了输入序列的信息，无法对标签转移关系进行建模，以BIO标注为例，可能会出现如下问题：</p><p><strong>例如输入序列为：“北京烤鸭“</strong></p><p><strong>理想标注结果：“B-地名 ｜ I-地名 ｜B-食物 ｜ I-食物”</strong></p><p><strong>可能出现结果：“B-食物 ｜ I-地名 ｜B-地名 ｜ I-食物”</strong></p><p>这时候就需要考虑对标签转移矩阵和标签转移关系进行建模。对于状态关系转移这种问题，常见的模型有HMM（生成模型）和CRF（判别模型）两种。不过在这里选择CRF这种判别式模型。这是因为NER问题中，标签的特性都是固定的，如”B-食物“后只能加“I-食物”或者“O”，不能是”B-食物“后加“I-地名“。而生成模型有更高的泛化能力和普适性，也就意味着更高的计算复杂度，CRF能帮助发现数据中新的特性。下面一个例子将展示判别模型和生成模型的区别：</p><p><strong>假如有4个samples：</strong></p><div class="table-container"><table><thead><tr><th></th><th>sample1</th><th>sample2</th><th>sample3</th><th>Sample4</th></tr></thead><tbody><tr><td>x</td><td>0</td><td>0</td><td>1</td><td>1</td></tr><tr><td>y</td><td>0</td><td>0</td><td>0</td><td>1</td></tr></tbody></table></div><p><strong>生成模型的预测主要基于联合概率：</strong></p><script type="math/tex; mode=display">\sum_{}P(x,y)=1</script><p>​    </p><div class="table-container"><table><thead><tr><th>生成模型</th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=0</td><td>1/2</td><td>0</td></tr><tr><td>x=1</td><td>1/4</td><td>1/4</td></tr></tbody></table></div><p><strong>而判别模型的预测主要基于后验概率：</strong></p><script type="math/tex; mode=display">\sum_{y}P(y|x)=1</script><div class="table-container"><table><thead><tr><th>判别模型</th><th>y=0</th><th>y=1</th></tr></thead><tbody><tr><td>x=0</td><td>1</td><td>0</td></tr><tr><td>x=1</td><td>1/2</td><td>1/2</td></tr></tbody></table></div><p>因此结合LSTM和CRF，可以在LSTM进行序列建模的基础上对标签转移矩阵和标签转移关系进行建模。CRF层的作用是加入一些约束来保证最终预测结果是有效的。这些约束可以在训练数据时被CRF层自动学习得到。</p><p>可能的约束条件有：</p><ul><li><p>句子的开头应该是“B-label”或“O”，而不是“I- label？”。</p></li><li><p>“B-label1， I-label2， I-label3…”，在该模式中，类别1,2,3应该是同一种实体类别。比如，</p><p>“B-Person，I-Person” 是正确的，而“B-Person I-Organization”则是错误的。</p></li><li><p>“O，I-label”是错误的，命名实体的开头应该是“B-”而不是“I-”。</p></li></ul><p>有了这些有用的约束，错误的预测序列将会大大减少。</p><h5 id="CRF层损失函数"><a href="#CRF层损失函数" class="headerlink" title="CRF层损失函数"></a>CRF层损失函数</h5><p>CRF损失函数由两部分组成，<strong>发射分数(Emission Score)</strong> 和 <strong>转移分数(Transition Score)</strong>。真实路径的分数应该是所有路径中分数最高的。</p><h6 id="发射分数"><a href="#发射分数" class="headerlink" title="发射分数"></a>发射分数</h6><p>分别对单词和类别建立索引。如$i $是 word 的索引，$y$ 是 label 的索引，则发射分数为$X_i,_y$</p><p>$X_1,_0$意思就是词汇表第二个单词$W_1$转移到第一个标签$L_0$的概率。</p><h6 id="转移分数"><a href="#转移分数" class="headerlink" title="转移分数"></a>转移分数</h6><p>使用 $T_{y_i},_{y_j}$来表示转移分数。$y$代表不同Lable。为了使 transition 评分矩阵更健壮，需要添加另外两个标签，START 和 END。START 是指一个句子的开头，而不是第一个单词。END 表示句子的结尾。</p><p>建一个transition得分矩阵，用来存储所有标签之间的所有得分。该矩阵是 BiLSTM-CRF 模型的一个参数。在训练模型之前，可以随机初始化矩阵中的所有 transition 分数。所有的随机分数将在训练过程中自动更新。换句话说，CRF 层可以自己学习这些约束，不需要手动构建矩阵。随着训练迭代次数的增加，分数会逐渐趋于合理。</p><h6 id="路径分数"><a href="#路径分数" class="headerlink" title="路径分数"></a>路径分数</h6><p>$S_i=Emission Score + Transition Score$</p><p>$P_{real}=e^{s_{real}}$</p><p>$P_{total}=P_1+P_2+P_3+…+P_n=e^{s_1}+e^{s_2}+e^{s_3}+…+e^{s_n}$</p><h6 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h6><p>$LossFunction=\frac{P_{real}}{P_{total}}$</p><h3 id="IDCNN-CRF"><a href="#IDCNN-CRF" class="headerlink" title="IDCNN+CRF"></a>IDCNN+CRF</h3><p>尽管BILSTM在NER任务中有很好的表现，但是却不能充分利用GPU的并行性，因此出现了一种新的NER模型方案IDCNN+CRF。</p><p>尽管传统的CNN有明显的计算优势，但是传统的CNN在经过卷积之后，末梢神经元只能得到输入文本的一小部分信息，为了获取上下文信息，需要加入更多的卷积层，导致网络越来越深，参数越来越多，容易发生过拟合。</p><p>IDCNN的改进基于CNN做出了改进，卷积操作更换为空洞卷积，那么为什么要做空洞卷积呢？因为正常CNN卷积之后会接一个池化层，池化层会损失信息，降低精度，那么不如不加池化层，直接在卷积操作中增大感受野，所以就有了空洞卷积（如下图所示）。</p><p><img src="https://pic4.zhimg.com/v2-4959201e816888c6648f2e78cccfd253_b.jpg" alt style="zoom:67%;"></p><p>IDCNN为这片filter增加了一个dilation width，作用在输入矩阵的时候，会skip掉所有dilation width中间的输入数据；而filter矩阵本身的大小仍然不变，这样filter获取到了更广阔的输入矩阵上的数据，看上去就像是“膨胀”了一般。dilated width会随着层数的增加而指数增加。这样随着层数的增加，参数数量是线性增加的，而receptive field却是指数增加的，可以很快覆盖到全部的输入数据。图像的空洞卷积如下图所示：</p><p><img src="/2021/12/31/ner-xue-xi-zheng-li/截屏2021-12-10 上午10.53.09.png" alt style="zoom: 33%;"></p><p>对应到文本中的卷积操作如下图所示：</p><p><img src="/2021/12/31/ner-xue-xi-zheng-li/截屏2021-12-10 上午10.55.50.png" alt style="zoom:33%;"></p><p>IDCNN获取特征之后，与BiLSTM一样，将结果输入到CRF层，利用维特比解码，得到最终的标签。</p><h3 id="Bert-BiLSTM-CRF"><a href="#Bert-BiLSTM-CRF" class="headerlink" title="Bert+BiLSTM+CRF"></a>Bert+BiLSTM+CRF</h3><p>BERT的全称为Bidirectional Encoder Representation from Transformers，由谷歌2018年提出来，是一个预训练的语言表征模型。它采用新的masked language model（MLM），以致能生成深度的双向语言表征。BERT通过微调的方法可以灵活的应用到下游业务，所以可以考虑使用Bert作为embedding层，将特征输入到Bilstm+CRF中，以谋求更好的效果。</p><p>该模型有以下主要优点：</p><p>1）以往的预训练模型的结构会受到单向语言模型<em>（从左到右或者从右到左）</em>的限制，因而也限制了模型的表征能力，使其只能获取单方向的上下文信息。而BERT利用MLM进行预训练并且采用深层的双向Transformer组件/（单向的Transformer一般被称为Transformer decoder，而双向的Transformer则被称为Transformer encoder）来构建整个模型，因此最终生成能融合左右上下文信息的深层双向语言表征。Transformer结构如下：</p><p><img src="/2021/12/31/ner-xue-xi-zheng-li/截屏2021-12-10 上午11.36.43.png" alt style="zoom: 30%;"></p><p>而Transformer又可以进行堆叠，形成一个更深的神经网络：</p><p><img src="/2021/12/31/ner-xue-xi-zheng-li/截屏2021-12-10 上午11.51.32.png" alt style="zoom: 20%;"></p><p>MLM是BERT能够不受单向语言模型所限制的原因。简单来说就是以15%的概率用mask token （[MASK]）随机地对每一个训练序列中的token进行替换，然后预测出[MASK]位置原有的单词。然而，由于[MASK]并不会出现在下游任务的微调（fine-tuning）阶段，因此预训练阶段和微调阶段之间产生了不匹配（预训练的目标会令产生的语言表征对[MASK]敏感，但是却对其他token不敏感）。因此BERT采用了以下策略来解决这个问题：</p><p>首先在每一个训练序列中以15%的概率随机地选中某个token位置用于预测，假如是第i个token被选中，则会被替换成以下三个token之一</p><ul><li>80%的时候是[MASK]。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>[MASK]</strong></li><li>10%的时候是随机的其他token。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>apple</strong></li><li>10%的时候是原来的token<em>（保持不变，个人认为是作为2）所对应的负类）</em>。如，my dog is <strong>hairy</strong>——&gt;my dog is <strong>hairy</strong></li></ul><p>再用该位置对应的$T_i$去预测出原来的token（<em>输入到全连接，然后用softmax输出每个token的概率，最后用交叉熵计算loss）</em>。该策略令到BERT不再只对[MASK]敏感，而是对所有的token都敏感，以致能抽取出任何token的表征信息。</p><p>2）预训练后，只需要添加一个额外的输出层进行fine-tune，就可以在各种各样的下游任务中取得state-of-the-art的表现。在这过程中并不需要对BERT进行任务特定的结构修改。</p><h3 id="FLAT"><a href="#FLAT" class="headerlink" title="FLAT"></a>FLAT</h3><p><img src="https://pic4.zhimg.com/80/v2-a317ee70e90df352fe518386fd0b1c3b_1440w.jpg" alt style="zoom:50%;"></p><p>FLAT的基本思想来源于Lattice-LSTM（如上图），Lattice-LSTM采取的RNN结构无法捕捉长距离依赖，同时引入词汇信息是有损的，同时动态的Lattice结构也不能充分进行GPU并行。为解决计算效率低下、引入词汇信息有损的这两个问题，FLAT基于Transformer结构进行了两大改进：</p><p><strong>改进1：Flat-Lattice Transformer，无损引入词汇信息</strong></p><p>FLAT设计了一种巧妙position encoding来融合Lattice 结构，具体地情况如下上图所示，对于每一个字符和词汇都构建两个head position encoding 和tail position encoding，这种方式可以重构原有的Lattice结构。</p><p><img src="/2021/12/31/ner-xue-xi-zheng-li/截屏2021-12-10 下午1.48.04.png" alt style="zoom:50%;"></p><p><strong>改进2：相对位置编码，让Transformer适用NER任务</strong></p><p>Lattice结构由不同长度的跨度组成。为了编码跨域之间的交互，FLAT提出了跨域的相对位置编码。对于两个空间的 $x_i $和 $x_j$，它们之间有三种关系:交集、包含和分离，由它们的正面和反面决定。这三种关系不是直接对其编码，而是使用密集向量来建模它们的关系。它是通过对头部和尾部信息的连续变换来计算的。因此，相对位置编码不仅可以表示两个符号之间的关系，还可以表示更详细的信息，如字符和单词之间的距离。让$head[i]$和$tail[i]$表示跨度$x_i$的头和尾位置。可以用四种相对距离来表示$x_i $和 $x_j$和 之间的关系,具体公式如下所示：</p><script type="math/tex; mode=display">d_{ij}^{hh}=head[i]-head[j]\\d_{ij}^{ht}=head[i]-tail[j]\\d_{ij}^{th}=tail[i]-head[j]\\d_{ij}^{tt}=tail[i]-tail[j]\\</script><h2 id="中文NER任务特点"><a href="#中文NER任务特点" class="headerlink" title="中文NER任务特点"></a>中文NER任务特点</h2><p>近年来，引入词汇信息逐渐成为提升中文NER指标的重要手段。不同于英文NER，中文NER通常以字符为单位进行序列标注建模。这主要是由于中文分词存在误差，导致基于字符通常要好于基于词汇（经过分词）的序列标注建模方法。但是中文NER仍然需要词汇信息，因为引入词汇信息往往可以强化实体边界，特别是对于span较长的实体边界更加有效。此外引入词汇信息也是一种增强方式。对于NLP分类任务增益明显的数据增强方法，往往不能直接应用于NER任务，并且指标增益也极为有限。相反，引入词汇信息的增强方式对于小样本下的中文NER增益明显。</p><p>下面列举两种词汇增强的方法：</p><ol><li><strong><em>词向量&amp;词汇列表：</em></strong>利用一个具备良好分词结果的词向量；或者不再利用词向量，仅利用词汇或者实体边界信息，通常可通过图网络提取相关信息。这种增强方式主要分为Dynamic Architecture和Adaptive Embedding两种。</li><li><strong><em>分词器：</em></strong>单一的分词器会造成边界错误，可以引入多源分词器并pooling不同分词结果。</li></ol><h2 id="NER的应用"><a href="#NER的应用" class="headerlink" title="NER的应用"></a>NER的应用</h2><ul><li>知识图谱]</li><li>文本理解</li><li>对话系统</li><li>舆情分析</li><li>槽位抽取</li></ul><h2 id="NER未来的研究方向"><a href="#NER未来的研究方向" class="headerlink" title="NER未来的研究方向"></a>NER未来的研究方向</h2><ol><li>多类别实体</li><li>嵌套实体</li><li>实体识别与实体链接联合任务</li><li>利用辅助资源进行基于深度学习的非正式文本NER</li><li>NER模型压缩</li><li>深度迁移学习 for NER</li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><ul><li>Strubell, Emma, et al. “<a href="https://aclanthology.org/D17-1283.pdf">Fast and Accurate Entity Recognition with Iterated Dilated Convolutions.</a>” <em>arXiv preprint arXiv:1702.02098</em> (2017).</li><li>Vaswani, Ashish, et al. “<a href="https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf">Attention is all you need.</a>“ <em>Advances in neural information processing systems</em>. 2017.</li><li>Devlin, Jacob, et al. “<a href="https://arxiv.org/pdf/1810.04805.pdf">Bert: Pre-training of deep bidirectional transformers for language understanding.</a>“ <em>arXiv preprint arXiv:1810.04805</em> (2018).</li><li>Li, Jing, et al. “<a href="https://arxiv.org/pdf/1812.09449.pdf">A survey on deep learning for named entity recognition.</a>“ <em>IEEE Transactions on Knowledge and Data Engineering</em> (2020).</li><li>Li, Xiaonan, et al. “<a href="https://arxiv.org/pdf/2004.11795.pdf">FLAT: Chinese NER using flat-lattice transformer.</a>“ <em>arXiv preprint arXiv:2004.11795</em> (2020).</li></ul>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NER </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ASR错误自动识别与纠正</title>
      <link href="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/"/>
      <url>/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/</url>
      
        <content type="html"><![CDATA[<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a><strong>背景</strong></h2><p>随着科技的发展，越来越多种类的智能语音产品出现。虽然智能语音产品种类，但是他们依赖着有着像汽车发动机一样重要的内核——语音对话系统（Spoken dialogue systems）。一个好的语音对话系统可以帮助用户高效的解决一些问题，或是可以和用户进行社交对话交流。下面这张图展示了语音对话系统的一般结构：</p><p><img src="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/截屏2021-12-15 上午10.29.52.png" alt style="zoom: 50%;"></p><p>在一般的语音对话系统中都会有一个唤醒功能来启动系统，如APPLE产品中的“Hi，Siri”。系统在唤醒后会开启Voice Activity Detection ( VAD ) 状态来检测当前有没有说话声音，如果有声音则会将语音记录下来传递到ASR模块。通过ASR模块将语音转文本后，这些信息讯号会依次经过三个不同的模块（红色）处理才会反馈回人类。这三个不同的模块大致的任务可以理解为——理解信息，规划反馈，生成反馈。在语音对话系统中，第一步首先要保证输入信息的准确性，这项任务相对于上面三个模块处于上游。因为在语音交互中，由于ASR等技术的缺陷，很容易出现记录噪声或者识别错误等情况，这会影响下游任务的准确性。 基于以上原因，准确的输入信息是保证下游任务质量的基础。</p><p>在语音对话系统的信息输入中除了声音输入，还有使用文本输入的情况。中文文本纠错任务，常见错误类型包括：</p><ul><li>谐音字词，如 配副眼睛-配副眼镜</li><li>混淆音字词，如 流浪织女-牛郎织女</li><li>字词顺序颠倒，如 伍迪艾伦-艾伦伍迪</li><li>字词补全，如 爱有天意-假如爱有天意</li><li>形似字错误，如 高梁-高粱</li><li>中文拼音全拼，如 xingfu-幸福</li><li>中文拼音缩写，如 sz-深圳</li><li>语法错误，如 想象难以-难以想象</li></ul><p>在识别这两种信息的错误时，其方法也是不一样的的。虽然一般的语音对话系统在处理声音信息时，也是将声音讯号通过ASR技术转为文本内容，但是这种经过转换的文本内容和普通的文本内容在纠错时面临的问题是不同的，一种是根据发音的相似性纠错，另一种是根据字形的相似性纠错，所以这两种信息出错的数据分布也是不同的。此外，由于声音的约束性，一般来说普通文本信息的错误范围会比声音转换的文本信息更广。不过，在本篇文章中主要探讨针对于ASR识别后的文本数据纠错模型。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a><strong>解决方法</strong></h2><h3 id="基于规则的解决思路"><a href="#基于规则的解决思路" class="headerlink" title="基于规则的解决思路"></a><strong>基于规则的解决思路</strong></h3><ol><li>中文纠错分为两步走，第一步是错误检测，第二步是错误纠正；</li><li>错误检测部分先通过中文分词器切词，由于句子中含有错别字，所以切词结果往往会有切分错误的情况，这样从字粒度（语言模型困惑度（ppl）检测某字的似然概率值低于句子文本平均值，则判定该字是疑似错别字的概率大）和词粒度（切词后不在词典中的词是疑似错词的概率大）两方面检测错误， 整合这两种粒度的疑似错误结果，形成疑似错误位置候选集；</li><li>错误纠正部分，是遍历所有的疑似错误位置，并使用音似、形似词典替换错误位置的词，然后通过语言模型计算句子困惑度，对所有候选集结果比较并排序，得到最优纠正词。</li></ol><h3 id="基于深度学习的解决思路"><a href="#基于深度学习的解决思路" class="headerlink" title="基于深度学习的解决思路"></a><strong>基于深度学习的解决思路</strong></h3><ol><li>端到端的深度模型可以避免人工提取特征，减少人工工作量，RNN序列模型对文本任务拟合能力强，rnn_attention在英文文本纠错比赛中取得第一名成绩，证明应用效果不错；</li><li>CRF会计算全局最优输出节点的条件概率，对句子中特定错误类型的检测，会根据整句话判定该错误，阿里参赛2016中文语法纠错任务并取得第一名，证明应用效果不错；</li><li>seq2seq模型是使用encoder-decoder结构解决序列转换问题，目前在序列转换任务中（如机器翻译、对话生成、文本摘要、图像描述）使用最广泛、效果最好的模型之一。</li></ol><h2 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a><strong>评估方法</strong></h2><p>在纠错领域，准确率的重要性往往比召回率重要的多，所以一般使用F0.5-Score作为评估标准。在科大讯飞的纠错比赛中使用了一种综合的评估方法：<script type="math/tex">0.8*检测得分 + 0.2*纠错得分</script>。</p><h2 id="模型"><a href="#模型" class="headerlink" title="模型"></a><strong>模型</strong></h2><h3 id="统计语言模型"><a href="#统计语言模型" class="headerlink" title="统计语言模型"></a><strong>统计语言模型</strong></h3><h4 id="错误检测"><a href="#错误检测" class="headerlink" title="错误检测"></a><strong>错误检测</strong></h4><ul><li>混淆词典匹配：混淆词典（人工添加）支持纠错和错误改正，如高梁-&gt;高粱</li><li>常用词典匹配：切词后不在常用词典中的词直接放入混淆集</li><li>Ngram语言模型：某个字前后搭配的2gram和3gram的似然概率值低于句子文本的平均ppl值放入混淆集</li></ul><h4 id="候选召回"><a href="#候选召回" class="headerlink" title="候选召回"></a><strong>候选召回</strong></h4><p>可根据任务的不同对混淆集进行音似或形似替换</p><h4 id="候选排序"><a href="#候选排序" class="headerlink" title="候选排序"></a><strong>候选排序</strong></h4><p>基于统计语言模型计算对所有替换过的句子的似然概率，取概率最大的那一个</p><h3 id="ConvSeq2Seq"><a href="#ConvSeq2Seq" class="headerlink" title="ConvSeq2Seq"></a><strong>ConvSeq2Seq</strong></h3><p>Paper: 微软亚洲研究院发表的”<a href="https://arxiv.org/pdf/1807.01270.pdf">Reaching human-level performance in automatic grammatical error correction: An empirical study</a>“中使用了ConvSqe2Sqe，从而首次在英文语法纠错上超过人类水平。</p><p>这篇文章的最大贡献就是提出了Fluency boost learning和Boost inference。因为在过往seq2seq的模型纠错时往往会出现两种问题，一是模型容易受训练数据的影响，对训练数据中没有见过的语法错误，改正能力很差。其二是模型很难一次修复多个错误。Fluency boost learning通过在训练过程中对数据进行增强（如下图），可以让模型看到更多的错误，增强模型的泛化能力。</p><p><img src="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/截屏2021-12-20 下午3.31.22.png" alt style="zoom:45%;"></p><p>Boost inference即预测过程中进行增强，，一是多轮预测，二是循环预测。循环预测即从左到右预测和从右到左预测，比如右向左可以有效解决冠词出错问题，左向右可以解决主谓一致出错问题。</p><h3 id="Bert模型"><a href="#Bert模型" class="headerlink" title="Bert模型"></a><strong>Bert模型</strong></h3><h4 id="简介"><a href="#简介" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>BERT模型的工作原理是，在大型语料库（Masked LM任务）上训练BERT模型，然后通过在最后添加一些额外的层来微调我们自己的任务的模型，该模型可以是分类，问题回答或NER等。Bert是通过使用Transformer-Encoder结构，成为了一个较好的的预训练语言表示模型。Bert有两种训练任务，一种是Masked Language Model ( MLM )，另一种是Next Sentence Prediction ( NSP )。通过把Bert模型来当作任务的预训练模型，可以对提取词向量的语义特征，从而提高下有任务的表现。</p><h4 id="结构"><a href="#结构" class="headerlink" title="结构"></a><strong>结构</strong></h4><p>Bert模型中与AS纠错任务相关的是MLM部分（如下图所示）， MLM训练阶段有 15% 的token被随机替换为 【MASK】 ( 占位符 )，模型需要学会根据 【MASK】 上下文预测这些被替换的token。</p><p><img src="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/截屏2021-12-15 下午4.53.30.png" alt style="zoom:67%;"></p><p>因为在实际训练中任务中没有MASK的情况，如果只利用MASK机制训练无法让模型得到好的迁移，所以模型需要对MASK的方式进行优化。</p><ul><li>10% 的【MASK】会被随机替换成另一个词</li><li>10% 的【MASK】会被还原为正确的词</li><li>80% 的【MASK】保留这个占位符状态</li></ul><h4 id="限制"><a href="#限制" class="headerlink" title="限制"></a><strong>限制</strong></h4><ul><li>此外，由于BERT模型过于庞大，在一些实时要求很高的项目上很难满足时间响应范围的要求，从而导致无法上线。因此需要对BERT进行压缩处理。</li><li>原始的MLM的训练效率是比较低的，因为每次只能mask掉一小部分的token来训练，所以没有足够的能力来检测每个位置是否存在误差（只有15%的错误被找出），所以这种方法的精度不够好。BERT的MASK位置是随机选择的，所以并不擅长侦测句子中出现错误的位置；并且BERT纠错未考虑约束条件，导致准确率低，比如：”今 [明] 天天气怎么样”，MASK的位置是”今”， 那么纠错任务需要给出的结果是”今”。但是由于训练预料中大多数人的query都是”明天天气怎么样”，这样在没有约束的条件下，大概率给出的纠正结果是”明”，虽然句子结构是合理的，但结果显然是不正确的。</li></ul><h3 id="Soft-Masked-BERT纠错模型"><a href="#Soft-Masked-BERT纠错模型" class="headerlink" title="Soft-Masked BERT纠错模型"></a><strong>Soft-Masked BERT纠错模型</strong></h3><h4 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>Paper：<a href="https://arxiv.org/pdf/2005.07421.pdf">Spelling Error Correction with Soft-Masked BERT</a></p><p>Soft-Masked BERT是字节AI-Lab与复旦大学合作提出了一种中文文本纠错模型。<a href="https://arxiv.org/pdf/2005.07421.pdf">“Soft-Masked BERT”</a>发表在了ACL 2020上。论文首次提出了Soft-Masked BERT模型，主要创新点在于：</p><ul><li>将文本纠错划分为检测网络（Detection）和纠正网络（Correction）两部分，纠正网络的输入来自于检测网络输出。</li><li>以检测网络的输出作为权重，将 masking-embedding以“soft方式”添加到各个字符特征上，即“Soft-Masked”。</li></ul><h4 id="结构-1"><a href="#结构-1" class="headerlink" title="结构"></a><strong>结构</strong></h4><p>具体来看，模型Input是字粒度的word-embedding，可以使用BERT-Embedding层的输出或者word2vec。检测网络由Bi-GRU组成，充分学习输入的上下文信息，输出是每个位置 i 可能为错别字的概率 p(i)，值越大表示该位置出错的可能性越大。结构图如下：</p><p><img src="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/截屏2021-12-16 下午4.19.03.png" alt style="zoom: 33%;"></p><h5 id="检测网络与Soft-Masking"><a href="#检测网络与Soft-Masking" class="headerlink" title="检测网络与Soft Masking"></a><strong>检测网络与Soft Masking</strong></h5><p>Soft Masking 部分，将每个位置的特征以 $ pi $ 的概率乘上masking 字符的特征 $emark$，以 $1-pi$ 的概率乘上原始的输入特征，最后两部分相加作为每一个字符的特征，输入到纠正网络中。</p><h5 id="纠正网络"><a href="#纠正网络" class="headerlink" title="纠正网络"></a><strong>纠正网络</strong></h5><p>纠正网络部分，是一个基于BERT的序列多分类标记模型。检测网络输出的特征作为BERT 12层Transformer模块的输入，最后一层的输出 + Input部分的Embedding特征 $ei$ (残差连接)作为每个字符最终的特征表示。最后，将每个字特征过一层 Softmax 分类器，从候选词表中输出概率最大的字符认为是每个位置的正确字符。</p><h3 id="ELECTRA模型"><a href="#ELECTRA模型" class="headerlink" title="ELECTRA模型"></a><strong>ELECTRA模型</strong></h3><h4 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a><strong>简介</strong></h4><p>ELECTRA的全称是Efficiently Learning an Encoder that Classifies Token Replacements Accurately。ELECTRA通过类似GAN的结构和新的预训练任务，在更少的参数量和数据下超过了BERT，而且仅用1/4的算力就达到了当时SOTA模型RoBERTa的效果。</p><p><img src="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/截屏2021-12-15 下午5.06.53.png" alt style="zoom: 60%;"></p><center>纵轴是GLUE分数，横轴是FLOPs (floating point operations)</center><h4 id="结构-2"><a href="#结构-2" class="headerlink" title="结构"></a><strong>结构</strong></h4><p>ELECTRA最主要的贡献是提出了新的预训练任务和框架，把生成式的Masked language model(MLM)预训练任务改成了判别式的Replaced token detection(RTD)任务，判断当前token是否被语言模型替换过。虽然BERT模型中也可以处理RTD任务，但是在随机替换一些词后，BERT在替换预测的效果并不好，因为随机替换过于简单了。ELECTRA使用一个MLM的G-BERT来对输入句子进行更改，然后丢给D-BERT去判断哪个字被改过，如下图所示：</p><p><img src="/2021/12/31/asr-cuo-wu-zi-dong-shi-bie-yu-jiu-zheng/截屏2021-12-15 下午5.49.14.png" alt style="zoom:50%;"></p><ul><li>生成器（Generator）的作用是输入一个正确的句子，负责生成一个错误的版本，如”the chef cooked the meal”经过生成器内部随机抽样15%的token进行MASK后，再对这些MASK的位置进行预测，输出结果为”the chef ate the meal”，生成器中语言模型保证了生成的错误句子仍然是比较合理的，只是区别于原始句子。</li><li>判别器 ( ELECTRA ) 是用来判别生成器输出的句子中哪些位置的token被改动了，因此对每个token的位置进行original/replaced标注，如”cooked”变成了”ate”，标注为”repalced”，其余位置相同token标注为”original”，类似于序列标注任务，判别器的输出为0或1。</li></ul><p>虽然模型的结构类似于GAN，但由于GAN应用于文本的困难，所以改模型以最大似然而不是逆向的方式训练生成器。在预训练之后，需要去掉生成器，只针对对下游任务中的鉴别器（ELECTRA模型）进行微调。</p><p>ELECTRA模型的判别器虽然可以检测错误，但模型设计不是为了纠错，而是为了在有限计算资源的条件下能提取更好特征表示，进而得到更好的效果，文章中表示在GLUE数据集上表现明显优于BERT。ELECTRA的一个变体ELECTRA-MLM模型，不再输出0和1，而是预测每个MASK位置正确token的概率。如果词表大小是10000个，那么每个位置的输出就是对应的一个10000维的向量分布，概率最大的是正确token的结果，这样就从原生ELECTRA检测错误变成具有纠错功能的模型。</p><h2 id="工具"><a href="#工具" class="headerlink" title="工具"></a><strong>工具</strong></h2><h3 id="pycorrector"><a href="#pycorrector" class="headerlink" title="pycorrector"></a><strong><a href="https://github.com/shibing624/pycorrector">pycorrector</a></strong></h3><p>pycorrector依据语言模型检测错别字位置，通过拼音音似特征、笔画五笔编辑距离特征及语言模型困惑度特征纠正错别字。</p><h4 id="Models-in-pycorrector"><a href="#Models-in-pycorrector" class="headerlink" title="Models in pycorrector"></a><strong>Models in pycorrector</strong></h4><ul><li>kenlm：kenlm统计语言模型工具</li><li>rnn_attention模型：参考Stanford University的nlc模型，该模型是参加2014英文文本纠错比赛并取得第一名的方法</li><li>rnn_crf模型：参考阿里巴巴2016参赛中文语法纠错比赛CGED2018并取得第一名的方法</li><li>seq2seq_attention模型：在seq2seq模型加上attention机制，对于长文本效果更好，模型更容易收敛，但容易过拟合</li><li>transformer模型：全attention的结构代替了lstm用于解决sequence to sequence问题，语义特征提取效果更好</li><li>bert模型：中文fine-tuned模型，使用MASK特征纠正错字</li><li>conv_seq2seq模型：基于Facebook出品的fairseq，北京语言大学团队改进ConvS2S模型用于中文纠错，在NLPCC-2018的中文语法纠错比赛中，是唯一使用单模型并取得第三名的成绩</li></ul><h4 id="Detector-in-pycorrector"><a href="#Detector-in-pycorrector" class="headerlink" title="Detector in pycorrector"></a><strong>Detector in pycorrector</strong></h4><ul><li>字粒度：语言模型困惑度（ppl）检测某字的似然概率值低于句子文本平均值，则判定该字是疑似错别字的概率大</li><li>词粒度：切词后不在词典中的词是疑似错词的概率大</li></ul><h4 id="Corrector-in-pycorrector"><a href="#Corrector-in-pycorrector" class="headerlink" title="Corrector in pycorrector"></a><strong>Corrector in pycorrector</strong></h4><ul><li>通过错误检测定位所有疑似错误后，取所有疑似错字的音似、形似候选词</li><li>使用候选词替换，基于语言模型得到类似翻译模型的候选排序结果，得到最优纠正词</li></ul><h4 id="Defect-in-pycorrector"><a href="#Defect-in-pycorrector" class="headerlink" title="Defect in pycorrector"></a><strong>Defect in pycorrector</strong></h4><ul><li>现在的处理手段，在词粒度的错误召回还不错，但错误纠正的准确率还有待提高</li><li>现在的文本错误不再局限于字词粒度上的拼写错误，需要提高中文语法错误检测（CGED, Chinese Grammar Error Diagnosis）及纠正能力</li></ul><h3 id="YoungCorrector"><a href="#YoungCorrector" class="headerlink" title="YoungCorrector"></a><strong>YoungCorrector</strong></h3><p>本项目是参考Pycorrector实现的一套基于规则的纠错系统。 总体来说，基于规则的文本纠错，性能取决于纠错词典和分词质量。目前与 Pycorrector相比，在准确率差不多的情况下，本模型所用的时间会少很多（归功于前向最大匹配替代了直接索引混淆词典）。</p><h4 id="基于规则的中文纠错流程"><a href="#基于规则的中文纠错流程" class="headerlink" title="基于规则的中文纠错流程"></a><strong>基于规则的中文纠错流程</strong></h4><ol><li><p>文本处理</p><ul><li>是否为空</li><li>是否全是英文</li><li>统一编码</li><li>统一文本格式</li></ul></li><li><p>错误检索</p><ul><li><p>从混淆词典/字典中检索</p><p>直接检索</p><p>最大匹配算法</p></li><li><p>分词后，查看是否存在于通用词典</p><p>分词质量</p><p>分词后产生的单字（字本身是ok的，但是在整个句子中是错误的）</p><p>分词后产生的错词（词本身是ok的，但是在整个句子中是错误的）</p><p>分词后的词是否包含其他字母及符号</p></li><li><p>词粒度的 N-gram</p><p>计算局部得分</p><p>MAD(Median Absolute Deviation)</p></li><li><p>字粒度的 N-gram</p><p>计算局部得分</p><p>MAD(Median Absolute Deviation)</p></li><li><p>将候选错误词中的连续单字合并</p></li></ul></li><li><p>候选召回</p><ul><li><p>编辑距离</p><p>错误词的长度大于 1</p><p>只使用编辑距离为 1</p><p>只使用变换和修改，无添加和删除</p><p>对编辑距离得到的词使用拼音加以限制</p></li><li><p>音近词</p></li><li><p>形近词</p></li></ul></li><li><p>纠错排序</p><ul><li>语言模型计算得分（困惑度）</li></ul></li></ol><h2 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h2><p>Chen, Wei, et al. “ASR error detection in a conversational spoken language translation system.” <em>2013 IEEE International Conference on Acoustics, Speech and Signal Processing</em>. IEEE, 2013.</p><p>Ge, Tao, Furu Wei, and Ming Zhou. “Reaching human-level performance in automatic grammatical error correction: An empirical study.” <em>arXiv preprint arXiv:1807.01270</em> (2018).</p><p>Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” <em>arXiv preprint arXiv:1810.04805</em> (2018).</p><p>Zhang, Shaohua, et al. “Spelling error correction with soft-masked BERT.” <em>arXiv preprint arXiv:2005.07421</em> (2020).</p><p>Clark, Kevin, et al. “Electra: Pre-training text encoders as discriminators rather than generators.” <em>arXiv preprint arXiv:2003.10555</em> (2020).</p>]]></content>
      
      
      <categories>
          
          <category> NLP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ASR纠错 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>M1搭建hexo博客</title>
      <link href="/2021/12/31/m1-da-jian-hexo-bo-ke/"/>
      <url>/2021/12/31/m1-da-jian-hexo-bo-ke/</url>
      
        <content type="html"><![CDATA[<p>重度引用：<a href="http://www.aspoir.cn/posts/20210715195018/">Aspoir的《在M1上搭建hexo博客》</a></p><p>简述在M1系统上搭建hexo博客，使用master主题</p><h2 id="本地搭建hexo"><a href="#本地搭建hexo" class="headerlink" title="本地搭建hexo"></a>本地搭建hexo</h2><h3 id="安装Node-js"><a href="#安装Node-js" class="headerlink" title="安装Node.js"></a>安装Node.js</h3><p>首先检查是否已经安装</p><pre class="line-numbers language-none"><code class="language-none">npm&gt; zsh: command not found: npm<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>提示<code>command not found</code>则未安装，去<a href="https://nodejs.org/zh-cn/download/">node.js官网</a>下载安装包，按照提示安装。</p><p>再次在终端中输入npm显示已经安装</p><pre class="line-numbers language-none"><code class="language-none">npm&gt; Usage: npm &lt;command&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="安装hexo"><a href="#安装hexo" class="headerlink" title="安装hexo"></a>安装hexo</h3><p>在终端中输入</p><pre class="line-numbers language-none"><code class="language-none">sudo npm install hexo-cli -g<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>再次输入hexo，若显示如下则安装成功</p><pre class="line-numbers language-none"><code class="language-none">hexoUsage: hexo &lt;command&gt;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="初始化hexo"><a href="#初始化hexo" class="headerlink" title="初始化hexo"></a>初始化hexo</h3><pre class="line-numbers language-none"><code class="language-none">cd Documents/hexo init blog<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>提示安装git直接安装即可</p><h3 id="安装模块"><a href="#安装模块" class="headerlink" title="安装模块"></a>安装模块</h3><pre class="line-numbers language-none"><code class="language-none">cd blognpm install<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h3><pre class="line-numbers language-none"><code class="language-none">hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p><img src="https://miro.medium.com/max/1400/0*2JvuTw2MYcjCYjo1" alt></p><h2 id="发布到github上"><a href="#发布到github上" class="headerlink" title="发布到github上"></a>发布到github上</h2><h3 id="创建仓库"><a href="#创建仓库" class="headerlink" title="创建仓库"></a>创建仓库</h3><p>登录github，新建项目</p><p>Respository name必须以自己用户名为开头：<code>用户名.github.io</code></p><p>选择public，点击创建即可</p><h3 id="创建用户"><a href="#创建用户" class="headerlink" title="创建用户"></a>创建用户</h3><pre class="line-numbers language-none"><code class="language-none">git config --global user.name "your name" &lt;-引号内为自定义内容git config --global user.email "mail" &lt;-引号内为自定义内容<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="生成ssh密钥"><a href="#生成ssh密钥" class="headerlink" title="生成ssh密钥"></a>生成ssh密钥</h3><pre class="line-numbers language-none"><code class="language-none">ssh-keygen -t rsa -C "mail" &lt;-之前输入的邮箱<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="进入ssh文件夹"><a href="#进入ssh文件夹" class="headerlink" title="进入ssh文件夹"></a>进入ssh文件夹</h3><p>返回根目录（当前目录为blog）</p><pre class="line-numbers language-none"><code class="language-none">cd ..cd ..cd .ssh/<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="生成ssh-keys"><a href="#生成ssh-keys" class="headerlink" title="生成ssh keys"></a>生成ssh keys</h3><pre class="line-numbers language-none"><code class="language-none">vim id_rsa.pub<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>复制粘贴到github上：settings -&gt; SSH and GPG keys -&gt; SSH keys</p><p><a href="https://tva1.sinaimg.cn/large/008i3skNly1gshwdk2h9cj312o0o6q5r.jpg"><img src="https://tva1.sinaimg.cn/large/008i3skNly1gshwdk2h9cj312o0o6q5r.jpg" alt></a></p><h3 id="测试连接github"><a href="#测试连接github" class="headerlink" title="测试连接github"></a>测试连接github</h3><pre class="line-numbers language-none"><code class="language-none">ssh -T git@github.com<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="安装插件"><a href="#安装插件" class="headerlink" title="安装插件"></a>安装插件</h3><p>当前位于根目录</p><pre class="line-numbers language-none"><code class="language-none">cd Documents/blog/npm install hexo-deployer-git --save<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><h3 id="更改配置"><a href="#更改配置" class="headerlink" title="更改配置"></a>更改配置</h3><p>打开blog/_config.yml，在最下方更改配，然后保存，最好使用ssh 来git clone来避免输入账号密码</p><pre class="line-numbers language-none"><code class="language-none">deploy:  type: 'git'  repo: git@github.com:github名字/github名字.github.io.git  branch: master<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><h3 id="部署"><a href="#部署" class="headerlink" title="部署"></a>部署</h3><pre class="line-numbers language-none"><code class="language-none">hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h2 id="更换主题"><a href="#更换主题" class="headerlink" title="更换主题"></a>更换主题</h2><p>本人博客使用<a href="https://github.com/blinkfox/hexo-theme-matery">matery</a>主题</p><h3 id="下载"><a href="#下载" class="headerlink" title="下载"></a>下载</h3><pre class="line-numbers language-none"><code class="language-none">git clone git@github.com:blinkfox/hexo-theme-matery.git<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h3><p>配置文档详见<a href="https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md">https://github.com/blinkfox/hexo-theme-matery/blob/develop/README_CN.md</a></p><p>每次更新文章需要做:</p><pre class="line-numbers language-none"><code class="language-none">hexo clean // 清除hexo g //generatehexo d //deploy<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h2 id="更进一步"><a href="#更进一步" class="headerlink" title="更进一步"></a>更进一步</h2><h3 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h3><p><a href="https://blinkfox.github.io/2018/09/28/qian-duan/hexo-bo-ke-zhu-ti-zhi-hexo-theme-matery-de-jie-shao/#toc-heading-7">Hexo博客主题之hexo-theme-matery的介绍</a></p><p><a href="https://m3df.xyz/2020/06/13/e9fff968/#toc-heading-84">Hexo进阶：基于matery主题的网站配置教程</a></p><p><a href="https://chen-shang.github.io/2019/08/15/ji-zhu-zong-jie/hexo/hexo-theme-matery-zhu-ti-you-hua/#toc-heading-5">hexo-theme-matery主题优化</a></p><p><a href="https://blog.csdn.net/howareyou2104/article/details/106312703">如何用Hexo优雅的书写文章</a></p><p><a href="https://yanyinhong.github.io/2017/05/02/How-to-insert-image-in-hexo-post/">Hexo博客搭建之在文章中插入图片</a></p>]]></content>
      
      
      <categories>
          
          <category> Installment </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Node.js </tag>
            
            <tag> Hexo </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
